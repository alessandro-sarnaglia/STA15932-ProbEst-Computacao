---
title: "Probabilidade e Estat√≠stica"
subtitle: "Vari√°veis Aleat√≥rias"
author: "Prof. Dr. Alessandro JQ Sarnaglia"
lang: "pt"
format:
  revealjs:
    width: 1280
    height: 720
    min-scale: 0.05
    fig-width: 5.5
    fig-height: 5.5
    theme: [custom.scss]
    css: [fonts.css, style.css]
    callout-icon: false
    toc: true
    toc-depth: 3
    toc-expand: 3
    number-sections: true
    number-depth: 3
    footer: "[Voltar üîô](../index.html)"
    menu:
      useTextContentForMissingTitles: false
editor: source
filters:
  - parse-latex
---



# Defini√ß√£o

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1:** Uma amostra de $n$ componentes √© submetida a testes para verificar os defeituosos e os n√£o defeituosos. Nesse experimento, podemos definir o espa√ßo amostral
$$
\newcommand{\invisible}[1]{\color{white}{#1}}
\Omega = \{\omega = (z_1, \ldots, z_n) : z_i \in \{S,F\} \},
$$
em que $S =$ "componente defeituoso" e $F =$ "componente n√£o defeituoso".

::: 

::: {.callout-caution}
## Observa√ß√£o

Como visto acima, o espa√ßo amostral pode assumir formas bem abstratas. No entanto, em muitos casos, √© de interesse associar um valor num√©rico a cada resultado de $\Omega$.
:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1 (cont.):** No exemplo anterior, suponha $n=3$, ent√£o
$$
\Omega = \{(FFF),(SFF),(FSF),(FFS),(SSF),(SFS),(FSS),(SSS)\}.
$$
Pode ser interesse registrar apenas o n√∫mero de componentes defeituosos e n√£o as falhas individuais (se $n$ grande por exemplo). Assim, ter√≠amos $X(\omega) =$ "n√∫mero de componentes defeituosos em $\omega$" e:

::: incremental

-   $X(FFF) = 0$;
-   $X(SFF) = X(FSF) = X(FFS) = 1$;
-   $X(SSF) = X(SFS) = X(FSS) = 2$; e
-   $X(SSS) = 3$.

:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

O tipo de situa√ß√£o acima nos leva a defini√ß√£o de [**vari√°veis aleat√≥rias**]{style="color:red;"}.
:::

:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o - Vari√°veis Aleat√≥rias

Seja $\Omega$ espa√ßo amostral de um experimento aleat√≥rio. Uma **vari√°vel aleat√≥ria** (va) √© qualquer regra que associe um valor num√©rico real a cada resultado de $\Omega$.
:::

::: {.callout-caution}
## Observa√ß√µes

Da defini√ß√£o de va, podemos fazer os seguintes apontamentos:

1.    Vari√°veis aleat√≥rias normalmente s√£o denotadas por letras mai√∫sculas do final do alfabeto, tais como $X$, $Y$, $Z$, entre outras;
2.    Matematicamente, podemos definir $X$ como va se $X : \Omega \to \mathbb{R}$;
3.    Podemos tamb√©m escrever $X(\omega) = x$ para dizer que $x$ √© o valor atribu√≠do a $\omega$ pela va $X$.
:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=55%}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1 (cont.):**

<figure>
  <img src="figs/ilustracao_variavel_aleatoria0.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

:::

:::

::: {.column #vcenter width=45%}

:::

:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=55%}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1 (cont.):**

<figure>
  <img src="figs/ilustracao_variavel_aleatoria1.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

:::

:::

::: {.column #vcenter width=45%}

:::

:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=55%}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1 (cont.):**

<figure>
  <img src="figs/ilustracao_variavel_aleatoria2.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

:::

:::

::: {.column #vcenter width=45%}

:::

:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=55%}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1 (cont.):**

<figure>
  <img src="figs/ilustracao_variavel_aleatoria3.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

:::

:::

::: {.column #vcenter width=45%}

:::

:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=55%}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1 (cont.):**

<figure>
  <img src="figs/ilustracao_variavel_aleatoria4.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

:::

:::

::: {.column #vcenter width=45%}

:::

:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=55%}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1 (cont.):**

<figure>
  <img src="figs/ilustracao_variavel_aleatoria5.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

:::

:::

::: {.column #vcenter width=45%}
::: {.callout-caution}
## Observa√ß√£o

Os valores de $\text{Im}(X)$ induzem uma parti√ß√£o de $\Omega$. No exemplo, vimos:

-   $[X=0] = \{(FFF)\}$;
-   $[X=1] = \{(FFS), (FSF), (SFF)\}$;
-   $[X=2] = \{(FSS), (SFS), (SSF)\}$; e
-   $[X=3] = \{(SSS)\}$.

:::
:::

:::



## 1 Defini√ß√£o {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

A qualquer va $X$, podemos associar um contradom√≠nio (normalmente o conjunto $\mathbb{R}$) e uma [**imagem**]{style="color:red;"}, que denotaremos por $\text{Im}(X)$.

:::

::: {.callout-note}
## Defini√ß√£o

O conjunto de todos poss√≠veis valores que uma va $X$ pode assumir √© denominado **imagem** de $X$ e √© denotado por $\text{Im}(X)$.

:::

::: {.callout-caution}
## Observa√ß√£o

No exemplo anterior, temos que $\text{Im}(X) = \{0,1,2,3\}$.
:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Similarmente a estat√≠stica descritiva, podemos classificar as va's de acordo com o seu conjunto imagem.

:::

::: {.callout-note}
## Defini√ß√£o

Dada uma va $X$, dizemos que:

-   $X$ √© va **discreta** (vad) se $\text{Im}(X)$ for conjunto finito ou infinito enumer√°vel;
-   $X$ √© va **cont√≠nua** (vac) se $\text{Im}(X)$ for conjunto n√£o enumer√°vel.

:::

::: {.callout-caution}
## Observa√ß√£o

Agora, fica claro que a va $X$ do exemplo anterior se trata de uma **vad**.

:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 2:** Uma linha de produ√ß√£o √© observada at√© a produ√ß√£o da primeira pe√ßa defeituosa. Seja $S =$ "Pe√ßa defeituosa" e $F =$ "Pe√ßa normal". Ent√£o temos que
$$
\Omega = \{(S), (FS), (FFS), (FFFS), \ldots\}.
$$

::: columns

::: {.column #vcenter width=60%}

A este experimento, podemos associar a va $X =$ "N√∫mero de pe√ßas produzidas". Nesse caso, ter√≠amos
$$
\text{Im}(X) = \{1, 2, 3, \ldots\}.
$$

:::

::: {.column #vcenter width=40%}

<figure>
  <img src="figs/ilustracao_variavel_aleatoria_discreta.png" style="display: block;margin-left: auto;margin-right: auto;width: 90%;">
</figure>

:::

:::

:::

::: {.callout-caution}
## Observa√ß√£o

Embora $\text{Im}(X)$ seja infinito, ele √© um conjunto enumer√°vel, de modo que $X$ √© **vad**.

:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

::: columns

::: {.column #vcenter width=60%}

**Exemplo 3:** Um ponto de uma regi√£o circular de raio unit√°rio √© sorteado. Nesse caso, podemos definir $\Omega = \{\omega = (z_1,z_2) : z_1^2 + z_2^2 \leq 1\}$.

[Nesse espa√ßo amostral, um evento $A$, √© uma [**regi√£o**]{style="color:red;"} de pontos, $A\subset\Omega$.]{style="visibility:hidden;"}

[A este experimento, poder√≠amos associar a vari√°vel aleat√≥ria $X =$ "Dist√¢ncia de $\omega$ √† origem". Isto √© $X(\omega) = (z_1^2 - z_2^2)^{1/2}$. Note que, nesse caso $\text{Im}(X) = [0,1]$.]{style="visibility:hidden;"}

:::

::: {.column #vcenter width=40%}

<figure>
  <img src="figs/ilustracao_variavel_aleatoria_continua0.svg" style="display: block;margin-left: auto;margin-right: auto;width: 95%;">
</figure>

:::

:::

:::

::: {style="visibility:hidden"}

::: {.callout-caution}
## Observa√ß√£o

Como $\text{Im}(X)$ √© conjunto n√£o enumer√°vel, classificamos $X$ como uma **vac**.

:::

:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

::: columns

::: {.column #vcenter width=60%}

**Exemplo 3:** Um ponto de uma regi√£o circular de raio unit√°rio √© sorteado. Nesse caso, podemos definir $\Omega = \{\omega = (z_1,z_2) : z_1^2 + z_2^2 \leq 1\}$.

Nesse espa√ßo amostral, um evento $A$, √© uma [**regi√£o**]{style="color:red;"} de pontos, $A\subset\Omega$.

[A este experimento, poder√≠amos associar a vari√°vel aleat√≥ria $X =$ "Dist√¢ncia de $\omega$ √† origem". Isto √© $X(\omega) = (z_1^2 - z_2^2)^{1/2}$. Note que, nesse caso $\text{Im}(X) = [0,1]$.]{style="visibility:hidden;"}

:::

::: {.column #vcenter width=40%}

<figure>
  <img src="figs/ilustracao_variavel_aleatoria_continua1.svg" style="display: block;margin-left: auto;margin-right: auto;width: 95%;">
</figure>

:::

:::

:::

::: {style="visibility:hidden"}

::: {.callout-caution}
## Observa√ß√£o

Como $\text{Im}(X)$ √© conjunto n√£o enumer√°vel, classificamos $X$ como uma **vac**.

:::

:::

## 1 Defini√ß√£o {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

::: columns

::: {.column #vcenter width=60%}

**Exemplo 3:** Um ponto de uma regi√£o circular de raio unit√°rio √© sorteado. Nesse caso, podemos definir $\Omega = \{\omega = (z_1,z_2) : z_1^2 + z_2^2 \leq 1\}$.

Nesse espa√ßo amostral, um evento $A$, √© uma [**regi√£o**]{style="color:red;"} de pontos, $A\subset\Omega$.

A este experimento, poder√≠amos associar a vari√°vel aleat√≥ria $X =$ "Dist√¢ncia de $\omega$ √† origem". Isto √© $X(\omega) = (z_1^2 + z_2^2)^{1/2}$. Note que, nesse caso $\text{Im}(X) = [0,1]$.

:::

::: {.column #vcenter width=40%}

<figure>
  <img src="figs/ilustracao_variavel_aleatoria_continua2.svg" style="display: block;margin-left: auto;margin-right: auto;width: 95%;">
</figure>

:::

:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Como $\text{Im}(X)$ √© conjunto n√£o enumer√°vel, classificamos $X$ como uma **vac**.

:::

:::

## Fun√ß√£o de distribui√ß√£o acumulada

::: {.callout-caution}
## Observa√ß√£o

Um evento aleat√≥rio muito importante para modelagem de va's √© $[X \leq x]$. A probabilidade deste evento nos diz o quanto de probabilidade a va $X$ acumula at√© o valor $x$.

:::

::: {.callout-note}
## Defini√ß√£o

A **fun√ß√£o de distribui√ß√£o acumulada** (fda) √© definida por $F_X(x) = P(X \leq x)$
:::

::: {.callout-caution}
## Observa√ß√£o

Ao se partir de uma medida de probabilidade definida em $\Omega$, para o calcular $F_X(x)$, devemos encontrar a probabilidade (em $\Omega$) do evento $A_x = \{\omega \in \Omega: X(\omega) \leq x\}$. Isto √©, $F_X(x) = P(X \leq x) = P(A_x)$.

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 1 (cont.) {.unnumbered .unlisted}

Se supomos que $\Omega$ √© equiprov√°vel. Teremos que

$$
F_X(x) = \begin{cases}
0, & x < 0;\\
P(X=0) = \frac{1}{8}, & 0 \leq x < 1;\\
P(X=0) + P(X=1) = \frac{1}{8} + \frac{3}{8} = \frac{4}{8}, & 1 \leq x < 2;\\
P(X=0) + P(X=1) + P(X=2) = \frac{1}{8} + \frac{3}{8} + \frac{3}{8} = \frac{7}{8}, & 2 \leq x < 3;\\
P(X=0) + P(X=1) + P(X=2) + P(X=3) = \frac{1}{8} + \frac{3}{8} + \frac{3}{8} + \frac{1}{8} = 1, & x \geq 3;\\
\end{cases}
$$

## {.unnumbered .unlisted}

### Exemplo 1 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/ilustracao_fda0.png" style="display: block;margin-left: auto;margin-right: auto;width: 65%;">
</figure>

## {.unnumbered .unlisted}

### Exemplo 1 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/ilustracao_fda1.png" style="display: block;margin-left: auto;margin-right: auto;width: 65%;">
</figure>

## {.unnumbered .unlisted}

### Exemplo 1 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/ilustracao_fda2.png" style="display: block;margin-left: auto;margin-right: auto;width: 65%;">
</figure>

## {.unnumbered .unlisted}

### Exemplo 1 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/ilustracao_fda3.png" style="display: block;margin-left: auto;margin-right: auto;width: 65%;">
</figure>

## {.unnumbered .unlisted}

### Exemplo 1 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/ilustracao_fda4.png" style="display: block;margin-left: auto;margin-right: auto;width: 65%;">
</figure>


## {.unnumbered .unlisted}

### Exemplo 1 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/ilustracao_fda5.png" style="display: block;margin-left: auto;margin-right: auto;width: 65%;">
</figure>

::: {.callout-caution}
## Observa√ß√£o

No caso discreto, os tamanhos dos saltos de $F_X(x)$ nos d√° os valores de $P(X=x)$.
:::

## 1.1 Fun√ß√£o de distribui√ß√£o acumulada {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Se $X$ √© vad, a fun√ß√£o $p_X(x) = P(X=x)$, $x \in \text{Im}(X)$, √© chamada [**fun√ß√£o massa de probabilidade**]{style="color:red;"} (fmp).
:::

::: fragment

::: {.callout-warning}
## Propriedades

A fmp $p_X(x)$ de um uma vad $X$ satisfaz:

::: incremental

1.    $p_X(x) \geq 0$;
2.    $\textstyle \sum_{x} p_X(x) = 1$.

:::

:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Assim, vemos que, para vari√°veis aleat√≥rias discretas, a fda $F_X(x)$ nos fornece a fmp $p_X(x)$ e vice-versa.
:::

:::

## 1.1 Fun√ß√£o de distribui√ß√£o acumulada {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

A fmp √© um modelo te√≥rico para as **frequ√™ncias relativas** vistas em estat√≠stica descritiva.
:::

::: fragment


<figure>
  <img src="figs/ilustracao_frequencia_fmp.png" style="display: block;margin-left: auto;margin-right: auto;width: 75%;">
</figure>


:::

## {.unnumbered .unlisted .smaller}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

Supondo que nenhum ponto de $\Omega$ √© privilegiado, podemos definir a medida de probabilidade de $A$ como
$$\textstyle P(A) = \frac{\text{√Årea}(A)}{\text{√Årea}(\Omega)} = \frac{\text{√Årea}(A)}{\pi}.$$
Nesse caso, as probabilidades de dois eventos de mesma √°rea ser√£o as mesmas (equiprobabilidade).  

::: columns

::: {.column #vcenter width=65%}

[Usando essa medida de probabilidade, para calcular a fda de $X$, precisamos encontrar a √°rea da regi√£o $[X \leq x]$. Veja na ilustra√ß√£o ao lado.]{style="visibility:hidden;"}

[Logo, a fda de $X$ √© dada por
$$
\textstyle F_X(x) = P(X \leq x) = \begin{cases}
0, & x < 0;\\
\frac{\text{√Årea}(X \leq x)}{\pi} = \frac{\pi x^2}{\pi} = x^2, & 0\leq x < 1;\\
1, & x \geq 1.
\end{cases}
$$
]{style="visibility:hidden;"}

:::

::: {.column #vcenter width=35%}

<figure>
  <img src="figs/ilustracao_fda_vac0.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

Supondo que nenhum ponto de $\Omega$ √© privilegiado, podemos definir a medida de probabilidade de $A$ como
$$\textstyle P(A) = \frac{\text{√Årea}(A)}{\text{√Årea}(\Omega)} = \frac{\text{√Årea}(A)}{\pi}.$$
Nesse caso, as probabilidades de dois eventos de mesma √°rea ser√£o as mesmas (equiprobabilidade).  

::: columns

::: {.column #vcenter width=65%}

Usando essa medida de probabilidade, para calcular a fda de $X$, precisamos encontrar a √°rea da regi√£o $[X \leq x]$. Veja na ilustra√ß√£o ao lado.

[Logo, a fda de $X$ √© dada por
$$
\textstyle F_X(x) = P(X \leq x) = \begin{cases}
0, & x < 0;\\
\frac{\text{√Årea}(X \leq x)}{\pi} = \frac{\pi x^2}{\pi} = x^2, & 0\leq x < 1;\\
1, & x \geq 1.
\end{cases}
$$
]{style="visibility:hidden;"}

:::

::: {.column #vcenter width=35%}

<figure>
  <img src="figs/ilustracao_fda_vac0.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>


:::

:::



## {.unnumbered .unlisted .smaller}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

Supondo que nenhum ponto de $\Omega$ √© privilegiado, podemos definir a medida de probabilidade de $A$ como
$$\textstyle P(A) = \frac{\text{√Årea}(A)}{\text{√Årea}(\Omega)} = \frac{\text{√Årea}(A)}{\pi}.$$
Nesse caso, as probabilidades de dois eventos de mesma √°rea ser√£o as mesmas (equiprobabilidade).  

::: columns

::: {.column #vcenter width=65%}

Usando essa medida de probabilidade, para calcular a fda de $X$, precisamos encontrar a √°rea da regi√£o $[X \leq x]$. Veja na ilustra√ß√£o ao lado.

[Logo, a fda de $X$ √© dada por
$$
\textstyle F_X(x) = P(X \leq x) = \begin{cases}
0, & x < 0;\\
\frac{\text{√Årea}(X \leq x)}{\pi} = \frac{\pi x^2}{\pi} = x^2, & 0\leq x < 1;\\
1, & x \geq 1.
\end{cases}
$$
]{style="visibility:hidden;"}

:::

::: {.column #vcenter width=35%}

<figure>
  <img src="figs/ilustracao_fda_vac1.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>


:::

:::


## {.unnumbered .unlisted .smaller}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

Supondo que nenhum ponto de $\Omega$ √© privilegiado, podemos definir a medida de probabilidade de $A$ como
$$\textstyle P(A) = \frac{\text{√Årea}(A)}{\text{√Årea}(\Omega)} = \frac{\text{√Årea}(A)}{\pi}.$$
Nesse caso, as probabilidades de dois eventos de mesma √°rea ser√£o as mesmas (equiprobabilidade).  

::: columns

::: {.column #vcenter width=65%}

Usando essa medida de probabilidade, para calcular a fda de $X$, precisamos encontrar a √°rea da regi√£o $[X \leq x]$. Veja na ilustra√ß√£o ao lado.

[Logo, a fda de $X$ √© dada por
$$
\textstyle F_X(x) = P(X \leq x) = \begin{cases}
0, & x < 0;\\
\frac{\text{√Årea}(X \leq x)}{\pi} = \frac{\pi x^2}{\pi} = x^2, & 0\leq x < 1;\\
1, & x \geq 1.
\end{cases}
$$
]{style="visibility:hidden;"}

:::

::: {.column #vcenter width=35%}

<figure>
  <img src="figs/ilustracao_fda_vac2.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>


:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

Supondo que nenhum ponto de $\Omega$ √© privilegiado, podemos definir a medida de probabilidade de $A$ como
$$\textstyle P(A) = \frac{\text{√Årea}(A)}{\text{√Årea}(\Omega)} = \frac{\text{√Årea}(A)}{\pi}.$$
Nesse caso, as probabilidades de dois eventos de mesma √°rea ser√£o as mesmas (equiprobabilidade).  

::: columns

::: {.column #vcenter width=65%}

Usando essa medida de probabilidade, para calcular a fda de $X$, precisamos encontrar a √°rea da regi√£o $[X \leq x]$. Veja na ilustra√ß√£o ao lado.

[Logo, a fda de $X$ √© dada por
$$
\textstyle F_X(x) = P(X \leq x) = \begin{cases}
0, & x < 0;\\
\frac{\text{√Årea}(X \leq x)}{\pi} = \frac{\pi x^2}{\pi} = x^2, & 0\leq x < 1;\\
1, & x \geq 1.
\end{cases}
$$
]{style="visibility:hidden;"}

:::

::: {.column #vcenter width=35%}

<figure>
  <img src="figs/ilustracao_fda_vac3.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>


:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

Supondo que nenhum ponto de $\Omega$ √© privilegiado, podemos definir a medida de probabilidade de $A$ como
$$\textstyle P(A) = \frac{\text{√Årea}(A)}{\text{√Årea}(\Omega)} = \frac{\text{√Årea}(A)}{\pi}.$$
Nesse caso, as probabilidades de dois eventos de mesma √°rea ser√£o as mesmas (equiprobabilidade).  

::: columns

::: {.column #vcenter width=65%}

Usando essa medida de probabilidade, para calcular a fda de $X$, precisamos encontrar a √°rea da regi√£o $[X \leq x]$. Veja na ilustra√ß√£o ao lado.

Logo, a fda de $X$ √© dada por
$$
\textstyle F_X(x) = P(X \leq x) = \begin{cases}
0, & x < 0;\\
\frac{\text{√Årea}(X \leq x)}{\pi} = \frac{\pi x^2}{\pi} = x^2, & 0\leq x < 1;\\
1, & x \geq 1.
\end{cases}
$$

:::

::: {.column #vcenter width=35%}

<figure>
  <img src="figs/ilustracao_fda_vac3.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>


:::

:::

## {.unnumbered .unlisted}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/ilustracao_fda_vac_grafico0.png" style="display: block;margin-left: auto;margin-right: auto;width: 50%;">
</figure>


## {.unnumbered .unlisted}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/ilustracao_fda_vac_grafico1.png" style="display: block;margin-left: auto;margin-right: auto;width: 50%;">
</figure>


## {.unnumbered .unlisted}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/ilustracao_fda_vac_grafico2.png" style="display: block;margin-left: auto;margin-right: auto;width: 50%;">
</figure>



::: {.callout-caution}
## Observa√ß√£o

Observe que n√£o h√° pontos de descontinuidade para a fda de uma va cont√≠nua. Portanto, a fda de uma vac n√£o fornece probabilidades diretamente.
:::

## 1.1 Fun√ß√£o de distribui√ß√£o acumulada {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Tomando a derivada da fda de uma vac, obtemos a chamada [**fun√ß√£o densidade de probabilidade**]{style="color:red;"}.
:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Se $X$ √© vac com fda $F_X(x)$, a sua **fun√ß√£o densidade de probabilidade** (fdp) √© definida por $f_X(x) = \frac{dF_X(x)}{dx}$.
:::

:::

::: fragment

::: {.callout-warning}
## Propriedades

A fdp $f_X(x)$ de uma vac $X$ satisfaz:

::: incremental

1.    $f_X(x) \geq 0$, $x\in \mathbb{R}$;
2.    $\int_{-\infty}^\infty f_X(x)dx = 1$.

:::

:::

:::

## 1.1 Fun√ß√£o de distribui√ß√£o acumulada {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

A fdp √© um modelo te√≥rico para as **densidades** de faixas vistas em estat√≠stica descritiva.
:::

::: fragment

<figure>
  <img src="figs/ilustracao_densidade_fdp.png" style="display: block;margin-left: auto;margin-right: auto;width: 75%;">
</figure>

:::

## 1.1 Fun√ß√£o de distribui√ß√£o acumulada {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Note que o tratamento probabil√≠stico para obter a fda, bem como o seu comportamento, dependem do tipo de va em quest√£o.
:::

::: fragment

::: {.callout-warning}
## Propriedades

Seja $F_X(x)$ fda de uma va $X$, ent√£o

::: incremental

1.    $\lim_{x\to-\infty} F_X(x) = 0$ e $\lim_{x\to\infty} F_X(x) = 1$;
2.    $x < x' \Rightarrow F_X(x) \leq F_X(x')$, isto √©, $F_X(\cdot)$ √© n√£o decrescente; e
3.    se $X$ √© vad, $P(X=x) = F_X(x) - \lim_{x'\uparrow x} F_X(x')$, isto √©, a fmp $p_X(x)$ √© o tamanho do salto de $F_X(\cdot)$ em $x$.
4.    se $X$ √© vac, $f_X(x) = \frac{dF_X(x)}{dx}$, isto √©, a fdp $f_X(x)$ √© a derivada de $F_X(\cdot)$ em $x$.

:::

:::

:::

# Vari√°veis aleat√≥rias discretas

## 2 Vari√°veis aleat√≥rias discretas {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√µes

::: incremental

1. Vimos que, se o modelo probabil√≠stico √© concebido sobre um espa√ßo amostral $\Omega$ abstrato e $X : \Omega \to \mathbb{R}$, ent√£o a fmp pode ser obtida por
$$p_X(x) = P(X = x) = P(A_x),$$
em que $A_x = \{\omega \in \Omega : X(\omega) = x\}$. Isto √©, a fmp de $X$ √© **induzida** pela medida de probabilidade em $\Omega$;
2. Muito frequentemente, a constru√ß√£o de um modelo probabil√≠stico em $\Omega$ √© muito complicada, o que dificultaria a determina√ß√£o da fmp induzida.
3. Para contornar esse problema, levando em considera√ß√£o o comportamento da vad em estudo, podemos especificar diretamente uma fmp para a vad, desde que atenda as propriedades j√° vistas:
    + $p_X(x) \geq 0$;
    + $\sum_{x\in\text{Im}(X)} p_X(x) = 1$.

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 4 {.unnumbered .unlisted}

Suponha que em um estoque existem seis lotes de um produto: tr√™s com $0$ itens defeituosos; dois com $1$ defeituoso; e um com $2$ defeituosos. Um lote ser√° sorteado entre os seis e ser√° enviado a um cliente. Seja $X=$ "n√∫mero de itens defeituosos no lote recebido pelo cliente". [Nesse caso, podemos especificar]{.fragment fragment-index=1}

::: {.fragment fragment-index=1}

::: columns

::: {.column #vcenter width=45%}

<div style="text-align: center;">
<table>
  <tr style="border-top:solid;">
    <td style="border-right:solid 1pt;">$x$</td>
    <td style="text-align: center;">$0$</td>
    <td style="text-align: center;">$1$</td>
    <td style="text-align: center;">$2$</td>
    <td style="text-align: center;border-left:solid 1pt;">Total</td>
  </tr>
  <tr style="border-top:solid 1pt;border-bottom:solid;">
    <td style="border-right:solid 1pt;">$p_X(x)$ </td>
    <td style="text-align: center;">$\frac{3}{6}$</td>
    <td style="text-align: center;">$\frac{2}{6}$</td>
    <td style="text-align: center;">$\frac{1}{6}$</td>
    <td style="text-align: center;border-left:solid 1pt;">$1$</td>
  </tr>
</table>
</div>

:::

::: {.column #vcenter width=10%}

<p style="text-align:center;">
ou ainda
</p>

:::

::: {.column #vcenter width=45%}

<div style="text-align: center;">
<table>
  <tr style="border-top:solid;">
    <td style="border-right:solid 1pt;">$x$</td>
    <td style="text-align: center;">$0$</td>
    <td style="text-align: center;">$1$</td>
    <td style="text-align: center;">$2$</td>
    <td style="text-align: center;border-left:solid 1pt;">Total</td>
  </tr>
  <tr style="border-top:solid 1pt;border-bottom:solid;">
    <td style="border-right:solid 1pt;">$p_X(x)$ </td>
    <td style="text-align: center;">$\frac{1}{2}$</td>
    <td style="text-align: center;">$\frac{1}{3}$</td>
    <td style="text-align: center;">$\frac{1}{6}$</td>
    <td style="text-align: center;border-left:solid 1pt;">$1$</td>
  </tr>
</table>
</div>

:::

:::

:::

::: {.fragment fragment-index=2}

Se n√£o sabemos a quantidade de lotes em estoque, a especifica√ß√£o acima n√£o √© pratic√°vel, pois n√£o sabemos o comportamento probabil√≠stico do experimento ($\Omega$ √© equiprov√°vel, mas desconhecido).

:::

::: {.fragment fragment-index=3}

Nesse caso, se ainda assumirmos que os lotes tem no m√°ximo 2 itens defeituosos, podemos especificar uma fmp arbitr√°ria para a vad $X$, como, por exemplo

<div style="text-align: center;">
<table>
  <tr style="border-top:solid;">
    <td style="border-right:solid 1pt;">$x$</td>
    <td style="text-align: center;">$0$</td>
    <td style="text-align: center;">$1$</td>
    <td style="text-align: center;">$2$</td>
    <td style="text-align: center;border-left:solid 1pt;">Total</td>
  </tr>
  <tr style="border-top:solid 1pt;border-bottom:solid;">
    <td style="border-right:solid 1pt;">$p_X(x)$ </td>
    <td style="text-align: center;">$0.7$</td>
    <td style="text-align: center;">$0.2$</td>
    <td style="text-align: center;">$0.1$</td>
    <td style="text-align: center;border-left:solid 1pt;">$1$</td>
  </tr>
</table>
</div>

:::

::: {.fragment fragment-index=4}

Note que $p_X(0), p_X(1), p_X(2) \geq 0$ e que $\sum_{x=0}^2 p_X(x) = 1$, de modo que $p_X(x)$ √© uma fmp v√°lida.

:::

## 2 Vari√°veis aleat√≥rias discretas {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

No exemplo anterior, especificamos de maneira arbitr√°ria uma fmp $p_X(x)$ para a vad $X$. No entanto, podemos ser mais flex√≠veis e propor uma [**fam√≠lia**]{style="color:red;"} de fmp's que dependa de [**par√¢metros**]{style="color:red;"}.

:::

::: {.fragment fragment-index=1 style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 4 (cont.):** 

Para flexibilizar o nosso modelo probabil√≠stico, podemos especificar a seguinte [**forma**]{style="color:red;"} para a fmp:

::: columns

::: {.column #vcenter width=30%}

<div style="text-align: center;">
<table>
  <tr style="border-top:solid;">
    <td style="border-right:solid 1pt;">$x$</td>
    <td style="text-align: center;">$0$</td>
    <td style="text-align: center;">$1$</td>
    <td style="text-align: center;">$2$</td>
    <td style="text-align: center;border-left:solid 1pt;">Total</td>
  </tr>
  <tr style="border-top:solid 1pt;border-bottom:solid;">
    <td style="border-right:solid 1pt;">$p_X(x)$ </td>
    <td style="text-align: center;">$\alpha$</td>
    <td style="text-align: center;">$\beta$</td>
    <td style="text-align: center;">$\gamma$</td>
    <td style="text-align: center;border-left:solid 1pt;">$1$</td>
  </tr>
</table>
</div>

:::

::: {.column #vcenter width=70%}

::: {.fragment fragment-index=2}

Para $p_X(x)$ ser uma fmp, precisamos impor as seguintes restri√ß√µes:

::: incremental
- $0 \leq \alpha \leq 1$;
- $0 \leq \beta \leq 1-\alpha$; e
- $\gamma = 1-\alpha-\beta$ (isto √©, $\gamma$ √© fixo).
:::

:::

:::

:::

::: fragment

Nesse caso, dizemos que $\alpha$ e $\beta$ s√£o **par√¢metros** (livres) da fam√≠lia de fmp's $p_X(x)$.

:::

::: 

## 2 Vari√°veis aleat√≥rias discretas {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Se a forma de $p_X(x)$ depende de um valor (ou vetor de valores) $\theta$, dizemos que $\theta$ √© um **par√¢metro** (ou vetor param√©trico) da fam√≠lia $p_X(x)$.
:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Nesse caso, podemos escrever $p_X(x; \theta)$ para enfatizar que se trata de uma fam√≠lia de fmp's e que sua forma s√≥ √© completamente conhecida se soubermos o valor do par√¢metro $\theta$ que indexa a fam√≠lia.

:::

:::

::: {.fragment style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 4 (cont.):** Nesse exemplo, podemos agrupar os par√¢metros no vetor $\theta = (\alpha, \beta)$. Note que, a fmp induzida pela medida equiprov√°vel em $\Omega$ e a fmp arbitrariamente especificada posteriormente s√£o membros particulares dessa fam√≠lia, em que $\theta = (\frac{1}{2}, \frac{1}{3})$ e $\theta = (0.7, 0.2)$, respectivamente.

:::

## 2 Vari√°veis aleat√≥rias discretas {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Como j√° visto anteriormente, a fda pode ser obtida acumulando as probabilidades da fmp.
:::

::: {.fragment style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

::: columns

::: {.column #vcenter width=50%}

**Exemplo 4 (cont.):** A fda para a fam√≠lia de fmp's neste caso √©
$$
F_X(x) = F_X(x; \theta) = \begin{cases}
0, & x < 0;\\
\alpha, & 0\leq x < 1;\\
\alpha + \beta, & 1\leq x < 2;\\
1, & x \geq 2;\\
\end{cases}
$$

:::

::: {.column #vcenter width=50%}

<figure>
  <img src="figs/ilustracao_fda_exemplo4.png" style="display: block;margin-left: auto;margin-right: auto;width: 90%;">
</figure>

:::

:::

:::

## 2 Vari√°veis aleat√≥rias discretas {.unnumbered .unlisted}

::: {.callout-tip}
## Exerc√≠cio

Considere a seguinte fun√ß√£o de $x$ que depende de um vetor de par√¢metros $\theta = (\theta_1, \theta_2, \theta_3, \theta_4)$:
$$
p_X(x;\theta) = \begin{cases}
\theta_1, & x=0;\\
\theta_2, & x=1;\\
\theta_3, & x=2;\\
\theta_4, & x=3.
\end{cases}
$$

Pede-se:

1. Quais restri√ß√µes necessitamos impor sobre $\theta = (\theta_1, \theta_2, \theta_3, \theta_4)$ para garantir que $p_X(x;\theta)$ seja de fato fam√≠lia de fmp's?
2. Qual a condi√ß√£o adicional sobre $\theta$ para que $p_X(x;\theta)$ seja sim√©trica? Qual √© o ponto de simetria?
3. Determine e esboce a fda associada a fam√≠lia $p_X(x;\theta)$.
:::


## Esperan√ßa e vari√¢ncia de vad's

::: {.callout-caution}
## Observa√ß√£o

Vimos que a fmp √© um modelo te√≥rico para as frequ√™ncias relativas vistas em estat√≠stica descritiva. Assim, fazendo uso da fmp, √© natural calcularmos valores te√≥ricos para a m√©dia e a vari√¢ncia de uma vad $X$.
:::

::: {.callout-note}
## Defini√ß√£o

Seja $X$ vad com imagem $\text{Im}(X)$ e fmp $p_X(x;\theta)$, sua **m√©dia**, tamb√©m chamada de **esperan√ßa** ou **valor esperado**, √© definida por 
$$\mu = \mu_X = E(X) = \sum_{x\in\text{Im}(X)} x\cdot p_X(x;\theta).$$
:::

## 2.1 Esperan√ßa e vari√¢ncia de vad's {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√µes

::: incremental

1. A f√≥rmula para o c√°lculo de $\mu$ √© muito similar √†quela vista em estat√≠stica descritiva, no entanto, agora a fmp $p_X(x;\theta)$ toma o lugar da frequ√™ncia relativa que teria sido efetivamente observada;
2. De maneira similar a estat√≠stica descritiva, $\mu$ √© calculada como uma m√©dia **ponderada** com pesos dados por $p_X(x;\theta)$;
3. Naturalmente, para fam√≠lias de fmp's, pelo fato de $p_X(x;\theta)$ ser indexada por $\theta$, a sua esperan√ßa tamb√©m ser√° fun√ß√£o de $\theta$.

:::

:::

::: {.fragment style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 4 (cont.):** Utilizando a f√≥rmula de esperan√ßa, obtemos
$$\mu_X = 0p_X(0;\theta) + 1p_X(1;\theta) + 2p_X(2;\theta) = \beta + 2(1-\alpha-\beta) = 2-2\alpha - \beta.$$

Se $\alpha = \frac{1-\beta}{2}$, vemos que a fmp √© sim√©trica em torno de $1$ e $\mu_X = 1$.

:::

## 2.1 Esperan√ßa e vari√¢ncia de vad's {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√µes

::: incremental

1. Quando est√° claro a que va $X$ a esperan√ßa se refere, usaremos $\mu$ em vez de $\mu_X$;
2. No Exemplo 4, se hipotetizarmos uma popula√ß√£o em que a vad $X$ com valores $0, 1$ e $2$ associados as frequ√™ncias relativas $\alpha$, $\beta$, $1-\alpha-\beta$, respectivamente, ent√£o $\mu = 2-2\alpha - \beta$ ser√° a m√©dia populacional. Por isso podemos nos referir a $\mu$ por m√©dia populacional ou te√≥rica;
3. No Exemplo 4, tomando $\theta = (\frac{1}{2}, \frac{1}{3})$, obtemos $\mu = 2-2\frac{1}{2} - \frac{1}{3} = \frac{2}{3} \approx 0.67$, que n√£o √© um valor poss√≠vel de $X$. A palavra esperado deve ser interpretada com cautela porque uma pessoa n√£o espera ver um valor de $X = 0.67$ quando um √∫nico estoque √© selecionado.

:::

:::


## 2.1 Esperan√ßa e vari√¢ncia de vad's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 5:** Seja $X$ o n√∫mero de entrevistas que um estudante faz para conseguir um emprego e suponha que a sua seja $p_X(x) = \frac{k}{x^2}$, $x = 1, 2, 3, \ldots$, e $p_X(x) = 0$ caso contr√°rio. A constante $k$ deve ser tal que $k^{-1} = \sum_{x=1}^\infty\frac{1}{x^2}$. √â poss√≠vel mostrar que $\sum_{x=1}^\infty\frac{1}{x^2} < +\infty$, portanto $p_X(x)$ √© de fato uma fmp.

::: {.fragment fragment-index=1}

Agora, o seu valor esperado √©
$$
\textstyle \mu = \sum_{x=1}^\infty x \cdot p_X(x) = \sum_{x=1}^\infty x\cdot\frac{k}{x^2} = k\sum_{x=1}^\infty \frac{1}{x}.
$$
O √∫ltimo termo √© a soma harm√¥nica, que diverge, isto √©, $\mu = +\infty$.

:::

:::

::: {.fragment fragment-index=2}

::: {.callout-caution}
## Observa√ß√£o

Dizemos que $p_X(x)$ tem **caudas longas** (as probabilidades da cauda decrescem muito lentamente). Ao selecionar uma amostra dessa vari√°vel, a m√©dia amostras tender√° a crescer indefinidamente com o tamanho da amostra.

:::

:::

## 2.1 Esperan√ßa e vari√¢ncia de vad's {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Frequentemente, temos interesse na esperan√ßa de $Y = h(X)$, em vez de $X$. Pela defini√ß√£o poder√≠amos obter $\mu_Y = \sum_y y \cdot p_Y(y)$, necessitando da fmp de $Y$. No entanto, existe uma maneira mais direta.
:::

::: {.fragment fragment-index=1 style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 6:** Seja $X$ vad e $Y = X^2$. [Por exemplo: ]{.fragment fragment-index=2}

::: columns

::: {.column #vcenter width=50%}

::: {.fragment fragment-index=2}
<div style="text-align: center;">
Se a fmp de $X$ for:
<table>
  <tr style="border-top:solid;">
    <td style="border-right:solid 1pt;">$x$</td>
    <td style="text-align: center;">$-2$</td>
    <td style="text-align: center;">$0$</td>
    <td style="text-align: center;">$2$</td>
    <td style="text-align: center;border-left:solid 1pt;">Total</td>
  </tr>
  <tr style="border-top:solid 1pt;border-bottom:solid;">
    <td style="border-right:solid 1pt;">$p_X(x)$ </td>
    <td style="text-align: center;">$p_{-2}$</td>
    <td style="text-align: center;">$p_{0}$</td>
    <td style="text-align: center;">$p_{2}$</td>
    <td style="text-align: center;border-left:solid 1pt;">$1$</td>
  </tr>
</table>
</div>

:::

:::

::: {.column #vcenter width=50%}

::: {.fragment fragment-index=3}

<div style="text-align: center;">
A fmp de $Y$ ser√°:
<table>
  <tr style="border-top:solid;">
    <td style="border-right:solid 1pt;">$x$</td>
    <td style="text-align: center;">$0$</td>
    <td style="text-align: center;">$4$</td>
    <td style="text-align: center;border-left:solid 1pt;">Total</td>
  </tr>
  <tr style="border-top:solid 1pt;border-bottom:solid;">
    <td style="border-right:solid 1pt;">$p_Y(y)$ </td>
    <td style="text-align: center;">$p_{0}$</td>
    <td style="text-align: center;">$p_{-2}+p_{2}$</td>
    <td style="text-align: center;border-left:solid 1pt;">$1$</td>
  </tr>
</table>
</div>

:::

:::

:::

[Logo,
$$\textstyle \mu_Y = \sum_y y\cdot p_Y(y) = 0p_0 + 4(p_{-2} + p_2) = (-2)^2p_{-2} + 0^2p_{0} + 2^2p_{2} = \sum_x h(x)\cdot p_X(x) = E[h(X)].$$
]{.fragment fragment-index=4}

:::

## 2.1 Esperan√ßa e vari√¢ncia de vad's {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Assim, se $X$ for vad com fmp $p_X(x)$ e $Y = h(X)$, ent√£o a esperan√ßa de $Y$ √©
$$\mu_Y = E[h(X)] = \sum_x h(x)\cdot p_X(x).$$
Isto √©, substitu√≠mos os $x$ pelos $h(x)$ no c√°lculo.

:::

::: fragment

::: {.callout-warning}
## Propriedade

Seja $X$ vad com esperan√ßa $\mu_X$ e $Y = aX+b$, com $a, b$ constantes, ent√£o
$$\mu_{Y} = \mu_{aX+b} = E(aX + b) = a E(X) + b = a\mu_X + b.$$
:::

:::

## 2.1 Esperan√ßa e vari√¢ncia de vad's {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

√â verdade que $E(aX + b) = a E(X) + b$. Mas, n√£o necessariamente vale que  $E[h(X)] = h[E(X)]$.

:::

::: {.fragment style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 6 (cont.):** Sejam $p_{-2} = p_2 = \frac{1}{4}$ e $p_0 = \frac{1}{2}$. [Ent√£o, temos
$$\textstyle E(X) = -2\frac{1}{4} + 0\frac{1}{2} + 2\frac{1}{4} = 0$$]{.fragment}
[e
$$\textstyle E(X^2) = (-2)^2\frac{1}{4} + 0^2\frac{1}{2} + 2^2\frac{1}{4} = 2.$$]{.fragment}

::: fragment

Assim, $E(X^2) = 2 \neq [E(X)]^2 = 0$.

:::

:::


## 2.1 Esperan√ßa e vari√¢ncia de vad's {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Seja $X$ vad com fmp $p_X(x)$ e esperan√ßa $\mu$. A vari√¢ncia de $X$, denotada por $V(X)$, √© definida por
$$\textstyle V(X) = \sum_{x}(x - \mu)^2\cdot p_X(x) = E[(X - \mu)^2].$$
O desvio padr√£o (DP) de $X$ √© definido por $DP(X) = \sqrt{V(X)}.$ A vari√¢ncia pode ser denotada por $\sigma^2_X$, ou apenas $\sigma^2$, e o DP por $\sigma_X$, ou apenas $\sigma$.

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Note que a vari√¢ncia √© definida como a esperan√ßa de $h(X) = (X-\mu)^2$. Isto √©, quanto **esperamos** que a vari√°vel $X$ se desvie de sua esperan√ßa $\mu$.
:::

:::

## 2.1 Esperan√ßa e vari√¢ncia de vad's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 6 (cont.):** Suponha agora que $p_2 = \frac{\alpha}{2}$, $p_{-2} = \frac{1-\alpha}{2}$ e $p_0 = \frac{1}{2}$, com $0 < \alpha < 1$. [Ent√£o,
$$\textstyle E(X) = \mu = -2\frac{1-\alpha}{2} + 2\frac{\alpha}{2} = 2\alpha - 1.$$]{.fragment}
[Ent√£o, a vari√¢ncia √© dada por
$$\textstyle V(X) = \sigma^2 = \sum_{x}(x - \mu)^2\cdot p_X(x) = \sum_{x}[x-(2\alpha - 1)]^2 p_X(x).$$]{.fragment}
[Depois de algumas √°lgebras obtemos $\sigma^2_X = 1+4\alpha(1-\alpha)$.]{.fragment}

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

O c√°lculo dos desvios quadr√°ticos $(x-\mu)^2$ para obter $\sigma^2$ pode ser tedioso. Felizmente, temos uma f√≥rmula que pode simplificar a sua determina√ß√£o.

:::

:::

## 2.1 Esperan√ßa e vari√¢ncia de vad's {.unnumbered .unlisted}

::: fragment

::: {.callout-warning}
## Propriedades

::: incremental

1. Seja $X$ vad com fmp $p_X(x)$ e esperan√ßa $\mu$. Ent√£o
$$\begin{align*}
\sigma^2 &= \textstyle \sum_{x}(x - \mu)^2\cdot p_X(x) = \sum_{x}(x^2 - 2\mu x + \mu^2)\cdot p_X(x)\\
&= \textstyle \sum_{x}x^2p_X(x) - 2\mu \sum_{x}x p_X(x) + \mu^2 \sum_{x}p_X(x) = E(X^2) - 2\mu\mu + \mu^2 = E(X^2) - \mu^2.
\end{align*}$$
2. Seja $X$ vad com esperan√ßa $\mu_X$ e $Y = aX+b$, com $a, b$ constantes, ent√£o
$$\sigma_{Y}^2 = \sigma_{aX+b}^2 = a^2 \sigma_X^2.$$
:::

:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√µes

::: incremental
- A Propriedade 1 pode ser utilizada para facilmente se chegar a vari√¢ncia do exemplo anterior;
- A Propriedade 2 mostra que a vari√¢ncia s√≥ √© afetada por mudan√ßas em $a$ (escala), e n√£o em $b$ (loca√ß√£o).
:::

:::

:::

## 2.1 Esperan√ßa e vari√¢ncia de vad's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 7:** Seja $X$ o n√∫mero obtido no arremesso de um dado honesto. Um jogo consiste de voc√™ pagar $Y = \frac{1}{X}$ e receber um pr√™mio de $\frac{1}{\mu_X}$ independente do resultado do dado. [Ser√° que o jogo √© vantajoso para o jogador?]{.fragment fragment-index=1}

[Note que $\mu_X = (1+2+\cdots+6)\frac{1}{6} = 3.5$, de onde vem que o pr√™mio √© sempre $\frac{1}{3.5} \approx 0.2857$.]{.fragment fragment-index=2}

[Por outro lado, temos
$$\textstyle \mu_Y = E(Y) = E(\frac{1}{X}) = (1+\frac{1}{2}+\cdots+\frac{1}{6})\frac{1}{6} = \frac{1}{6}\frac{(60+30+20+15+12+10)}{60} = \frac{147}{360} = 0.4083.$$]{.fragment fragment-index=3}

[Assim, isso significa, em m√©dia, pagar $0.4083$ para jogar um jogo que oferece pr√™mio fixo de $0.2857$.]{.fragment fragment-index=4}

:::

::: fragment

::: {.callout-tip}
## Exerc√≠cios

Qual seria a vari√¢ncia do valor pago?
:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 8 {.unnumbered .unlisted}

Uma loja de computadores comprou 3 computadores de certo tipo a R\$ $1000$ cada. Eles ser√£o vendidos a R\$
$1500$ cada. Ap√≥s um per√≠odo, o fabricante aceita os n√£o vendidos por R\$ 400 cada. Seja $X$ o n√∫mero de computadores vendidos e suponha que $p_X(0) = 0.1$, $p_X(1) = 0.2$, $p_X(2)=0.3$ e $p_X(3) = 0.4$. [Note que
$$\mu_X = 0\cdot0.1 + 1\cdot0.2 + 2\cdot0.3 + 3\cdot0.4 = 2$$
e
$$\sigma^2_X = 0^2\cdot0.1 + 1^2\cdot0.2 + 2^2\cdot0.3 + 3^2\cdot0.4 - 2^2 = 0.2 + 4\cdot0.3 + 9\cdot0.4 - 4 = 5 - 4 = 1.$$]{.fragment}

[Por sua vez, o lucro no per√≠odo seria de
$$L = h(X) = 1500X - 3000 + 400(3-X) = 1100X - 1800.$$]{.fragment}

[Logo, o lucro esperado ser√° $\mu_L = 1100\mu_x-1800 = 1100\cdot 2 - 1800 = 400$ e a vari√¢ncia do lucro ser√° de $\sigma_L = 1100^2\sigma^2_X = 1100^2\cdot 1 = 1100^2$. O desvio-padr√£o do lucro √© de $\sigma_L = \sqrt{1100^2} = 1100$.]{.fragment}

## Modelos probabil√≠sticos discretos

::: {.callout-caution}
## Observa√ß√£o

Em muitas situa√ß√µes, lidamos com experimentos cujo resultado √© dicot√¥mico (possui apenas dois resultados) ou pode ser dicotomizado.
:::

::: {.fragment style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 9:** Um exerc√≠cio possui 5 alternativas de resposta. Um estudante decide responder o exerc√≠cio aleatoriamente. Duas poss√≠veis formas de escrever o espa√ßo amostral s√£o:

<table>
<tr class=fragment><td>$\Omega = \{a, b, c, d, e\}$</td> <td>(do ponto de vista de todas alternativas poss√≠veis)</td></tr>
<tr class=fragment style="border-bottom:none;"><td>$\Omega = \{s,f\}$</td> <td>(do ponto de vista de se a alternativa est√° correta ($s$) ou errada $f$)</td></tr>
</table>

[Note que o segundo espa√ßo amostral √© escrita de maneira dicotomizada.]{.fragment}
:::

## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Dizemos que um experimento aleat√≥rio √© um experimento de Bernoulli se seu espa√ßo amostral √© dicot√¥mico e pode ser escrito como $\Omega = \{s,f\}$, em que costumamos denominar $s =$ sucesso e $f =$ fracasso. Normalmente, denotamos a probabilidade de sucesso por $P(\{s\}) = p$, $0\leq p\leq 1$.
:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Num experimento de Bernoulli com $P(\{s\}) = p$, podemos definir a vad
$$X = \begin{cases}
1, & \text{ se } \ \ s;\\
0, & \text{ se } \ \ f.
\end{cases}$$
Nesse caso, dizemos que $X$ tem distribui√ß√£o **Bernoulli** com probabilidade de sucesso $p$ e usamos a nota√ß√£o $X \sim \text{Ber}(p)$.
:::

:::

## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}

::: {.callout-warning}
## Propriedade

Se $X \sim \text{Ber}(p)$, ent√£o

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\mu = E(X) = p$</td> <td>e</td> <td>$\sigma^2 = V(X) = p(1-p).$</td>
  </tr>
</table>

:::

::: {.fragment style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 9 (cont.):** Defina a vad $X$ como $X = 1$, se a alternativa est√° correta, e $X = 0$, se a alternativa est√° errada. Ent√£o $X \sim \text{Ber}(p)$, com $p = \frac{1}{5}$.

:::

::: fragment

::: {.callout-caution}
## Observa√ß√µes

::: incremental

1. No Exemplo 9, definimos como sucesso o aluno acertar a quest√£o, mas esse n√£o precisa ser o caso. A classifica√ß√£o do que ser√° entendido como **sucesso** depender√° do problema em quest√£o.
2. O experimento de Bernoulli pode ser utilizado para constru√ß√£o de vad's mais complexas.

:::

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 9 (cont.) {.unnumbered .unlisted}

Se o estudante responde aleatoriamente uma prova com 4 exerc√≠cios de 5 alternativas cada, tratam-se de 4 realiza√ß√µes **independentes** de um experimento de Bernoulli com $P(\{s\}) = p = \frac{1}{5}$. 

::: {.fragment fragment-index=1}

Defina $X =$ "n√∫mero de exerc√≠cios respondidos corretamente". Note que $\text{Im}(X) = \{0,1,2,3,4\}$. A fmp de $X$ √© dada por

<table>
  <tr>
    <td style="border-top:solid;border-bottom:solid;">$\omega$</td>
    <td style="text-align:center;border-top:solid;border-bottom:solid">$X(\omega)$</td>
    <td style="text-align:center;border-top:solid;border-bottom:solid">$P(\{\omega\})$</td>
    <td style="text-align:center;border-top:none;border-bottom:none;visibility:hidden;">$P(X = x)$</td>
  </tr>
  <tr>
    <td>[$(ffff)$]{.fragment fragment-index=2}</td>
    <td style="text-align:center;">[$0$]{.fragment fragment-index=2}</td>
    <td style="text-align:center;">[$p^0(1-p)^4$]{.fragment fragment-index=2}</td>
    <td style="text-align:center;visibility:hidden;">$p^0(1-p)^4$</td>
  </tr>
  <tr>
    <td>[$(sfff)$, $(fsff)$, $(ffsf)$, $(fffs)$]{.fragment fragment-index=3}</td>
    <td style="text-align:center;">[$1$]{.fragment fragment-index=3}</td>
    <td style="text-align:center;">[$p^1(1-p)^3$]{.fragment fragment-index=3}</td>
    <td style="text-align:center;visibility:hidden;">$4p^1(1-p)^3$</td>
  </tr>
  <tr>
    <td>[$(ssff)$, $(sfsf)$, $(sffs)$, $(fssf)$, $(fsfs)$, $(ffss)$]{.fragment fragment-index=4}</td>
    <td style="text-align:center;">[$2$]{.fragment fragment-index=4}</td>
    <td style="text-align:center;">[$p^2(1-p)^2$]{.fragment fragment-index=4}</td>
    <td style="text-align:center;visibility:hidden;">$6p^2(1-p)^2$</td>
  </tr>
  <tr>
    <td>[$(sssf)$, $(ssfs)$, $(sfss)$, $(fsss)$]{.fragment fragment-index=5}</td>
    <td style="text-align:center;">[$3$]{.fragment fragment-index=5}</td>
    <td style="text-align:center;">[$p^3(1-p)^1$]{.fragment fragment-index=5}</td>
    <td style="text-align:center;visibility:hidden;">$4p^3(1-p)^1$</td>
  </tr>
  <tr style="border-bottom:none">
    <td style="border-bottom:solid">[$(ssss)$]{.fragment fragment-index=6}</td>
    <td style="text-align:center;border-bottom:solid">[$4$]{.fragment fragment-index=6}</td>
    <td style="text-align:center;border-bottom:solid">[$p^4(1-p)^0$]{.fragment fragment-index=6}</td>
    <td style="text-align:center;border-bottom:none;visibility:hidden;">$p^4(1-p)^0$</td>
  </tr>
</table>

:::

[Nesse caso, dizemos que $X$ tem distribui√ß√£o **Binomial** com $4$ repeti√ß√µes e probabilidade de sucesso $\frac{1}{5}$.]{style="visibility:hidden"}

## {.unnumbered .unlisted .smaller}

### Exemplo 9 (cont.) {.unnumbered .unlisted}

Se o estudante responde aleatoriamente uma prova com 4 exerc√≠cios de 5 alternativas cada, tratam-se de 4 realiza√ß√µes **independentes** de um experimento de Bernoulli com $P(\{s\}) = p = \frac{1}{5}$. 

Defina $X =$ "n√∫mero de exerc√≠cios respondidos corretamente". Note que $\text{Im}(X) = \{0,1,2,3,4\}$. A fmp de $X$ √© dada por

<table>
  <tr>
    <td style="border-top:solid;border-bottom:solid;">$\omega$</td>
    <td style="text-align:center;border-top:solid;border-bottom:solid">$X(\omega)$</td>
    <td style="text-align:center;border-top:solid;border-bottom:solid">$P(\{\omega\})$</td>
    <td style="text-align:center;border-top:solid;border-bottom:solid;background-color:lightgray;">$P(X = x)$</td>
  </tr>
  <tr>
    <td>$(ffff)$</td>
    <td style="text-align:center;">$0$</td>
    <td style="text-align:center;">$p^0(1-p)^4$</td>
    <td style="text-align:center;background-color:lightgray;">$p^0(1-p)^4$</td>
  </tr>
  <tr>
    <td>$(sfff)$, $(fsff)$, $(ffsf)$, $(fffs)$</td>
    <td style="text-align:center;">$1$</td>
    <td style="text-align:center;">$p^1(1-p)^3$</td>
    <td style="text-align:center;background-color:lightgray;">$4p^1(1-p)^3$</td>
  </tr>
  <tr>
    <td>$(ssff)$, $(sfsf)$, $(sffs)$, $(fssf)$, $(fsfs)$, $(ffss)$</td>
    <td style="text-align:center;">$2$</td>
    <td style="text-align:center;">$p^2(1-p)^2$</td>
    <td style="text-align:center;background-color:lightgray;">$6p^2(1-p)^2$</td>
  </tr>
  <tr>
    <td>$(sssf)$, $(ssfs)$, $(sfss)$, $(fsss)$</td>
    <td style="text-align:center;">$3$</td>
    <td style="text-align:center;">$p^3(1-p)^1$</td>
    <td style="text-align:center;background-color:lightgray;">$4p^3(1-p)^1$</td>
  </tr>
  <tr style="border-bottom:none">
    <td style="border-bottom:solid">$(ssss)$</td>
    <td style="text-align:center;border-bottom:solid">$4$</td>
    <td style="text-align:center;border-bottom:solid">$p^4(1-p)^0$</td>
    <td style="text-align:center;border-bottom:solid;background-color:lightgray;">$p^4(1-p)^0$</td>
  </tr>
</table>

::: fragment

Nesse caso, dizemos que $X$ tem distribui√ß√£o **Binomial** com $4$ repeti√ß√µes e probabilidade de sucesso $\frac{1}{5}$.

:::

## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Considere $n$ repeti√ß√µes independentes de um experimento de Bernoulli com probabilidade de sucesso $p$ e seja $X =$ "n√∫mero de sucessos nas $n$ repeti√ß√µes". Dizemos que $X$ tem distribui√ß√£o **Binomial** com $n$ repeti√ß√µes e probabilidade de sucesso $p$, denotada por $X \sim \text{Bin}(n,p)$. A fmp de $X$ √©
$$p_X(x) = P(X = x) = {n \choose x} p^x (1-p)^{n-x}, \ \ x=0,1,2,\ldots,n.$$
:::

::: {.callout-caution}
## Observa√ß√£o

Note que, se $X_i \sim \text{Ber}(p)$ √© a va Bernoulli associada √† $i$-√©sima repeti√ß√£o, ent√£o podemos escrever $X = \sum_{i=1}^nX_i$. Isto √©, a va Binomial pode ser vista como soma de $n$ va's Bernoulli indendentes com a mesma probabilidade de sucesso.
:::

## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}

::: {.callout-warning}
## Propriedade

Se $X \sim \text{Bin}(n,p)$, ent√£o

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\mu = E(X) = np$</td> <td>e</td> <td>$\sigma^2 = V(X) = np(1-p).$</td>
  </tr>
</table>

:::

::: {.fragment style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 9 (cont.):** Como a prova tem $n=4$ quest√µes, cada uma com $5$ alternativas ($p=\frac{1}{5}$), temos que $\mu = E(X) = np = 4 \cdot \frac{1}{5} = 0.8$ e $\sigma^2 = V(X) = np(1-p) = 4 \cdot \frac{1}{5} \cdot \frac{4}{5} = 0.64$. Logo, $\sigma = 0.8$. Portanto, em m√©dia, o estudante acerta menos do que 1 das 4 quest√µes.

::: fragment

A probabilidade de ele acertar mais da metade da prova √©
$$\textstyle P(X > 2) = P(X = 3) + P(X = 4) = {4 \choose 3} (\frac{1}{5})^3 (\frac{4}{5})^{1} + {4 \choose 4} (\frac{1}{5})^4 (\frac{4}{5})^{0} = 0.0256 + 0.0016 = 0.0272.$$

:::

:::

## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center' width=960}
:::
:::



## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}
:::
:::


## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}
:::
:::



## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 10:** Um lote cont√©m 10 pe√ßas, sendo 3 defeituosas. Ser√£o sorteadas para inspe√ß√£o 4 pe√ßas do lote. Seja $X =$ "n√∫mero de pe√ßas defeituosas na amostra". Se a amostra resulta em no m√°ximo 1 pe√ßa defeituosa, o lote √© classificado como conforme.

Se a sele√ß√£o √© feita [**com reposi√ß√£o**]{style='color:red;'}, ent√£o estamos contando o n√∫mero de sucessos (pe√ßas defeituosas) em 4 repeti√ß√µes independentes de um experimento de Bernoulli. Assim, $X \sim \text{Bin}(n=4, p=\frac{3}{10})$. Logo, a probabilidade do lote ser conforme √©
$$\textstyle P(X \leq 1) = P(X = 0) + P(X = 1) = {4 \choose 0}(\frac{7}{10})^4 + {4 \choose 1}(\frac{3}{10})(\frac{7}{10})^3 = 0.2401 + 0.4116 = 0.6517.$$

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

No Exemplo 10, a distribui√ß√£o Binomial s√≥ foi apropriada para a vari√°vel $X$ pelo fato da sele√ß√£o ser **com reposi√ß√£o**. Como veremos a seguir, $X$ ter√° outra distribui√ß√£o quando a sele√ß√£o √© **sem reposi√ß√£o**.
:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 11 {.unnumbered .unlisted}

Uma urna cont√©m $M$ objetos do Tipo 1 e $N-M$ do Tipo 2. Desses, ser√£o selecionados $n$ [**sem reposi√ß√£o**]{style="color:red;"}. Seja $X =$ "objetos do Tipo 1 nos $n$ selecionados". [Vamos determinar $p_X(x) = P(X=x)$.]{.fragment fragment-index=1} [Abaixo s√£o apresentadas a quantidade de maneiras que observamos $x$ objetos do Tipo 1 e $n-x$ do Tipo 2:]{.fragment fragment-index=2}

::: {.fragment fragment-index=2}

<table>
  <tr style='border-top:solid;border-bottom:solid'>
    <td style='border-right:solid 1pt'>Objeto</td>
    <td style='border-right:solid 1pt;text-align:center' colspan=4>Tipo 1</td>
    <td style='text-align:center' colspan=4>Tipo 2</td>
  </tr>
  <tr>
    <td style='border-right:solid 1pt'>Retirada</td>
    <td style='border-right:dotted 1pt;text-align:center'>$1$</td>
    <td style='border-right:dotted 1pt;text-align:center'>$2$</td>
    <td style='border-right:dotted 1pt;text-align:center'>$\cdots$</td>
    <td style='border-right:solid 1pt;text-align:center'>$x$</td>
    <td style='border-right:dotted 1pt;text-align:center'>$x+1$</td>
    <td style='border-right:dotted 1pt;text-align:center'>$x+2$</td>
    <td style='border-right:dotted 1pt;text-align:center'>$\cdots$</td>
    <td style='text-align:center'>$n$</td>
  </tr>
  <tr style='border-top:solid 1pt;border-bottom:solid'>
    <td style='border-right:solid 1pt'>Maneiras</td>
    <td style='border-right:dotted 1pt;text-align:center'>$M$</td>
    <td style='border-right:dotted 1pt;text-align:center'>$M-1$</td>
    <td style='border-right:dotted 1pt;text-align:center'>$\cdots$</td>
    <td style='border-right:solid 1pt;text-align:center'>$M-x+1$</td>
    <td style='border-right:dotted 1pt;text-align:center'>$N-M$</td>
    <td style='border-right:dotted 1pt;text-align:center'>$N-M-1$</td>
    <td style='border-right:dotted 1pt;text-align:center'>$\cdots$</td>
    <td style='text-align:center'>$N-M-n+x+1$</td>
  </tr>
</table>

:::

[Da regra do produto, temos $\frac{M!}{(M-x)!}\frac{(N-M)!}{(N-M-(n-x))!}$.]{.fragment fragment-index=3} [Como a ordem em que os objetos s√£o selecionados n√£o importa, precisamos dividir esse valor pelas $x!$ permuta√ß√µes dos objetos do Tipo 1 e pelas $(n-x)!$ do Tipo 2, resultando em ${M \choose x} {N-M \choose n-x}$ maneiras favor√°veis a $\{X = x\}$.]{.fragment fragment-index=4} [Sem nenhuma restri√ß√£o, obtemos $N(N-1)\cdots(N-n+1) = \frac{N!}{(N-n)!}$.]{.fragment fragment-index=5} [Descontando as $n!$ permuta√ß√µes, h√° ${N \choose n}$ elementos em $\Omega$.]{.fragment fragment-index=6} [Assim,
$$p_X(x) = P(X = x) = \frac{ {M \choose x} {M-M \choose n-x} }{ {N \choose n} }, \ \ \max\{0, n-(N-M)\} \leq x \leq \min\{n,M\}.$$]{.fragment fragment-index=7}

## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Sejam $N$ itens, $M$ do Tipo 1 e $N-M$ do Tipo 2, dos quais $n$ s√£o selecionados **sem reposi√ß√£o**. Seja $X =$ "itens do Tipo 1 na amostra". Dizemos que $X$ tem distribui√ß√£o hipergeom√©trica com par√¢metros $N$ (total), $M$ (itens do Tipo 1) e $n$ (tamanho da amostra) e denotamos por $X \sim \text{Hip}(N,M,n)$. A fmp de $X$ √©
$$p_X(x) = \frac{ {M \choose x} {N-M \choose n-x} }{ {N \choose n} }, \ \ \max\{0, n-(N-M)\} \leq x \leq \min\{n,M\}.$$
:::

::: fragment

::: {.callout-warning}
## Propriedade

Se $X \sim \text{Hip}(N,M,n)$ e $p = \frac{M}{N}$ √© a propor√ß√£o de itens do Tipo 1, ent√£o

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\mu = E(X) = np$</td> <td>e</td> <td>$\sigma^2 = V(X) = np(1-p)(\frac{N-n}{N-1}).$</td>
  </tr>
</table>

:::

:::

## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 10 (cont.):** Se a sele√ß√£o fosse realizada [**sem reposi√ß√£o**]{style="color:red;"}, ent√£o $X \sim \text{Hip}(N=10, M=3, n=4)$, logo a probabilidade do lote ser conforme √©
$$P(X \leq 1) = P(X=0) + P(X=1) = \frac{ {3 \choose 0} {7 \choose 4} }{ {10 \choose 4} } + \frac{ {3 \choose 1} {7 \choose 3} }{ {10 \choose 4} } = 0.1667 + 0.5 = 0.6667.$$

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Note o efeito do tipo de sele√ß√£o em algumas caracter√≠sticas da vad $X$:

<table style="margin-bottom: 12pt !important;">
  <tr style='border-top:solid;border-bottom:solid'>
    <td style='border-right:solid 1pt;'>Sele√ß√£o</td>
    <td>$P(X \leq 1)$</td>
    <td>$E(X)$</td>
    <td>$V(X)$</td>
  </tr>
  <tr>
    <td style='border-right:solid 1pt;'>Com reposi√ß√£o</td>
    <td>$0.6517$</td>
    <td>$1.2\ \ [np]$</td>
    <td>$0.84\ \ [np(1-p)]$</td>
  </tr>
  <tr style='border-bottom:solid;'>
    <td style='border-right:solid 1pt;'>Sem reposi√ß√£o</td>
    <td>$0.6667$</td>
    <td>$1.2 \ \ [np]$</td>
    <td>$0.56 \ \ [np(1-p)(\frac{N-n}{N-1})]$</td>
  </tr>
</table>

:::

:::

## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}



::: {.cell layout-ncol="2" layout-align="center" fig.ncol='2'}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=576}
:::

::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-4-2.png){fig-align='center' width=576}
:::
:::


## {.unnumbered .unlisted .smaller}

### Exemplo 12 {.unnumbered .unlisted}

Um sistema de transa√ß√µes **independentes** possui 3 computadores que operam de modo redundante:

- se o 1¬∫ apresenta falha de transa√ß√£o (com probabilidade $p$), o 2¬∫ entra em opera√ß√£o;
- se o 2¬∫ apresenta falha de transa√ß√£o (com probabilidade $p$), o 3¬∫ entra em opera√ß√£o;
- se o 3¬∫ apresenta falha de transa√ß√£o (com probabilidade $p$), o sistema √© interrompido;

[Tratam-se de repeti√ß√µes independentes de um experimento de Bernoulli com $P(\{s\}) = p$ at√© obter o 3¬∫ sucesso.]{.fragment fragment-index=1} [Seja $X =$ "n√∫mero de transa√ß√µes realizadas". Note que $\text{Im}(X) = \{3,4,\ldots\}$.]{.fragment fragment-index=2} [Veja a seguinte tabela:]{.fragment fragment-index=3}

::: {.fragment fragment-index=3}

<table>
  <tr style="border-top:solid;border-bottom:solid">
    <td style="text-align:left;border-right:solid 1pt;">$\omega$</td>
    <td style="text-align:center;border-right:solid 1pt;">$x$</td>
    <td style="text-align:center;border-right:solid 1pt;">$P(\{\omega\})$</td>
    <td style="text-align:center;">$P(X=x)$</td>
  </tr>
  <tr>
    <td style="text-align:left;border-right:solid 1pt;">$(sss)$</td>
    <td style="text-align:center;border-right:solid 1pt;">$3$</td>
    <td style="text-align:center;border-right:solid 1pt;">$p^3$</td>
    <td style="text-align:center;">$p^3$</td>
  </tr>
  <tr>
    <td style="text-align:left;border-right:solid 1pt;">$(ssfs)$, $(sfss)$, $(fsss)$</td>
    <td style="text-align:center;border-right:solid 1pt;">$4$</td>
    <td style="text-align:center;border-right:solid 1pt;">$(1-p)p^3$</td>
    <td style="text-align:center;">$3(1-p)p^3$</td>
  </tr>
  <tr>
    <td style="text-align:left;border-right:solid 1pt;">$(ssffs)$, $(sfsfs)$, $(fssfs)$, $(sffss)$, $(fsfss)$, $(ffsss)$</td>
    <td style="text-align:center;border-right:solid 1pt;">$5$</td>
    <td style="text-align:center;border-right:solid 1pt;">$(1-p)^2p^3$</td>
    <td style="text-align:center;">$6(1-p)^2p^3$</td>
  </tr>
  <tr style="border-bottom:solid">
    <td style="text-align:left;border-right:solid 1pt;">$\vdots$</td>
    <td style="text-align:center;border-right:solid 1pt;">$\vdots$</td>
    <td style="text-align:center;border-right:solid 1pt;">$\vdots$</td>
    <td style="text-align:center;">$\vdots$</td>
  </tr>
</table>

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 12 (cont.) {.unnumbered .unlisted}

Assim, notando que a √∫ltima repeti√ß√£o tem que ser $s$, um exemplo de $\omega$ favor√°vel a $[X = x]$ √©
$$\omega = (\underbrace{ssff\ldots f}_{x-1}s),$$
cuja probabilidade √© $P(\{\omega\}) = (1-p)^{x-3}p^3$. [Os outros resultados favor√°veis a $[X=x]$ t√™m a mesma probabilidade e s√£o contados verificando de quantas formas podemos posicionar os dois sucessos dentre as primeiras $x-1$ repeti√ß√µes.]{.fragment fragment-index=1} [Para o 1¬∫ $s$, temos $x-1$ posi√ß√µes e, para o 2¬∫ $s$, temos $x-2$ posi√ß√µes.]{.fragment fragment-index=2}

[Pela regra da multiplica√ß√£o ter√≠amos $(x-1)(x-2) = \frac{(x-1)!}{(x-3)!}$ maneiras poss√≠veis.]{.fragment fragment-index=3} [Como a ordem que escolhemos a posi√ß√£o n√£o importa (escolher 1 e 2 √© o mesmo que 2 e 1, por exemplo), precisamos descontar as $2!$ permuta√ß√µes poss√≠veis entre as duas, o que resulta em $\frac{(x-1)!}{2!(x-3)!} = { x-1 \choose 2 }$ maneiras poss√≠veis.]{.fragment fragment-index=4} [Assim
$$P(X = x) = { x-1 \choose 2 } (1-p)^{x-3} p^x, \ \ \ x = 3, 4, 5, \ldots.$$]{.fragment fragment-index=5}

## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Um experimento de Bernoulli com $P(\{s\}) = p$ √© repetido **independentemente** at√© a ocorr√™ncia do $k$-√©simo sucesso. Seja $X =$ "n√∫mero de repeti√ß√µes realizadas". Dizemos que $X$ tem distribui√ß√£o Binomial Negativa com $k$ sucessos e probabilidade de sucesso $p$, denotada por $X \sim \text{BN}(k,p)$. A fmp de $X$ √©
$$p_X(x) = P(X = x) = { x-1 \choose k-1 } (1-p)^{x-k} p^k, \ \ \ x = k, k+1, k+2, \ldots.$$
:::

::: fragment

::: {.callout-warning}
## Propriedades

Se $X \sim \text{BN}(k,p)$, ent√£o

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\mu = E(X) = \frac{k}{p}$</td> <td>e</td> <td>$\sigma^2 = V(X) = k\frac{(1-p)}{p^2}.$</td>
  </tr>
</table>
:::

:::

## 2.2 Modelos probabil√≠sticos discretos {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√µes

::: incremental

1. No Exemplo 12, temos que $X \sim \text{BN}(k=3, p)$. Se $p = 0.01$, por exemplo, ter√≠amos $\mu = E(X) = \frac{3}{0.01} = 300$, $\sigma^2 = V(X) = 3\frac{0.99}{0.01^2} = 29700$ e $\sigma \approx 172.34$. Em m√©dia, s√£o realizadas $300$ transa√ß√µes at√© a terceira falha com DP de $172.34$. Al√©m disso, $P(X > \mu) = P(X > 300) \approx 0.4221$;
2. A distribui√ß√£o BN tamb√©m pode ser definida como a distribui√ß√£o do n√∫mero de fracassos (em vez de repeti√ß√µes) at√© o $k$-√©simo sucesso. Nesse caso, sua fmp √©
$$\textstyle p_X(x) = { x + k -1 \choose k-1 } (1-p)^{x} p^k, \ \ \ x = 0, 1, 2, \ldots,$$
e sua esperan√ßa e vari√¢ncia s√£o, respectivamente, $\mu = k\frac{(1-p)}{p}$ e $\sigma^2 = k\frac{(1-p)}{p^2}$;
3. Se $X \sim \text{BN}(k = 1, p)$, dizemos que $X$ tem distribui√ß√£o geom√©trica, cuja nota√ß√£o √© $X \sim \text{Geo}(p)$;
4. Se $X \sim \text{BN}(k, p)$, ent√£o $X = \sum_{j=1}^k X_j$, em que $X_1,\ldots,X_k \sim \text{Geo}(p)$ independentes.
5. Bin: repeti√ß√µes fixas e sucessos aleat√≥rios; BN: sucessos fixos e repeti√ß√µes aleat√≥rias.

:::

:::

# Vari√°veis aleat√≥rias cont√≠nuas

## 3 Vari√°veis aleat√≥rias cont√≠nuas {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Anteriormente, definimos uma vac $X$ como sendo aquelas cuja imagem $\text{Im}(X)$ √© n√£o enumer√°vel. De agora em diante, as vac's ser√£o restringidas √†quelas em que a fda √© [**absolutamente cont√≠nua**]{style="color:red"}.
:::

::: {.callout-note}
## Defini√ß√£o

Diremos que $X$ √© va **cont√≠nua** (vac) se sua imagem $\text{Im}(X)$ √© n√£o enumer√°vel e se sua fda $F_X(x) = P(X \leq x)$ √© **absolutamente cont√≠nua**. Neste caso, podemos escrever
$$F_X(x) = \int_{-\infty}^x f_X(t)dt,$$
em que $f_X(\cdot)$ √© denominada **fun√ß√£o densidade de probabilidade** (fdp).
:::

## 3 Vari√°veis aleat√≥rias cont√≠nuas {.unnumbered .unlisted}

::: {.callout-warning}
## Propriedades
Seja $X$ vac com fda $F_X(\cdot)$ e fdp $f_X(\cdot)$, ent√£o:

::: incremental

1. sua fdp satisfaz:
    + $f_X(\cdot) \geq 0$;
    + $\int_{-\infty}^\infty f_X(x)dx = 1$.
2. a probabilidade do intervalo $(a,b]$, $a<b$, √© dada por
$$\textstyle P(a < X \leq b) = F_X(b) - F_X(a) = \int_a^b f_X(x) dx;$$
3. a probabilidade de um ponto $x$ √© $P(X=x) = 0$;
4. valem as igualdades $P(a < X \leq b) = P(a \leq X \leq b) = P(a \leq X < b) = P(a < X < b)$, sendo que o mesmo n√£o √© v√°lido no caso discreto.

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 13 {.unnumbered .unlisted}

Tempo de avan√ßo √© a diferen√ßa entre o instante em que um carro termina de passar por um ponto e o instante em que o pr√≥ximo carro come√ßa a passar por esse ponto. Seja $X =$ "tempo de avan√ßo para dois carros escolhidos ao acaso". O artigo "*The Statistical Properties of Freeway Traffic*" sugeriu a fdp
$$f_X(x) = 0.15e^{-0.15(x-0.5)} \mathbb{I}(x \geq 0.5) = \begin{cases}
0.15e^{-0.15(x-0.5)}, & x \geq 0.5;\\
0, & x < 0.5,
\end{cases}$$
em que $\mathbb{I}(A) = 1$, se a condi√ß√£o $A$ √© v√°lida, e $\mathbb{I}(A) = 0$, caso contr√°rio.

::: fragment

Note que $f_X(x) \geq 0$ e que $\int_{-\infty}^\infty f_X(x)dx = 1$. Vamos encontrar $P(X > 5)$. Note que
$$P(X > 5) = \int_{5}^\infty f_X(x) dx = \int_{5}^\infty 0.15e^{-0.15(x-0.5)} dx = \left[\left.-e^{-0.15(x-0.5)}\right|_5^\infty\right] = e^{-0.675} \approx 0.5092.$$

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 13 (cont.) {.unnumbered .unlisted}

Veja abaixo a representa√ß√£o da fdp e a da probabilidade $P(X > 5)$:


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=768}
:::
:::


## 3 Vari√°veis aleat√≥rias cont√≠nuas {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

√â importante refor√ßar que podemos obter a fdp $f_X(\cdot)$ da fda $F_X(\cdot)$ e vice-versa, por meio de
$$\textstyle f_X(x) = \frac{d}{dx}F_X(x)\ \ \ \ \text{e} \ \ \ \ F_X(x) = \int_{-\infty}^x f_X(t)dt.$$
:::

::: {.fragment style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):** No caso, dada a fdp, podemos obter a fda como a antiderivada de $f_X(x)$
[$$\textstyle F_X(x) = \int_{-\infty}^{0.5}0dt + \int_{0.5}^x 0.15e^{-0.15(t-0.5)}dt = 0 + \left[\left.-e^{-0.15(t-0.5)}\right|_{0.5}^x\right] = 1-e^{-0.15(x-0.5)}.$$]{.fragment}
[Agora, a fda pode ser utilizada para obter a probabilidade anterior:
$$\textstyle P(X > 5) = 1- P(X \leq 5) = 1- F_X(5) = 1-[1-e^{-0.15(5-0.5)}] = e^{-0.675} \approx 0.5092.$$]{.fragment}
:::

## 3 Vari√°veis aleat√≥rias cont√≠nuas {.unnumbered .unlisted}


::: {.callout-caution}
## Observa√ß√£o

A inversa de $F_X(x)$ pode ser utilizada para encontrar os quantis.
:::

::: {.fragment style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):** Vimos que $F_X(x) = 1-e^{-0.15(x-0.5)}$. Vamos determinar a mediana de $X$, denotada por $\tilde{\mu}_X$. Para $\tilde{\mu}_X$ ser a mediana, devemos ter $F_X(\tilde{\mu}_X) = 0.5$. [Logo
$$1-e^{-0.15(\tilde{\mu}_X-0.5)} = 0.5 \Rightarrow e^{-0.15(\tilde{\mu}_X-0.5)} = 0.5 \Rightarrow \tilde{\mu}_X = 0.5 -\frac{20}{3}\log(0.5) \approx 5.121.$$]{.fragment}
:::

::: fragment

::: {.callout-tip}
## Exerc√≠cio

Encontre o quantil de ordem $p$, denotado por $Q_p$, isto √©, o valor tal que $F_X(Q_p) = p$.
:::

:::

## Esperan√ßa e vari√¢ncia de vac's

::: {.callout-caution}
## Observa√ß√£o

Em Estat√≠stica Descritiva, ao tratar de distribui√ß√µes intervalares, uma aproxima√ß√£o para a m√©dia era
$$\textstyle \bar{x} \approx \sum_{i}x_i^{(m)} f_i = \sum_{i}x_i^{(m)} d_i h_i,$$
em que $x_i^{(m)}$, $f_i$, $d_i$ e $h_i$ s√£o, respectivamente, o ponto m√©dio, a frequ√™ncia relativa, a [**densidade**]{style="color:red"} e o [**comprimento**]{style="color:red"} do intervalo $i$. Como a fdp $f_X(x)$ √© um modelo te√≥rico para as densidades $d_i$ no intervalo de comprimento infinitesimal $dx$ em torno de $x$, isso motiva a seguinte defini√ß√£o de esperan√ßa.
:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Seja $X$ vac com fdp $f_X(x)$, ent√£o a sua esperan√ßa √© definida por
$$\textstyle \mu = \mu_X = E(X) = \int_{-\infty}^\infty xf_X(x)dx.$$
:::

:::

## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {.callout-warning}
## Propriedade

Da mesma forma que no caso discreto, se $X$ √© vac e $Y = h(X)$, ent√£o
$$\textstyle \mu_Y = E(Y) = E[h(X)] = \int_{-\infty}^\infty h(x) f_X(x) dx.$$
:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

A propriedade acima contorna a necessidade de obter $f_Y(y)$ para calcular $\mu_Y$ via $\mu_Y = \int_{-\infty}^\infty y f_Y(y)dy$.
:::

:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Seja $X$ vac com fdp $f_X(x)$, ent√£o a sua vari√¢ncia √© definida por
$$\textstyle \sigma^2 = \sigma^2_X = V(X) = E[(X-\mu)^2] = \int_{-\infty}^\infty (x-\mu)^2f_X(x)dx.$$
:::

:::

## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {.callout-warning}
## Propriedade

Da mesma forma que no caso discreto, se $X$ √© vac, temos a seguinte forma alternativa para a vari√¢ncia:
$$\textstyle \sigma^2 = E(X^2) - \mu^2, \ \ \ \ \  E(X^2) = \int_{-\infty}^\infty x^2 f_X(x) dx.$$
:::

:::fragment

::: {.callout-warning}
## Propriedades

Da mesma forma que no caso discreto, se $X$ √© vac e $a$ e $b$ s√£o constantes, ent√£o:

::: incremental

1. $\mu_{aX+b} = E(aX + b) = aE(X) + b = a\mu_X + b$;
2. $\sigma_{aX+b}^2 = V(aX + b) = a^2V(X) = a^2\sigma_X^2$;
3. $\sigma_{aX+b} = DP(aX + b) = aDP(X) = a\sigma_X$.

:::

:::

:::

## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. [Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$.]{style='color:white'} [Portanto,]{style='color:white'}
$$\begin{align*}
\color{white}{\mu} &\color{white}{=}{\textstyle \color{white}{\int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx}\color{white}{=\int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}}\\
&\color{white}{=}{\textstyle \color{white}{\frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds}\color{white}{= \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}}\\
\end{align*}$$

[Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$.]{style='color:white;'} [Logo,]{style='color:white;'}
$${\textstyle\color{white}{\mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\}}\color{white}{= 0.5 + \frac{20}{3}[0 + 1]}\color{white}{= \frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}}$$

[Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.]{style='color:white;'}

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::


## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. [Portanto,]{style='color:white'}
$$\begin{align*}
\color{white}{\mu} &\color{white}{=}{\textstyle \color{white}{\int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx}\color{white}{=\int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}}\\
&\color{white}{=}{\textstyle \color{white}{\frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds}\color{white}{= \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}}\\
\end{align*}$$

[Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$.]{style='color:white;'} [Logo,]{style='color:white;'}
$${\textstyle\color{white}{\mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\}}\color{white}{= 0.5 + \frac{20}{3}[0 + 1]}\color{white}{= \frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}}$$

[Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.]{style='color:white;'}

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::



## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. Portanto, 
$$\begin{align*}
\mu &={\textstyle \int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx\color{white}{=\int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}}\\
&\color{white}{=}{\textstyle \color{white}{\frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds}\color{white}{= \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}}\\
\end{align*}$$

[Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$.]{style='color:white;'} [Logo,]{style='color:white;'}
$${\textstyle\color{white}{\mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\}}\color{white}{= 0.5 + \frac{20}{3}[0 + 1]}\color{white}{= \frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}}$$

[Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.]{style='color:white;'}

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::




## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. Portanto, 
$$\begin{align*}
\mu &={\textstyle \int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx = \int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}\\
&\color{white}{=}{\textstyle \color{white}{\frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds}\color{white}{= \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}}\\
\end{align*}$$

[Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$.]{style='color:white;'} [Logo,]{style='color:white;'}
$${\textstyle\color{white}{\mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\}}\color{white}{= 0.5 + \frac{20}{3}[0 + 1]}\color{white}{= \frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}}$$

[Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.]{style='color:white;'}

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::



## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. Portanto, 
$$\begin{align*}
\mu &={\textstyle \int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx = \int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}\\
&={\textstyle \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds\color{white}{= \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}}\\
\end{align*}$$

[Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$.]{style='color:white;'} [Logo,]{style='color:white;'}
$${\textstyle\color{white}{\mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\}}\color{white}{= 0.5 + \frac{20}{3}[0 + 1]}\color{white}{= \frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}}$$

[Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.]{style='color:white;'}

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::



## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. Portanto, 
$$\begin{align*}
\mu &={\textstyle \int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx = \int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}\\
&={\textstyle \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds = \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}\\
\end{align*}$$

[Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$.]{style='color:white;'} [Logo,]{style='color:white;'}
$${\textstyle\color{white}{\mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\}}\color{white}{= 0.5 + \frac{20}{3}[0 + 1]}\color{white}{= \frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}}$$

[Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.]{style='color:white;'}

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::



## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. Portanto, 
$$\begin{align*}
\mu &={\textstyle \int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx = \int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}\\
&={\textstyle \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds = \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}\\
\end{align*}$$

Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$. [Logo,]{style='color:white;'}
$${\textstyle\color{white}{\mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\}}\color{white}{= 0.5 + \frac{20}{3}[0 + 1]}\color{white}{= \frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}}$$

[Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.]{style='color:white;'}

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::


## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. Portanto, 
$$\begin{align*}
\mu &={\textstyle \int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx = \int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}\\
&={\textstyle \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds = \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}\\
\end{align*}$$

Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$. Logo
$${\textstyle \mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\}\color{white}{= 0.5 + \frac{20}{3}[0 + 1]}\color{white}{=\frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}}$$

[Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.]{style='color:white;'}

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::



## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. Portanto, 
$$\begin{align*}
\mu &={\textstyle \int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx = \int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}\\
&={\textstyle \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds = \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}\\
\end{align*}$$

Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$. Logo
$${\textstyle \mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\} = 0.5 + \frac{20}{3}[0 + 1]\color{white}{=\frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}}$$

[Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.]{style='color:white;'}

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::


## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. Portanto, 
$$\begin{align*}
\mu &={\textstyle \int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx = \int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}\\
&={\textstyle \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds = \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}\\
\end{align*}$$

Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$. Logo
$${\textstyle \mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\} = 0.5 + \frac{20}{3}[0 + 1] = \frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}$$

[Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.]{style='color:white;'}

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::


## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. Portanto, 
$$\begin{align*}
\mu &={\textstyle \int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx = \int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}\\
&={\textstyle \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds = \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}\\
\end{align*}$$

Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$. Logo
$${\textstyle \mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\} = 0.5 + \frac{20}{3}[0 + 1] = \frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}$$

Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.

:::

::: {style='visibility:hidden;'}

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

:::


## 3.1 Esperan√ßa e vari√¢ncia de vac's {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 13 (cont.):**

Vamos calcular $\mu$. Tomando $s = 0.15(x - 0.5)$, ent√£o $0.15x = s+0.15\cdot0.5$ e $ds = 0.15dx$. Portanto, 
$$\begin{align*}
\mu &={\textstyle \int_{-\infty}^{0.5} x\cdot 0 dx + \int_{0.5}^{\infty} x\cdot 0.15e^{-0.15(x-0.5)} dx = \int_{0}^\infty (s+0.15\cdot0.5)e^{-s}\frac{ds}{0.15}}\\
&={\textstyle \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5\int_{0}^\infty e^{-s}ds = \frac{1}{0.15}\int_{0}^\infty se^{-s}ds + 0.5.}\\
\end{align*}$$

Na integral ap√≥s a √∫ltima igualdade, fazendo $u = s$ e $dv = e^{-s}ds$, vem que $du=ds$ e $v = -e^{-s}$. Logo
$${\textstyle \mu = 0.5 + \frac{1}{0.15}\{[-\frac{s}{e^s}|_0^\infty] + \int_0^{\infty}e^{-s}ds\} = 0.5 + \frac{20}{3}[0 + 1] = \frac{3 + 40}{6} = \frac{43}{6} \approx 7.1667.}$$

Note que obtivemos $\mu = 7.1667 > \tilde\mu = 5.121$, o que reflete a assimetria √† direita da fdp.

:::

::: {.callout-tip}
## Exerc√≠cios

Calcule $\sigma^2$ para a vac $X$ do Exemplo 13.
:::

## Modelos probabil√≠sticos cont√≠nuos

::: {.callout-caution}
## Observa√ß√£o

Sem d√∫vidas, em termos pr√°ticos, o modelo probabil√≠stico mais importante para vari√°veis aleat√≥rias √© o n normal. Algumas de suas caracter√≠sticas ser√£o discutidas mais adiante.
:::

::: {.callout-note}
## Defini√ß√£o

Seja $X$ vac. Dizemos que $X$ tem distribui√ß√£o normal com m√©dia $\mu$ e vari√¢ncia $\sigma^2$ e escrevemos $X \sim \text{N}(\mu,\sigma^2)$ se sua fdp for dada por
$$f_X(x) = f_X(x; \mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}, \ \ \ \ \ \ -\infty <x < \infty, \ \ -\infty < \mu < \infty, \ \ \sigma^2 > 0.$$
:::


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=768}
:::
:::


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-warning}
## Propriedades

Se $X \sim \text{N}(\mu, \sigma^2)$, ent√£o
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$E(X) = \mu$</td> <td>e</td> <td>$V(X) = \sigma^2.$</td>
  </tr>
</table>
:::

::: fragment

::: {.callout-caution}
## Observa√ß√µes
Se $X \sim \text{N}(\mu, \sigma^2)$, ent√£o:

::: incremental

1. a sua fdp √© completamente especificada por sua m√©dia $\mu$ e sua vari√¢ncia $\sigma$;
2. √© √≥bvio que, $f_X(x) \geq 0$, mas a demonstra√ß√£o que $\int_{-\infty}^\infty f_X(x)dx = 1$ requer t√©cnicas de integra√ß√£o via coordenadas polares;
3. a opera√ß√£o definida por $Z = \frac{X - \mu}{\sigma}$ √© denominada [**padroniza√ß√£o**]{style='color:red;'} de $X$ e podemos mostrar que $Z \sim \text{N}(0,1)$, caso em que dizemos que $Z$ tem distribui√ß√£o [**normal padr√£o**]{style='color:red;'}.

:::

:::

:::


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√µes

Se $X \sim \text{N}(\mu, \sigma^2)$, ent√£o:

::: incremental

1. a probabilidade intervalar $P(a < X \leq b)$ deveria ser obtida por
$${\textstyle P(a < X \leq b) = \int_a^b \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} dx},$$
que n√£o possui solu√ß√£o anal√≠tica, exceto se $a = -\infty$ e $b = \infty$;
2. poder√≠amos, a princ√≠pio, utilizar m√©todos num√©ricos para obt√™-la, mas para cada $\mu$ e $\sigma^2$ diferentes precisar√≠amos executar o c√≥digo novamente;
3. podemos utilizar a **padroniza√ß√£o** para constatar que
$${\textstyle P(a < X \leq b) = P(\frac{a-\mu}{\sigma} < \frac{X-\mu}{\sigma} \leq \frac{b-\mu}{\sigma}) = P(\frac{a-\mu}{\sigma} < Z \leq \frac{b-\mu}{\sigma})}.$$

:::

:::

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 14:** Seja $X =$ "tempo de rea√ß√£o de um motorista √†s luzes de freio do carro a frente". Um artigo na *Ergonomics* (1993, p. 391-395) sugere que $X \sim \text{N}(1.25, 0.46^2)$. [Determinemos $P(1 \leq X \leq 1.75)$:
$$
\begin{align*}
P(1 \leq X \leq 1.75) &= {\textstyle P(\frac{1-\mu}{\sigma} \leq \frac{X-\mu}{\sigma} \leq \frac{1.75-\mu}{\sigma}) = P(\frac{1-1.25}{0.46} \leq Z \leq \frac{1.75-1.25}{0.46})}\\
&= {\textstyle P(\frac{1-1.25}{0.46} \leq Z \leq \frac{1.75-1.25}{0.46}) = P(-0.54 \leq Z \leq 1.09)}\\
&= {\textstyle P(Z \leq 1.09) - P(Z \leq -0.54) = \Phi(1.09) - \Phi(-0.54),}
\end{align*}
$$
em que $\Phi(z) = P(Z \leq z)$ √© uma nota√ß√£o especial para representar a fda da normal padr√£o $Z$.]{.fragment}
::: 

::: fragment

::: {.callout-caution}
## Observa√ß√µes

::: incremental

1. Note que passamos de um problema de calcular as probabilidades com respeito a $X \sim \text{N}(\mu, \sigma^2)$, com $\mu$ e $\sigma^2$ quaisquer, para as probabilidades de uma **normal padr√£o** $Z \sim \text{N}(0,1)$;
2. As probabilidades para $Z \sim \text{N}(0,1)$ tamb√©m n√£o tem solu√ß√£o anal√≠tica, mas m√©todos num√©ricos foram utilizados para computar e tabelar $\Phi(z)$ para diversos valores de $z$.

:::

:::

:::

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=768}
:::
:::


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

<figure>
  <img src="figs/fig_normal_padrao1.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

<figure>
  <img src="figs/fig_normal_padrao2.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

<figure>
  <img src="figs/fig_normal_padrao3.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-8-1.png){fig-align='center' width=768}
:::
:::


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

<figure>
  <img src="figs/fig_normal_padrao2_1.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

<figure>
  <img src="figs/fig_normal_padrao2_2.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

<figure>
  <img src="figs/fig_normal_padrao2_3.png" style="display: block;margin-left: auto;margin-right: auto;width: 70%;">
</figure>

## {.unnumbered .unlisted .smaller}

### Exemplo 14 (cont.) {.unnumbered .unlisted}

Vimos que $P(1 \leq X \leq 1.75) = P(-0.54 \leq Z \leq 1.09) = \Phi(1.09) - \Phi(-0.54)$. [Consultando a tabela da normal padr√£o, temos]{.fragment fragment-intex=1}

::: columns

::: {.column #vcenter width=50%}

::: {.fragment fragment-intex=2}

<figure>
  <img src="figs/figura_normal_padrao_ex14_1.png" style="display: block;margin-left: auto;margin-right: auto;width: 95%;">
  <figcaption style='text-align:center'>$\Phi(-0.54) = 0.2946$</figcaption>
</figure>

:::

:::

::: {.column #vcenter width=50%}

::: {.fragment fragment-intex=3}

<figure>
  <img src="figs/figura_normal_padrao_ex14_2.png" style="display: block;margin-left: auto;margin-right: auto;width: 95%;">
  <figcaption style='text-align:center'>$\Phi(1.09) = 0.8621$</figcaption>
</figure>

:::

:::

:::

[Assim,
$$P(1 \leq X \leq 1.75) = P(-0.54 \leq Z \leq 1.09) = \Phi(1.09) - \Phi(-0.54) = 0.8621 - 0.2946 = 0.5675.$$]{.fragment fragment-intex=4}


## {.unnumbered .unlisted .smaller}

### Exemplo 14 (cont.) {.unnumbered .unlisted}

Suponha que desejamos $P(X > 1.75)$. [Utilizando a padroniza√ß√£o, temos
$${\textstyle P(X > 1.75) = P(\frac{X-\mu}{\sigma} > \frac{1.75-1.25}{0.46}) = P(Z > 1.09) = 1 - P(Z \leq 1.09) = 1 - \Phi(1.09).}$$]{.fragment fragment-intex=1}

::: columns

::: {.column #vcenter width=60%}

::: {.fragment fragment-intex=2}

Consultando a tabela da normal padr√£o, temos

<figure>
  <img src="figs/figura_normal_padrao_ex14_2.png" style="display: block;margin-left: auto;margin-right: auto;width: 99%;">
  <figcaption style='text-align:center'>$\Phi(1.09) = 0.8621$</figcaption>
</figure>

:::

:::

::: {.column #vcenter width=40%}

::: {.fragment fragment-intex=3}

Logo,
$$\begin{align*}
P(X > 1.75) &= 1 - \Phi(1.09)\\
&= 1 - 0.8621\\
&= 0.1379.
\end{align*}$$

:::

:::

:::

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Muitas vezes, √© necess√°rio determinar o quantil de ordem $p$ da va $X$, denotado por $Q_{X,p}$, cuja defini√ß√£o √© $P(X \leq Q_{X,p}) = p$. Vejas as Figuras abaixo.
:::

::: {style='visibility:hidden'}


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=768}
:::
:::


:::

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Muitas vezes, √© necess√°rio determinar o quantil de ordem $p$ da va $X$, denotado por $Q_{X,p}$, cuja defini√ß√£o √© $P(X \leq Q_{X,p}) = p$. Vejas as Figuras abaixo.
:::



::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=768}
:::
:::


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Muitas vezes, √© necess√°rio determinar o quantil de ordem $p$ da va $X$, denotado por $Q_{X,p}$, cuja defini√ß√£o √© $P(X \leq Q_{X,p}) = p$. Vejas as Figuras abaixo.
:::



::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=768}
:::
:::



## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Muitas vezes, √© necess√°rio determinar o quantil de ordem $p$ da va $X$, denotado por $Q_{X,p}$, cuja defini√ß√£o √© $P(X \leq Q_{X,p}) = p$. Vejas as Figuras abaixo.
:::



::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=768}
:::
:::


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 14 (cont.):** Suponha que desejamos encontrar o valor que corresponde ao tempo de rea√ß√£o dos $70\%$ motoristas que reagem mais rapidamente. Na verdade, estamos interessados no quantil de ordem $p=0.7$, isto √©, $Q_{X,0.7}$. Para encontrarmos, note que
$${\textstyle 0.7 = P(X \leq Q_{X,0.7}) = P(\frac{X - \mu}{\sigma} \leq \frac{Q_{X,0.7} - \mu}{\sigma})} = P(Z \leq Q_{Z,0.7}),$$
em que $Q_{Z,0.7} = \frac{Q_{X,0.7} - \mu}{\sigma} = \frac{Q_{X,0.7} - 1.25}{0.46}$, ou ainda, $Q_{X,0.7} = 1.25 + 0.46 Q_{Z,0.7}$.

:::

::: {.callout-caution}
## Observa√ß√£o

Note que, novamente, o problema passa do quantil de uma normal qualquer para o quantil de uma normal padr√£o. Para encontrar $Q_{Z,0.7}$, necessitamos consultar a tabela normal padr√£o de maneira inversa.

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 14 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/figura_normal_padrao_ex14_2_1.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

## {.unnumbered .unlisted .smaller}

### Exemplo 14 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/figura_normal_padrao_ex14_2_2.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

## {.unnumbered .unlisted .smaller}

### Exemplo 14 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/figura_normal_padrao_ex14_2_3.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

[Assim, conclu√≠mos que $0.52 < Q_{Z,0.7} < 0.53$. Podemos usar interpola√ß√£o para aproximar $Q_{Z, 0.7}$.]{.fragment fragment-index=1} [Note que $Q_{Z,0.6985} = 0.52$ e $Q_{Z,0.7019} = 0.53$]{.fragment fragment-index=2}. [Agora, admitindo que $Q_{Z,p}$ tem uma rela√ß√£o aproximadamente linear para $0.6985 \leq p \leq 0.7019$, temos que $Q_{Z,p} = a+bp$, com as restri√ß√µes]{.fragment fragment-index=3}

:::{.fragment fragment-index=3}

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$0.52 = a+0.6985b$</td> <td>([**Eq. 1**]{style='color:red;'})</td> <td></td> <td></td> <td>e</td> <td></td> <td></td> <td>$0.53 = a+0.7019b$.</td> <td>([**Eq. 2**]{style='color:red;'})</td>
  </tr>
</table>

:::

[Pela 1¬™ equa√ß√£o, $a = 0.52 - 0.6985b$.]{.fragment fragment-index=4} [Substituindo na 2¬™ equa√ß√£o, vem $b = \frac{0.53 - 0.52}{0.7019 - 0.6985} \approx 2.9412$.]{.fragment fragment-index=5} [Retornando na 1¬™ equa√ß√£o, temos $a = 0.52 - 0.6985\cdot 2.9412 \approx -1.5344$.]{.fragment fragment-index=6} [Finalmente, temos
$$Q_{Z,0.7} = a+0.7b = -1.5344 + 0.7 \cdot 2.9412 \approx 0.5244 \color{white}{\Rightarrow Q_{X,0.7} = 1.25 + 0.46 Q_{Z,0.7} = 1.4912.}$$]{.fragment fragment-index=7}


## {.unnumbered .unlisted .smaller}

### Exemplo 14 (cont.) {.unnumbered .unlisted}

<figure>
  <img src="figs/figura_normal_padrao_ex14_2_3.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

Assim, conclu√≠mos que $0.52 < Q_{Z,0.7} < 0.53$. Podemos usar interpola√ß√£o para aproximar $Q_{Z, 0.7}$. Note que $Q_{Z,0.6985} = 0.52$ e $Q_{Z,0.7019} = 0.53$. Agora, admitindo que $Q_{Z,p}$ tem uma rela√ß√£o aproximadamente linear para $0.6985 \leq p \leq 0.7019$, temos que $Q_{Z,p} = a+bp$, com as restri√ß√µes

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$0.52 = a+0.6985b$</td> <td>([**Eq. 1**]{style='color:red;'})</td> <td></td> <td></td> <td>e</td> <td></td> <td></td> <td>$0.53 = a+0.7019b$.</td> <td>([**Eq. 2**]{style='color:red;'})</td>
  </tr>
</table>

Pela 1¬™ equa√ß√£o, $a = 0.52 - 0.6985b$. Substituindo na 2¬™ equa√ß√£o, vem $b = \frac{0.53 - 0.52}{0.7019 - 0.6985} \approx 2.9412$. Retornando na 1¬™ equa√ß√£o, temos $a = 0.52 - 0.6985\cdot 2.9412 \approx -1.5344$. Finalmente, temos
$$Q_{Z,0.7} = a+0.7b = -1.5344 + 0.7 \cdot 2.9412 \approx 0.5244 \color{red}{\Rightarrow Q_{X,0.7} = 1.25 + 0.46 Q_{Z,0.7} = 1.4912.}$$ 
**Conclus√£o:** 70% dos motoristas apresentam tempo de rea√ß√£o inferior a 1,4912 seg.


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-warning}
## Propriedades

De modo geral, podemos encontrar o quantil $Q_{Z,p}$ de uma normal padr√£o por interpola√ß√£o da seguinte forma. Sejam $p_L$ e $p_U$, respectivamente, a maior e a menor probabilidades tabeladas tais que $p_L < p < p_U$ e $Q_{Z,p_L}$ e $Q_{Z,p_U}$ os respectivos quantis, ent√£o
$$Q_{Z,p} \approx a + bp,$$
em que
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$b = \frac{p_U - p_L}{Q_{Z,p_U} - Q_{Z,p_L}}$</td> <td>e</td> <td>$a = p_L - bQ_{Z,p_L}$.</td>
  </tr>
</table>

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

No Exemplo 14, obtivemos $P_L = 0.52$, $P_U = 0.7019$, $Q_{Z,p_L} = 0.6985$ e $Q_{Z,p_U} = 0.7019$.
:::

:::

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√µes

::: incremental

1. Em infer√™ncia, veremos um dos teoremas mais importantes para utiliza√ß√£o em aplica√ß√µes, o **Teorema Central do Limite** (TCL). Basicamente, o TCL afirma que, em grandes amostras, a m√©dia amostral tem distribui√ß√£o aproximadamente normal.
2. Como a m√©dia amostral √© soma de va's dividida pelo tamanho da amostra, √© natural, imaginar que, se uma va $X$ pode ser enxergada como soma de v√°rias va's, ent√£o a sua distribui√ß√£o tamb√©m ser√° bem aproximada por uma normal. Esse √© o caso da distribui√ß√£o Binomial (soma de Bernoulli's), por exemplo.

:::

:::

::: fragment

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 15:** Dos motoristas de um estado, 25% n√£o t√™m seguro. Seja $X =$ "quantidade sem seguro numa amostra de tamanho 50". [Se $s =$ "motorista n√£o ter seguro", ent√£o $X \sim \text{Bin}(n = 50, p = 0.25)$.]{.fragment} [Vamos determinar a probabilidade de 20% ou menos da amostra n√£o ter seguro:]{.fragment} [
$$\textstyle P(\frac{X}{n} \leq 0.2) = P(X \leq 10) = \sum_{x=0}^{10} p_X(x) = \sum_{x=0}^{10} {50 \choose x} 0.25^x0.75^{50-x} \approx 0.2622.$$]{.fragment}

:::

:::

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√µes

::: incremental

1. Note que, no Exemplo 15, o c√°lculo do valor exato de $P(X \leq 10)$ envolveu o c√°lculo de 11 probabilidades da distribui√ß√£o binomial, o que pode ser exaustivo se realizado sem aux√≠lio computacional;
2. A alternativa que surge √© utilizar $Y \sim \text{N}(\mu, \sigma^2)$ como aproxima√ß√£o para $X \sim \text{Bin}(n,p)$;
3. Os par√¢metros $\mu$ e $\sigma^2$ da distribui√ß√£o de $Y$ utilizada para a aproxima√ß√£o s√£o $\mu = \mu_X = np$ e $\sigma^2 = np(1-p)$, que s√£o a m√©dia e a vari√¢ncia da $X \sim \text{Bin}(n,p)$;
4. Tendo em vista que $X$ pode ser representada como soma de $n$ va's $\text{Ber}(p)$, o TCL garante que a aproxima√ß√£o do Item 2 fica melhor a medida que $n$ aumenta;
5. Para ilustra√ß√£o, os gr√°ficos a seguir mostram a fmp de $X \sim \text{Bin}(n,p)$ com a fdp de $Y \sim \text{N}(\mu = np, \sigma^2 = np(1-p))$, para $p=0.05$ e $0.25$, e $n = 5, 10, 50$ e $100$.

:::

:::

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}



::: {.cell layout-ncol="2"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-13-1.png){width=100%}
:::

::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-13-2.png){width=100%}
:::
:::


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}


::: {.cell layout-ncol="2"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-14-1.png){width=100%}
:::

::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-14-2.png){width=100%}
:::
:::



## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}


::: {.cell layout-ncol="2"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-15-1.png){width=100%}
:::

::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-15-2.png){width=100%}
:::
:::


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}


::: {.cell layout-ncol="2"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-16-1.png){width=100%}
:::

::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-16-2.png){width=100%}
:::
:::




## {.unnumbered .unlisted .smaller}

### Exemplo 15 (cont.) {.unnumbered .unlisted}

Nesse caso, temos $\mu_X = np = 50 \cdot 0.25 = 12.5$ e $\sigma_X^2 = np(1-p) = 50 \cdot 0.25 \cdot 0.75 = 9.375$. Vamos ilustrar a aproxima√ß√£o de $P(X \leq 10)$ por meio de $Y \sim \text{N}(12.5, 9.375)$. Veja a figura abaixo.


## {.unnumbered .unlisted .smaller}

### Exemplo 15 (cont.) {.unnumbered .unlisted}

Nesse caso, temos $\mu_X = np = 50 \cdot 0.25 = 12.5$ e $\sigma_X^2 = np(1-p) = 50 \cdot 0.25 \cdot 0.75 = 9.375$. Vamos ilustrar a aproxima√ß√£o de $P(X \leq 10)$ por meio de $Y \sim \text{N}(12.5, 9.375)$. Veja a figura abaixo.


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=1056}
:::
:::



## {.unnumbered .unlisted .smaller}

### Exemplo 15 (cont.) {.unnumbered .unlisted}

Nesse caso, temos $\mu_X = np = 50 \cdot 0.25 = 12.5$ e $\sigma_X^2 = np(1-p) = 50 \cdot 0.25 \cdot 0.75 = 9.375$. Vamos ilustrar a aproxima√ß√£o de $P(X \leq 10)$ por meio de $Y \sim \text{N}(12.5, 9.375)$. Veja a figura abaixo.


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=1056}
:::
:::


## {.unnumbered .unlisted .smaller}

### Exemplo 15 (cont.) {.unnumbered .unlisted}

Nesse caso, temos $\mu_X = np = 50 \cdot 0.25 = 12.5$ e $\sigma_X^2 = np(1-p) = 50 \cdot 0.25 \cdot 0.75 = 9.375$. Vamos ilustrar a aproxima√ß√£o de $P(X \leq 10)$ por meio de $Y \sim \text{N}(12.5, 9.375)$. Veja a figura abaixo.


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=1056}
:::
:::


## {.unnumbered .unlisted .smaller}

### Exemplo 15 (cont.) {.unnumbered .unlisted}

Assim, a ideia √© realizar a aproxima√ß√£o
$$\textstyle P(X \leq 10) \approx P(Y \leq 10.5) = P(Z \leq \frac{10.5 - 12.5}{\sqrt{9.375}}) = P(Z \leq -0.65) = 0.2578.$$

<figure>
  <img src="figs/figura_normal_padrao_ex15_3.png" style="display: block;margin-left: auto;margin-right: auto;width: 80%;">
</figure>

## {.unnumbered .unlisted .smaller}

### Exemplo 15 (cont.) {.unnumbered .unlisted}

Nesse caso, temos $\mu_X = np = 50 \cdot 0.25 = 12.5$ e $\sigma_X^2 = np(1-p) = 50 \cdot 0.25 \cdot 0.75 = 9.375$. Vamos ilustrar a aproxima√ß√£o de $P(X \leq 10)$ por meio de $Y \sim \text{N}(12.5, 9.375)$. Veja a figura abaixo.


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=1056}
:::
:::


## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-tip}
## Exerc√≠cios

Considerando as condi√ß√µes do Exemplo 15 e utilizando $Y \sim \text{N}(12.5, 9.375)$, aproxime as seguintes probabilidades:

1. $P(X < 10)$;
2. $P(X = 10)$;
3. $P(X > 10)$;
4. $P(X \geq 10)$.
:::

::: {.callout-caution}
## Observa√ß√µes

1. √â poss√≠vel notar que a va normal tem fdp sim√©trica em torno de $mu$ com formato de sino (a fdp $f_X(x)$ decresce conforme $x$ se afasta de $\mu$).
2. Em muitas situa√ß√µes pr√°ticas, a vac √© positiva e possui assimetria. Uma fam√≠lia de fdp com essas caracter√≠sticas √© a fam√≠lia **gama**.
:::

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Dizemos que $X$ tem distribui√ß√£o Gama com par√¢metros $\alpha$ e $\beta$, se sua fdp √© dada por
$$\textstyle f_X(x) = \frac{1}{\beta^\alpha\Gamma(\alpha)}x^{\alpha - 1}e^{-x/\beta}, \ \ \ \ x > 0, \ \ \ \ \alpha,\beta > 0,$$
em que $\Gamma(\alpha) = \int_0^\infty z^{\alpha-1}e^{-z}dz$ √© a fun√ß√£o gama. Nesse caso, utilizamos a nota√ß√£o $X \sim \text{Gama}(\alpha, \beta)$.
:::

::: fragment

::: {.callout-warning}
## Propriedades

A fun√ß√£o gama possui as seguintes propriedades:

::: incremental

1. Para qualquer $\alpha > 1$, $\Gamma(\alpha) = (\alpha - 1)\Gamma(\alpha - 1)$;
2. Se $n \in \mathbb{N}$, ent√£o $\Gamma(n) = (n-1)!$;
3. $\Gamma(\frac{1}{2}) = \pi$.

:::

:::

:::

## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}


::: {.cell layout-align="center"}
::: {.cell-output-display}
![](variaveis-aleatorias_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=768}
:::
:::



## 3.2 Modelos probabil√≠sticos cont√≠nuos {.unnumbered .unlisted}

::: {.callout-warning}
## Propriedades

Se $X \sim \text{Gama}(\alpha, \beta)$, ent√£o
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$E(X) = \alpha\beta$</td> <td>e</td> <td>$V(X) = \alpha\beta^2.$</td>
  </tr>
</table>

:::

# Vetores Aleat√≥rios Bidimensionais

## Covari√¢ncia e Correla√ß√£o

# Fun√ß√µes de Vari√°veis Aleat√≥rias

# FIM {.unnumbered .unlisted}

# APAGAR DPS 1 {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

:::

::: {.callout-note}
## Defini√ß√£o

:::

::: {.callout-warning}
## Propriedades

:::

::: {.callout-tip}
## Exerc√≠cios

:::

::: {.callout-important}
## Problema

:::


::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo:** 

::: 

# APAGAR DPS 2 {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=45%}
1
:::

::: {.column #vcenter width=55%}
2

3

4
:::

:::




```{=latex}
\begin{align*}
\Omega &= \{(1,1), (1,2), \ldots, (4,4)\}\\
       &= \{(x,y) : x,y = 1,\ldots,4\}.
\end{align*}
```



# FIM {.unnumbered .unlisted}

