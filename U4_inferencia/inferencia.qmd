---
title: "Probabilidade e Estat√≠stica"
subtitle: "Infer√™ncia"
author: "Prof. Dr. Alessandro JQ Sarnaglia"
lang: "pt"
format:
  revealjs:
    width: 1280
    height: 720
    min-scale: 0.05
    fig-width: 5.5
    fig-height: 5.5
    theme: [custom.scss]
    css: [fonts.css, style.css]
    callout-icon: false
    toc: true
    toc-depth: 3
    toc-expand: 3
    number-sections: true
    number-depth: 3
    footer: "[T√≥picos üîô](../index.html) | [Sum√°rio üìã](inferencia.html#/TOC)"
    menu:
      useTextContentForMissingTitles: false
editor: source
filters:
  - parse-latex
---


# Distribui√ß√µes Amostrais

## Fun√ß√µes de Vari√°veis Aleat√≥rias

::: {.callout-caution}
## Observa√ß√£o

Muitas vezes √© de interesse determinar a distribui√ß√£o de uma fun√ß√£o de va's $X_1, \ldots, X_n$.
:::

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1:** Um sistema de transa√ß√µes √© composto por dois componentes id√™nticos independentes. Quando o primeiro falha (com probabilidade $p$), √© imediatamente substituido pelo outro. Seja $X_i$ o n√∫mero de transa√ß√µes at√© a falha do componente $i$. Ent√£o $X_i\sim\text{Geo}(p)$. Qual a distribui√ß√£o de $Y = X_1 + X_2$?

::: 

::: {.callout-note}
## Defini√ß√£o - Caso discreto

Sejam $X_1, \ldots, X_n$ vad's com fmp conjunta $p(x_1, \ldots, x_n)$ e $Y = h(X_1, \ldots, X_n)$ uma fun√ß√£o dessas vari√°veis. A fmp de $Y$ √© dada por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\displaystyle p_Y(y) = \underset{x_i\ :\ h(x_1,\ldots, x_n)\ =\ y}{\sum \cdots \sum} p(x_1, \ldots, x_n)$.</td>
  </tr>
</table>
:::

## 1.1 Fun√ß√µes de Vari√°veis Aleat√≥rias {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 6px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1 (cont):** Note que $Y$ representa a quantidade de total de transa√ß√µes. Pela independ√™ncia de $X_1$ e $X_2$, temos que $p(x_1,x_2) = (1-p)^{x_1-1}p(1-p)^{x_2-1}p = (1-p)^{x_1+x_2-2}p^2$. Logo,
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$p_Y(y)$ </td> <td> $=$ </td> <td> ${\displaystyle\underset{x_1,\ x_2\ :\ x_1\ +\ x_2\ =\ y}{\sum \sum} p(x_1, x_2) = \sum_{x_1 = 1}^{y-1} \sum_{x_2 = y-x_1}^{y-x_1} (1-p)^{x_1+x_2-2}p^2 = \sum_{x_1 = 1}^{y-1} (1-p)^{y-2}p^2}$</td>
  </tr>
  <tr style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> ${\displaystyle (y-1)(1-p)^{y-2}p^2 = {y-1 \choose 1} (1-p)^{y-2}p^2}$.</td>
  </tr>
</table>
Portanto, $Y \sim \text{BN}(2,p)$.
:::

::: {.callout-caution}
## Observa√ß√£o

O Exemplo 1 pode ser generalizado. Mais precisamente, se $X_1, \ldots, X_k$ s√£o vad's independentes com $X_i \sim \text{Geo}(p)$ e $Y = \sum_{i=1}^k X_i$, ent√£o √© poss√≠vel mostrar que $Y \sim \text{BN}(k, p)$. Isto √©, soma de va's geom√©tricas independentes tem distribui√ß√£o binomial negativa.
:::

## 1.1 Fun√ß√µes de Vari√°veis Aleat√≥rias {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 6px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 2:** Uma pessoa chega no caixa em um tempo $X_1$ ap√≥s o instante 0 e uma segunda $X_2$ instantes ap√≥s a primeira. Se $X_1$ e $X_2$ s√£o independentes e $X_i \sim \text{Exp}(\lambda)$, qual √© distribui√ß√£o de $Y = X_1 + X_2$?

:::

::: {.callout-note}
## Defini√ß√£o - Caso cont√≠nuo

Sejam $X_1, \ldots, X_n$ vac's com fdp conjunta $f(x_1, \ldots, x_n)$ e $Y = h(X_1, \ldots, X_n)$ uma fun√ß√£o dessas vari√°veis. A fdp de $Y$ √© dada por $f_Y(y) = \frac{d}{dy}F_Y(y)$ em que $F_Y(y)$ √© a fda de $Y$ definida por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\displaystyle F_Y(y) = P(Y \leq y) = P(h(X_1, \ldots, X_n) \leq y) = \underset{x_i\ :\ h(x_1,\ldots, x_n)\ \leq\ y}{\int \cdots \int} f(x_1, \ldots, x_n) dx_1 \cdots dx_n$.</td>
  </tr>
</table>
:::

::: {.callout-caution}
## Observa√ß√£o

Pela defini√ß√£o, n√£o obtemos diretamente $f_Y(y)$. Devemos obter $F_Y(y)$ e ent√£o calcular $f_Y(y) = \frac{d}{dy}F_Y(y)$.
:::

## 1.1 Fun√ß√µes de Vari√°veis Aleat√≥rias {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 6px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 2 (cont.):** Note que $Y$ o momento em que o segundo cliente chega √† loja. Pela independ√™ncia de $X_1$ e $X_2$, temos que $f(x_1,x_2) = \lambda e^{-\lambda x_1} \lambda e^{-\lambda x_2} = \lambda^2 e^{-\lambda (x_1 + x_2)}$. Logo,
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$F_Y(y)$ </td> <td> $=$ </td> <td> ${\underset{x_1\ +\ x_2\ \leq\ y}{\int \int} f(x_1, x_2) dx_1 dx_2 = \int_{0}^{y} \int_{0}^{y-x_2} \lambda^2 e^{-\lambda (x_1 + x_2)} dx_1 dx_2 = \int_{0}^{y} \lambda e^{-\lambda x_2} [-e^{-\lambda x_1}|_{0}^{y-x_2}] dx_2}$</td>
  </tr>
  <tr style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> ${ \int_{0}^{y} \lambda e^{-\lambda x_2} [1-e^{-\lambda (y-x_2)}] dx_2 = \int_{0}^{y} \lambda e^{-\lambda x_2} - \lambda e^{-\lambda y} dx_2 = [-e^{-\lambda x_2} - \lambda x_2 e^{-\lambda y}|_{0}^{y}]}$</td>
  </tr>
  <tr style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> ${-e^{-\lambda y} - \lambda ye^{-\lambda y} + 1 - 0}$.</td>
  </tr>
</table>
Agora, derivando $F_Y(y)$, obtemos
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$f_Y(y) = \frac{d}{dy}F_Y(y) = \frac{d}{dy}[-e^{-\lambda y} - \lambda ye^{-\lambda y} + 1] = \lambda e^{-\lambda y} - \lambda e^{-\lambda y} + \lambda^2 y e^{-\lambda y} = \frac{1}{(\frac{1}{\lambda})^2\Gamma(2)}y^{2-1}e^{-\frac{y}{\lambda^{-1}}}$. </td>
  </tr>
</table>
Portanto, $Y \sim \text{Gama}(2, \lambda^{-1})$.
:::

::: {.callout-caution}
## Observa√ß√£o

O Ex. 2 pode ser extendido. Se $X_1, \ldots, X_n$ s√£o vac's independentes com $X_i \sim \text{Exp}(\lambda)$ e $Y = \sum_{i=1}^n X_i$, ent√£o $Y \sim \text{Gama}(n, \lambda^{-1})$. Isto √©, soma de exponenciais independentes tem distribui√ß√£o gama.
:::

## Distribui√ß√£o Amostral

::: {.callout-caution}
## Observa√ß√µes

1. As defini√ß√µes anteriores nos permitem obter a distribui√ß√£o de fun√ß√µes de va's de maneira gen√©rica. Existem diversas ferramentas que podem facilitar essa tarefa: convolu√ß√£o, fun√ß√µes geradora de momentos e caracter√≠stica, m√©todo do jacobiano, entre outras. Elas n√£o ser√£o vistas aqui;
2. No contexto de infer√™ncia, dois conceitos importantes s√£o o de amostra aleat√≥ria e de estat√≠stica.
:::

::: {.callout-note}
## Defini√ß√£o - Amostra Aleat√≥ria

Seja $X$ va e $X_1, \ldots, X_n$ va's independentes com a mesma distribui√ß√£o que $X$. Dizemos que $X_1, \ldots, X_n$ √© uma **amostra aleat√≥ria** (aa) de $X$. Nesse caso, podemos dizer que $X$ descreve a vari√°vel na **popula√ß√£o**.
:::

::: {.callout-note}
## Defini√ß√£o - Estat√≠stica

Seja $X_1, \ldots, X_n$ aa de $X$. Uma estat√≠stica $T$ √© qualquer fun√ß√£o $T = T(X_1, \ldots, X_n)$ da referida amostra.
:::

## 1.2 Distribui√ß√£o Amostral {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o - Distribui√ß√£o Amostral

Seja $X_1, \ldots, X_n$ aa de $X$ e $T = T(X_1, \ldots, X_n)$ uma estat√≠stica. Dizemos que a fda $F_T(t) = P(T \leq t) = P(T(X_1,\ldots, X_n) \leq t)$ √© a **distribui√ß√£o amostral** de $T$.
:::

::: {.callout-caution}
## Observa√ß√µes

1. O termo distribui√ß√£o amostral tamb√©m poder√° ser utilizado para nos referirmos a fmp $p_T(t)$ ou a fdp $f_T(t)$ da estat√≠stica $T$ se ela for discreta ou cont√≠nua, respectivamente;
2. As generaliza√ß√µes dos Exemplos 1 e 2 se tratavam de aa's das popula√ß√µes $X \sim \text{Geo}(p)$ e $X \sim \text{Exp}(\lambda)$, respectivamente. A estat√≠stica considerada nos dois exemplos era o **total amostral** $Y = \sum_i X_i$;
3. Por esses exemplos, fica claro que a distribui√ß√£o amostral de uma estat√≠stica depende da distribui√ß√£o da popula√ß√£o $X$: binomial negativa no Exemplo 1; e gama no Exemplo 2.
:::

## 1.2 Distribui√ß√£o Amostral {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Se a distribui√ß√£o populacional e/ou se a estat√≠stica for muito complicada, a obten√ß√£o anal√≠tica da distribui√ß√£o populacional pode ser muito dif√≠cil. Nesses casos, podemos utilizar m√©todos de **Monte Carlo** (t√©cnica computacional) para obter uma aproxima√ß√£o da distribui√ß√£o amostral da estat√≠stica de interesse.
:::

::: {style="border-style: solid; border-width: 3px; padding-top: 12px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 3:** Seja $X_1, \ldots, X_n$ aa de $X \sim \text{Weibull}(10, 2)$, isto √© $\alpha=10$ e $\beta=2$. Considere a estat√≠stica
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\tilde{X} = T(X_1, \ldots, X_n) = \begin{cases} \frac{X_{(n/2)} + X_{(n/2+1)}}{2}, & n \text{ par}; \\ X_{(n+1)/2}, & n \text{ par}. \end{cases}$. </td>
  </tr>
</table>
Isto √©, $\tilde{X}$ √© a mediana amostral. Qual a distribui√ß√£o amostral de $\tilde{X}$ se $n=20$ e $n=40$? A estat√≠stica e a distribui√ß√£o populacional s√£o complexas para uma an√°lise anal√≠tica. Podemos usar m√©todos de Monte Carlo.
:::

## {.unnumbered .unlisted .smaller}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=65%}

```{.r code-line-numbers="|1-8|9-15|11-12|13-14|16-17|18-21|18-19|20-21|22-23"}
# par√¢metros da Weibull(a,b)
a <- 10; b<- 2
# Quantidade de r√©plicas
R <- 1000
# Tamanho da aa
n <- 20
# Vetor vazio para armazenar as medianas amostrais de cada r√©plica
med <- NULL
# Inicia as r√©plicas de Monte Carlo
for(r in 1:R){
  # Gera uma aa de tamanho n da Weibull(a, b)
  x <- rweibull(n=n, shape = a, scale = b)
  # Calcula a mediana amostral da amostra x
  med[r] <- median(x)                      
}
# Mediana populacional
med_pop <- b*(log(2))^(1/a)
# Histograma da distribui√ß√£o amostral da mediana amostral
hist(med)
# Tra√ßa a mediana da popula√ß√£o e intervalo +0.1 ou -0.1 desse valor
abline(v = med_pop + c(-.1,0,.1), col = 'red', lty = c(2,1,2), lwd = 2)
# Prop. das medianas amostrais entre med_pop - 0.1 e med_pop + 0.1
mean(med > med_pop-.1 & med < med_pop+.1)
```

:::

::: {.column #vcenter width=35%}

::: fragment

```{r}

a <- 10; b<- 2 # par√¢metros da Weibull(a,b)
R <- 1000      # Quantidade de r√©plicas
n <- 20        # Tamanho da aa
med <- NULL    # Vetor vazio para armazenar as medianas
               # amostrais de cada r√©plica
for(r in 1:R){ # Inicia as r√©plicas de Monte Carlo
  x <- rweibull(n=n, shape = a, scale = b) # Gera uma aa de tamanho n
                                           # da Weibull(a, b)
  med[r] <- median(x)                      # Calcula a mediana amostral
}
med_pop <- b*(log(2))^(1/a) # Mediana populacional
hist(med)                   # Histograma da distribui√ß√£o
                            # amostral da mediana amostral
abline(v = med_pop + c(-.1,0,.1),
       col = 'red', lty = c(2,1,2),
       lwd = 2) # Tra√ßa a mediana da popula√ß√£o e intervalo
                # +0.1 ou -0.1 desse valor
prp <- mean(med > med_pop-.1 & med < med_pop+.1)
cat("P[Med. Amost. \u2208 (Med. Pop. \u00b1 0.1)] \u2248 ", prp, sep='') 
```

:::

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=65%}

```{.r code-line-numbers="|5-6"}
# par√¢metros da Weibull(a,b)
a <- 10; b<- 2
# Quantidade de r√©plicas
R <- 1000
# Tamanho da aa
n <- 40
# Vetor vazio para armazenar as medianas amostrais de cada r√©plica
med <- NULL
# Inicia as r√©plicas de Monte Carlo
for(r in 1:R){
  # Gera uma aa de tamanho n da Weibull(a, b)
  x <- rweibull(n=n, shape = a, scale = b)
  # Calcula a mediana amostral da amostra x
  med[r] <- median(x)                      
}
# Mediana populacional
med_pop <- b*(log(2))^(1/a)
# Histograma da distribui√ß√£o amostral da mediana amostral
hist(med)
# Tra√ßa a mediana da popula√ß√£o e intervalo +0.1 ou -0.1 desse valor
abline(v = med_pop + c(-.1,0,.1), col = 'red', lty = c(2,1,2), lwd = 2)
# Prop. das medianas amostrais entre med_pop - 0.1 e med_pop + 0.1
mean(med > med_pop-.1 & med < med_pop+.1)
```

:::

::: {.column #vcenter width=35%}

::: fragment

```{r}

a <- 10; b<- 2 # par√¢metros da Weibull(a,b)
R <- 1000      # Quantidade de r√©plicas
n <- 40        # Tamanho da aa
med <- NULL    # Vetor vazio para armazenar as medianas
               # amostrais de cada r√©plica
for(r in 1:R){ # Inicia as r√©plicas de Monte Carlo
  x <- rweibull(n=n, shape = a, scale = b) # Gera uma aa de tamanho n
                                           # da Weibull(a, b)
  med[r] <- median(x)                      # Calcula a mediana amostral
}
med_pop <- b*(log(2))^(1/a) # Mediana populacional
hist(med)                   # Histograma da distribui√ß√£o
                            # amostral da mediana amostral
abline(v = med_pop + c(-.1,0,.1),
       col = 'red', lty = c(2,1,2),
       lwd = 2) # Tra√ßa a mediana da popula√ß√£o e intervalo
                # +0.1 ou -0.1 desse valor
prp <- mean(med > med_pop-.1 & med < med_pop+.1)
cat("P[Med. Amost. \u2208 (Med. Pop. \u00b1 0.1)] \u2248 ", prp, sep='') 
```

:::

:::

:::

## Distribui√ß√£o Amostral da M√©dia Amostral

::: {.callout-caution}
## Observa√ß√£o

Embora a distribui√ß√£o amostral dependa da distribui√ß√£o da popula√ß√£o, no caso da estat√≠stica **m√©dia amostral**, $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$, o Teorema Central do Limite nos fornece a sua [**distribui√ß√£o amostral limite**]{style='color:red;'}, independente da distribui√ß√£o populacional.
:::

::: {.callout-warning}
## Propriedade - Teorema Central do Limite (TCL)

Seja $X_1, \ldots, X_n$ aa de $X$, onde $E(X) = \mu$ e $V(X) = \sigma^2 < \infty$. Ent√£o a estat√≠stica
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle Z_n = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}}$ </td>
  </tr>
</table>
tem distribui√ß√£o aproximadamente normal padr√£o. A aproxima√ß√£o fica melhor com o aumento de $n$.
:::

## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}


::: {.callout-caution}
## Observa√ß√µes

1. A m√©dia amostral tem distribui√ß√£o aproximadamente $\text{N}(\mu, \frac{\sigma^2}{n})$, em que $\mu$ e $\sigma^2$ s√£o a m√©dia e a vari√¢ncia da popula√ß√£o $X$ da qual a amostra ser√° observada;
2. A aproxima√ß√£o se torna melhor com o aumento do tamanho amostral $n$;
3. O valor de $n$ necess√°rio para que a aproxima√ß√£o seja ''boa'' depende da distribui√ß√£o da popula√ß√£o;
4. Por exemplo, dados provenientes de popula√ß√µes assim√©tricas exigem o valor de $n$ maior.
:::

::: {.callout-warning}
## Propriedades - Corol√°rio do TCL

Seja $X_1, \ldots, X_n$ aa de $X$, onde $E(X) = \mu$ e $V(X) = \sigma^2 < \infty$ e $S_n = \sum_{i=1}^n X_i$ o total amostral. Ent√£o
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle Z_n = \frac{S_n - n\mu}{\sigma\sqrt{n}}}$ </td>
  </tr>
</table>
tem distribui√ß√£o aproximadamente normal padr√£o.
:::

## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=65%}

```{.r code-line-numbers="|1|2-3|4|5-11|12|13-19"}
R <- 1000; nn <- c(15, 30, 100)
a <- 0; b <- 10
mu <- (a+b)/2; sig2 <- (b-a)^2/12
medias <- matrix(NA, ncol=length(nn), nrow=R)
for(i in 1:length(nn)){
  n <- nn[i]
  for(r in 1:R){
    x <- runif(n = n, min = 0, max = 10)
    medias[r, i] <- mean(x)
  }
}
rng <- range(medias); xx <- seq(rng[1], rng[2], length.out=1000)
for(i in 1:length(nn)){
  par(family = "serif", mar=c(5,4,1,2))
  hist(medias[, i], probability=TRUE, xlim = rng, xlab = '', main='', ylab='')
  mtext(text = paste('n = ',nn[i],sep=''), side = 1, line = 2)
  mtext(text = 'Densidade', side = 2, line = 2)
  lines(xx, dnorm(xx, mu, sqrt(sig2/nn[i])), col='red', lwd=2)
}
```

:::

::: {.column #vcenter width=35%}

```{r}
a <- 0; b <- 10
x <- seq(a-0.05*(b-a), b+0.05*(b-a), length.out=1000)
fx <- dunif(x, a, b)
par(family = "serif", mar=c(5,4,1,2))
plot(x, fx, type='n', xlab='', ylab='', main='', lwd=3,
     ylim=range(fx)+c(-.1*diff(range(fx)),.13*diff(range(fx))),
     xlim=range(x)+c(-.02*diff(range(x)),.08*diff(range(x))))
arrows(y0 = min(fx)-.1*diff(range(fx)), y1 = max(fx)+.13*diff(range(fx)), x0=0, x1=0, length = .1)
arrows(x0 = min(x)-.02*diff(range(x)), x1 = max(x)+.08*diff(range(x)), y0=0, y1=0, length = .1)
lines(x[x > b], fx[x > b], lwd=3)
lines(x[x < a], fx[x < a], lwd=3)
lines(c(a,a), c(0, 1/(b-a)), lwd=3, lty=2)
lines(c(b,b), c(0, 1/(b-a)), lwd=3, lty=2)
lines(x[x >= a & x <= b], fx[x >= a & x <= b], lwd=3)
points(c(a,a,b,b), c(0, 1/(b-a), 0, 1/(b-a)), cex=1.5, pch=21, bg=c('white','black', 'white','black'))
```

:::

:::

## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

```{r fig.ncol=3}
R <- 1000; nn <- c(15, 30, 100)
a <- 0; b <- 10
mu <- (a+b)/2; sig2 <- (b-a)^2/12
medias <- matrix(NA, ncol=length(nn), nrow=R)
for(i in 1:length(nn)){
  n <- nn[i]
  for(r in 1:R){
    x <- runif(n = n, min = 0, max = 10)
    medias[r, i] <- mean(x)
  }
}
rng <- range(medias); xx <- seq(rng[1], rng[2], length.out=1000)
for(i in 1:length(nn)){
  par(family = "serif", mar=c(5,4,1,2))
  hist(medias[, i], probability=TRUE, xlim = rng, xlab = '', main='', ylab='')
  mtext(text = paste('n = ',nn[i],sep=''), side = 1, line = 2)
  mtext(text = 'Densidade', side = 2, line = 2)
  lines(xx, dnorm(xx, mu, sqrt(sig2/nn[i])), col='red', lwd=2)
}
```


## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=65%}

```{.r code-line-numbers="|1|2-3|4|5-11|12|13-19"}
R <- 1000; nn <- c(15, 30, 100)
lambda <- 0.5
mu <- 1/lambda; sig2 <- 1/(lambda^2)
medias <- matrix(NA, ncol=length(nn), nrow=R)
for(i in 1:length(nn)){
  n <- nn[i]
  for(r in 1:R){
    x <- rexp(n = n, rate = lambda)
    medias[r, i] <- mean(x)
  }
}
rng <- range(medias); xx <- seq(rng[1], rng[2], length.out=1000)
for(i in 1:length(nn)){
  par(family = "serif", mar=c(5,4,1,2))
  hist(medias[, i], probability=TRUE, xlim = rng, xlab = '', main='', ylab='')
  mtext(text = paste('n = ',nn[i],sep=''), side = 1, line = 2)
  mtext(text = 'Densidade', side = 2, line = 2)
  lines(xx, dnorm(xx, mu, sqrt(sig2/nn[i])), col='red', lwd=2)
}
```

:::

::: {.column #vcenter width=35%}

```{r}
lambda = 0.5; M <- 10
x <- seq(0-0.05*(M), M+0.05*(M), length.out=1000)
fx <- dexp(x = x, rate = lambda)
par(family = "serif", mar=c(5,4,1,2))
plot(x, fx, type='n', xlab='', ylab='', main='', lwd=3,
     ylim=range(fx)+c(-.1*diff(range(fx)),.13*diff(range(fx))),
     xlim=range(x)+c(-.02*diff(range(x)),.08*diff(range(x))))
arrows(y0 = min(fx)-.1*diff(range(fx)), y1 = max(fx)+.13*diff(range(fx)), x0=0, x1=0, length = .1)
arrows(x0 = min(x)-.02*diff(range(x)), x1 = max(x)+.08*diff(range(x)), y0=0, y1=0, length = .1)
lines(x[x < 0], fx[x < 0], lwd=3)
lines(c(0,0), c(0, lambda), lwd=3, lty=2)
lines(x[x >= 0], fx[x >= 0], lwd=3)
points(c(0,0), c(0, lambda), cex=1.5, pch=21, bg=c('white','black'))
```

:::

:::

## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

```{r fig.ncol=3}
R <- 1000; nn <- c(15, 30, 100)
lambda <- 0.5
mu <- 1/lambda; sig2 <- 1/(lambda^2)
medias <- matrix(NA, ncol=length(nn), nrow=R)
for(i in 1:length(nn)){
  n <- nn[i]
  for(r in 1:R){
    x <- rexp(n = n, rate = lambda)
    medias[r, i] <- mean(x)
  }
}
rng <- range(medias); xx <- seq(rng[1], rng[2], length.out=1000)
for(i in 1:length(nn)){
  par(family = "serif", mar=c(5,4,1,2))
  hist(medias[, i], probability=TRUE, xlim = rng, xlab = '', main='', ylab='')
  mtext(text = paste('n = ',nn[i],sep=''), side = 1, line = 2)
  mtext(text = 'Densidade', side = 2, line = 2)
  lines(xx, dnorm(xx, mu, sqrt(sig2/nn[i])), col='red', lwd=2)
}
```


## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 0px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 2 (cont.):** Tinhamos uma aa de uma popula√ß√£o $X \sim \text{Exp}(\lambda)$. Note que $\mu = E(X) = \frac{1}{\lambda}$ e $\sigma^2 = V(X) = \frac{1}{\lambda^2}$. Vimos que a distribui√ß√£o exata do total amostral era uma $\text{Gama}(n, \frac{1}{\lambda})$.

Suponha que temos $\lambda = 0.5$ e desejamos encontrar $P(\bar{X} \leq \mu - 0.5) = P(S_n \leq n\mu - 0.5n)$, cujo valor exato pela gama √© `r round(pgamma(20*(2-0.5), shape = 20, scale=2), 4)`, se $n=20$ e `r round(pgamma(40*(2-0.5), shape = 40, scale=2), 4)`, se $n=40$, por exemplo.

Agora, temos que
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$P(S_n \leq n\mu - 0.5n)$ </td> <td>$=$ </td> <td> $P(\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \leq \frac{\mu - 0.5 - \mu}{\sigma/\sqrt{n}}) = P(Z_n \leq -\frac{\lambda\sqrt{n}}{2})$ </td>
  </tr>
  <tr style="border-bottom:none;text-align:center;">
    <td> </td> <td>$\approx$ </td> <td> ${\displaystyle \begin{cases} P(Z \leq -1.12) = 0.1314, & n=20;\\ P(Z \leq -1.58) = 0.0571, & n=40. \end{cases}}$ </td>
  </tr>
</table>

:::

::: {.callout-caution}
## Observa√ß√£o

Note que, considerando a assimetria da popula√ß√£o ($X\sim\text{Exp}(0.5)$), as probabilidades exatas e a aproximadas s√£o relativamente pr√≥ximas, mesmo para valores de $n$ moderados.
:::

## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 0px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 3:** A resist√™ncia √† ruptura de um rebite possui valor m√©dio de 10000 psi e desvio padr√£o de 500 psi. Qual √© a probabilidade de a resist√™ncia √† ruptura m√©dia de uma amostra aleat√≥ria de 40 rebites estar entre 9900 e 10200?

::: fragment

Note que o enunciado nos fornece que $\mu = E(X) = 10000$, $\sigma = 500$ e $n = 40$. Desejamos obter $P(9900 < \bar{X} < 10200)$.

:::

::: fragment

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$P(9900 < \bar{X} < 10200)$ </td> <td>$=$ </td> <td> $P(\frac{9900 - 10000}{500/\sqrt{40}} < \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} < \frac{10200 - 10000}{500/\sqrt{40}}) \approx P(-1.27 < Z < 2.53)$ </td>
  </tr>
  <tr style="border-bottom:none;text-align:center;">
    <td> </td> <td>$=$ </td> <td> $\Phi(2.53) - \Phi(-1.27) = 0.9943 - 0.1020 = 0.8923$. </td>
  </tr>
</table>

:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Note que sequer foi necess√°rio o conhecimento da distribui√ß√£o da popula√ß√£o. Apenas especificamos sua m√©dia $\mu$ e desvio-padr√£o $\sigma$. Claro que estamos supondo que $n = 40$ √© suficientemente grande para o TCL fornecer boa aproxima√ß√£o.
:::

:::

# Estima√ß√£o

## 2 Estima√ß√£o {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Normalmente, usamos estat√≠sticas para aproximar o valor de algum par√¢metro populacional. Nesse caso, elas recebem um nome especial.
:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Um **estimador** √© uma estat√≠stica utilizada para aproximar o valor de um par√¢metro populacional.
:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Podemos dividir a tarefa de estima√ß√£o de par√¢metros em:

- Estima√ß√£o pontual: lida com a avalia√ß√£o do desempenho de um estimador;
- Estima√ß√£o intervalar: lida com a constru√ß√£o de intervalos que englobam a variabilidade do
estimador no processo de estima√ß√£o.
:::

:::


## Estima√ß√£o Pontual

::: {.callout-note}
## Defini√ß√£o

Sejam $X_1, \ldots, X_n$ uma aa de uma popula√ß√£o $X$ e $\theta$ um par√¢metro. Um estimador $\hat\theta$ de $\theta$ √© qualquer fun√ß√£o da aa $\hat\theta = \hat\theta(X_1, \ldots, X_n)$ utilizada para aproximar o valor de $\theta$. Uma **Estimativa** √© o valor observado de um estimador para uma amostra $x_1, \ldots, x_n$ espec√≠fica.
:::

::: fragment

::: {.callout-important}
## Problema

Sendo poss√≠vel definir diversos estimadores para $\theta$, como avaliar a qualidade de diferentes estimadores?
:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Existem diferentes medidas. Veremos: V√≠cio; Erro-padr√£o; e Erro Quadr√°tico M√©dio (EQM).
:::

:::

## 2.1 Estima√ß√£o Pontual (V√≠cio) {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Deve ser **esperado** de um bom estimador que ele forne√ßa o verdadeiro valor do par√¢metro.
:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Seja $\theta$ um par√¢metro e $\hat\theta$ um estimador de $\theta$. O v√≠cio (ou vi√©s) de $\hat\theta$ √© definido por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$B(\hat\theta) = E(\hat\theta) - \theta = \mu_{\hat\theta} - \theta;$ </td>
  </tr>
</table>
onde $\mu_{\hat\theta} = E(\hat\theta)$ √© o valor esperado de $\hat\theta$.
:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√µes

1. O v√≠cio de um estimador mede, em m√©dia, quanto o estimador se dist√¢ncia do par√¢metro;
2. Dizemos que $\hat\theta$ √© **n√£o viciado** (ou n√£o viesado) se seu v√≠cio $B(\hat\theta) = 0$.

:::

:::

## 2.1 Estima√ß√£o Pontual (V√≠cio) {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=50%}

```{r}
n <- 25; i <- 0
th0 <- th1 <- NULL
while(i < n){
  x0 <- runif(1,-1,1); y0 <- runif(1,-1,1)
  x1 <- runif(1,-1,1); y1 <- runif(1,-1,1)
  if(x0^2 + y0^2 < 1 & x1^2 + y1^2 < 1){
    th0 <- rbind(th0, c(x0, y0))
    th1 <- rbind(th1, c(x1+1, y1+1))
    i <- i+1
  }
}
rngX <- range(th0[,1], th1[,1])
rngY <- range(th0[,2], th1[,2])
par(family = "serif", mar=c(5,4,1,2))
plot(0, type='n', xlab='', ylab='', main ='', frame.plot=FALSE,
     axes = FALSE, xlim=rngX, ylim=rngY)
points(x = th0[,1], y = th0[,2], pch=21, bg='blue', cex=1.5)
points(x = th1[,1], y = th1[,2], pch=21, bg='red', cex=1.5)
points(x = 0, y = 0, pch=21, bg='black', cex=2)
text(x = 0, y = 0, labels=expression(theta), adj=c(2,2), cex=2)
```

:::

::: {.column #vcenter width=50%}

::: {.callout-caution}
## Observa√ß√£o
Ilustra√ß√£o de estimativas fornecidas por um estimador [**n√£o viesado**]{style="color:blue;"} e um [**viesado**]{style="color:red;"} de um par√¢metro $\theta$.
:::

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 4 {.unnumbered .unlisted}

Suponha que ser√£o observadas $n ‚àí 1 \geq 1$ observa√ß√µes $X_1, \ldots, X_{n-1}$ de $X$, com $E(X) = \mu$. Defina a m√©dia dessas $n-1$ observa√ß√µes por $\bar{X}_{n-1} = \frac{1}{n-1}\sum_{i=1}^{n-1}$.

::: fragment

Suponha que uma nova observa√ß√£o $X_n$ ficar√° dispon√≠vel. Considere os seguintes estimadores de $\mu$:
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\hat{\mu}_1 = \frac{n-1}{n}\bar{X}_{n-1} + \frac{1}{n}X_n$ </td> <td> e </td> <td>$\hat{\mu}_2 = \frac{\bar{X}_{n-1} + X_n}{2}$. </td>
  </tr>
</table>

:::

::: fragment

Qual √© o v√≠cio de $\hat{\mu}_1$ e $\hat{\mu}_2$?

:::

::: fragment

Temos que
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$E(\hat{\mu}_1) = E[\frac{n-1}{n}\bar{X}_{n-1} + \frac{1}{n}X_n] = \frac{n-1}{n}E(\bar{X}_{n-1}) + \frac{1}{n}E(X_n) = \frac{n-1}{n}\mu + \frac{1}{n}\mu = \mu.$</td>
  </tr>
</table>

:::

::: fragment

e que

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$E(\hat{\mu}_2) = E[\frac{\bar{X}_{n-1} + X_n}{2}] = \frac{E(\bar{X}_{n-1}) + E(X_n)}{2} = \frac{\mu + \mu}{2} = \mu.$</td>
  </tr>
</table>

:::

::: fragment

Assim, $B(\hat{\mu}_1) = B(\hat{\mu}_2) = \mu-\mu = 0$. Portanto, ambos s√£o **n√£o viesados**.

:::

## 2.1 Estima√ß√£o Pontual (V√≠cio) {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 0px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 5:** Verifiquemos que $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i - \bar{X})^2$ √© um estimador viesado de $\sigma^2$. De fato,

<table>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td>$E(\hat{\sigma}^2)$ </td> <td> $=$ </td> <td> $\frac{1}{n}\sum_{i=1}^nE[(X_i-\bar{X})^2]=\frac{1}{n}\sum_{i=1}^n
E[(X_i-\mu-(\bar{X}-\mu))^2]$</td>
  </tr>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> $\frac{1}{n} \sum_{i=1}^n E[(X_i-\mu)^2-2(X_i-\mu)(\bar{X}-\mu)+(\bar{X}-\mu)^2]$</td>
  </tr>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> $\frac{1}{n}\sum_{i=1}^n\left\{\sigma^2-2E\left[(X_i-\mu)\left(\frac{1}{n}\sum_j(X_j-\mu)\right)\right] + E[(\bar{X}-\mu)^2]\right\}$</td>
  </tr>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> $\frac{1}{n}\sum_{i=1}^n\left\{\sigma^2-\frac{2}{n}\sum_jE[(X_i-\mu)(X_j-\mu)] + \frac{\sigma^2}{n}\right\}$</td>
  </tr>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> $\frac{1}{n}\sum_{i=1}^n\left\{\sigma^2-\frac{2}{n}\sum_j\text{Cov}(X_i, X_j) + \frac{\sigma^2}{n}\right\}$</td>
  </tr>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> $\frac{1}{n}\sum_{i=1}^n\left\{\sigma^2-\frac{2\sigma^2}{n}+\frac{\sigma^2}{n}\right\}=\sigma^2 \color{red}{-\frac{\sigma^2}{n}}$.</td>
  </tr>
</table>

:::

::: fragment

::: {.callout-tip}
## Exerc√≠cio

Tendo em vista o Exemplo 5, mostre que $S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X})^2$ √© um estimador **n√£o** viesado de $\sigma^2$.
:::

:::

## 2.1 Estima√ß√£o Pontual (Erro-Padr√£o) {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

- Estimadores s√£o va;
- Al√©m do v√≠cio, √© importante saber a sua variabilidade;
- Podemos comparar dois estimadores n√£o viesados pelo seu **erro-padr√£o**.

:::


::: {.callout-note}
## Defini√ß√£o

Seja $\hat{\theta}$ estimador de $\theta$. O erro-padr√£o ($EP(\hat{\theta})$) de $\hat{\theta}$ √© o desvio-padr√£o de $\hat{\theta}$, isto √©, $EP(\hat{\theta})=\sqrt{V(\hat{\theta})}$.

:::

::: {.callout-note}
## Defini√ß√£o

Sejam $\hat{\theta}_1$ e $\hat{\theta}_2$ estimadores n√£o viesados de $\theta$. Dizemos que $\hat{\theta}_1$ √© mais eficiente que $\hat{\theta}_2$ se $EP(\hat{\theta}_1) < EP(\hat{\theta}_2)$.

:::


## 2.1 Estima√ß√£o Pontual (Erro-Padr√£o) {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=50%}

```{r}
n <- 30; i <- 0
th0 <- th1 <- NULL
while(i < n){
  x0 <- runif(1,-1,1); y0 <- runif(1,-1,1)
  x1 <- runif(1,-2,2); y1 <- runif(1,-2,2)
  if(x0^2 + y0^2 < 1 & x1^2 + y1^2 < 2^2){
    th0 <- rbind(th0, c(x0, y0))
    th1 <- rbind(th1, c(x1, y1))
    i <- i+1
  }
}
rngX <- range(th0[,1], th1[,1])
rngY <- range(th0[,2], th1[,2])
par(family = "serif", mar=c(5,4,1,2))
plot(0, type='n', xlab='', ylab='', main ='', frame.plot=FALSE,
     axes = FALSE, xlim=rngX, ylim=rngY)
points(x = th1[,1], y = th1[,2], pch=21, bg='red', cex=1.5)
points(x = th0[,1], y = th0[,2], pch=21, bg='blue', cex=1.5)
points(x = 0, y = 0, pch=21, bg='black', cex=2)
text(x = 0, y = 0, labels=expression(theta), adj=c(2,2), cex=2)
```

:::

::: {.column #vcenter width=50%}

::: {.callout-caution}
## Observa√ß√£o
Ilustra√ß√£o de estimativas fornecidas por dois estimadores [$\hat\theta_1$]{style="color:blue;"} e [$\hat\theta_2$]{style="color:red;"} de um par√¢metro $\theta$, em que [$\hat\theta_1$]{style="color:blue;"} √© mais eficiente que [$\hat\theta_2$]{style="color:red;"}.
:::

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 4 (cont.) {.unnumbered .unlisted}

Verifiquemos os Erros-Padr√£o de $\hat{\mu}_1$ e de $\hat{\mu}_2$. [Temos que]{.fragment fragment-index=1}
<table class="fragment" data-fragment-index="1">
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle V(\hat{\mu}_1) = V\left(\frac{n-1}{n}\bar{X}_{n-1}+\frac{1}{n}X_n\right) = \left(\frac{n-1}{n}\right)^2\frac{\sigma^2}{n-1}+\frac{1}{n^2}\sigma^2 = \frac{n-1+1}{n^2}\sigma^2=\frac{1}{n}\sigma^2}$</td>
  </tr>
</table>
[e]{.fragment fragment-index=2}
<table class="fragment" data-fragment-index="2">
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle V(\hat{\mu}_2) = V\left(\frac{\bar{X}_{n-1}+X_n}{2}\right) = \frac{1}{4}(V(\bar{X}_{n-1})+V(X_n)) = \frac{1}{4} \left(\frac{\sigma^2}{n-1} + \sigma^2\right)=\frac{n}{4(n-1)}\sigma^2}$.</td>
  </tr>
</table>

[Vejamos quando $\hat{\mu}_1$ √© mais eficiente do que $\hat{\mu}_2$.]{.fragment fragment-index=3} [De fato,]{.fragment fragment-index=4}
<table class="fragment" data-fragment-index="4">
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle V(\hat{\mu}_1) < V(\hat{\mu}_2) \Rightarrow \frac{\sigma^2}{n} < \frac{n\sigma^2}{4(n-1)} \Rightarrow n^2-4n+4>0 \Rightarrow (n-2)^2>0}$.</td>
  </tr>
</table>

[Assim, $\hat{\mu}_1$ √© mais eficiente do que $\hat{\mu}_2$ se $(n-2)^2>0 \Rightarrow n > 2$.]{.fragment fragment-index=5} [Se $n = 2$ ambos estimadores s√£o igualmente eficientes.]{.fragment fragment-index=6}

## 2.1 Estima√ß√£o Pontual (Erro Quadr√°tico M√©dio) {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Vimos que o Erro-Padr√£o nos permite comparar a efici√™ncia de estimadores n√£o viesados. Para comparar estimadores com v√≠cios diferentes, podemos usar o [**Erro Quadr√°tico M√©dio**]{style='color:red;'}.
:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Seja $\hat{\theta}$ estimador de $\theta$. Definimos o **Erro Quadr√°tico M√©dio** (EQM) de
$\hat{\theta}$ por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$EQM(\hat{\theta})=E[(\hat{\theta}-\theta)^2]$.</td>
  </tr>
</table>
:::

:::

::: fragment

::: {.callout-tip}
## Exerc√≠cios

Mostre que o EQM de $\hat\theta$ pode ser escrito como a soma do quadrado do v√≠cio e da vari√¢ncia
de $\hat\theta$, isto √©,
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$EQM(\hat{\theta})=[B(\hat{\theta})]^2+V(\hat{\theta})$.</td>
  </tr>
</table>

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 5 (cont.) {.unnumbered .unlisted}

No Exemplo 5, mostramos que $B(\hat{\sigma}^2)=-\frac{\sigma^2}{n}$. √â poss√≠vel mostrar
que $B(S^2)=0$ e que, se $X\sim N(\mu,\sigma^2)$, ent√£o

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle V(S^2)=\frac{2\sigma^4}{n-1}}$</td> <td> e </td> <td> ${\displaystyle V(\hat{\sigma}^2) = \left( \frac{n-1}{n} \right)^2 \frac{2\sigma^4}{n-1}}$. </td>
  </tr>
</table>

::: fragment

Logo, os EQM's de $S^2$ e de $\hat{\sigma}^2$ s√£o, respectivamente,

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle EQM(S^2)=\frac{2\sigma^4}{n-1}}$</td> <td> e </td> <td> ${\displaystyle EQM(\hat{\sigma}^2)=\left(\frac{\sigma^2}{n}\right)^2 + \frac{(n-1)2\sigma^4}{n^2} = \frac{2\sigma^4}{n}-\frac{\sigma^4}{n^2}}$. </td>
  </tr>
</table>

:::

::: fragment

Assim, em termos de EQM, vemos que $\hat{\sigma}^2$ √© melhor do que $S^2$.

:::

## 2.1 Estima√ß√£o Pontual (Consist√™ncia) {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Seja $\hat{\theta}$ um estimador do par√¢metro $\theta$. Dizemos que $\hat{\theta}$ √© **consistente** se seu EQM tende a zero quando o tamanho da amostra tende a infinito. Isto √©, se
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle \lim_{n\to\infty}EQM(\hat{\theta})=0}$. </td>
  </tr>
</table>

:::

::: {.callout-caution}
## Observa√ß√£o

1. A defini√ß√£o acima implica que $\hat{\theta}$ √© consistente se seu v√≠cio e sua vari√¢ncia tendem a 0;
2. Em poucas palavras, quando um estimador √© consistente suas estimativas tendem a ser pr√≥ximas do verdadeiro valor do par√¢metro

:::

::: {style="border-style: solid; border-width: 3px; padding-top: 0px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 5 (cont.):** Como ${\displaystyle \lim_{n\to\infty} EQM(S^2) = \lim_{n\to\infty} EQM(\hat\sigma^2) = 0}$, ambos s√£o consistentes para $\sigma^2$.
:::

## 2.1 Estima√ß√£o Pontual (Consist√™ncia) {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=50%}

```{r out.width="95%"}
x <- seq(0, 10, length.out=1000)
f1 <- dgamma(x, shape=2, scale = 2)
f2 <- dgamma(x, shape=4, scale = 1)
f3 <- dgamma(x, shape=8, scale = 0.5)
par(family = "serif", mar=c(5,4,1,2))
rngY <- range(f1, f2, f3)
rngX <- range(x)
plot(x, f1, type='n', xlab='', ylab='', main='', lwd=3,
     ylim=rngY+c(-.1*diff(rngY),.13*diff(rngY)),
     xlim=rngX+c(-.02*diff(rngX),.08*diff(rngX)))
arrows(y0 = rngY[1]-.1*diff(rngY), y1 = rngY[2]+.13*diff(rngY), x0=0, x1=0, length = .1)
arrows(x0 = rngX[1]-.02*diff(rngX), x1 = rngX[2]+.08*diff(rngX), y0=0, y1=0, length = .1)
lines(x, f1, lwd=3, col='red')
lines(x, f2, lwd=3, col='blue')
lines(x, f3, lwd=3, col='darkgreen')
lines(c(4.2,4.2), rngY+c(0,.13*diff(rngY)), lwd=2.5, lty=3)
lines(c(4.2,4.2), rngY[1]+c(-0.03*diff(rngY),0), lwd=1.5)
text(x=4.2, y=rngY[1]-0.05*diff(rngY), labels=expression(theta), adj=c(.5, 1.2))
```

:::

::: {.column #vcenter width=50%}

::: {.callout-caution}
## Observa√ß√£o

Ilustra√ß√£o da distribui√ß√£o amostral de um estimador consistente com
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle \color{red}{n}< \color{blue}{n} < \color{darkgreen}{n}}$. </td>
  </tr>
</table>
:::

:::

:::

## M√©todos de obten√ß√£o de estimadores

## Estima√ß√£o Intervalar

# Testes de Hip√≥teses

# FIM {.unnumbered .unlisted}

# APAGAR DPS 1 {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

:::

::: {.callout-note}
## Defini√ß√£o

:::

::: {.callout-warning}
## Propriedade

:::

::: {.callout-tip}
## Exerc√≠cios

:::

::: {.callout-important}
## Problema

:::


::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo:** 

::: 

# APAGAR DPS 2 {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=45%}
1
:::

::: {.column #vcenter width=55%}
2

3

4
:::

:::



```{=latex}
\begin{align*}
\Omega &= \{(1,1), (1,2), \ldots, (4,4)\}\\
       &= \{(x,y) : x,y = 1,\ldots,4\}.
\end{align*}
```


# FIM {.unnumbered .unlisted}
