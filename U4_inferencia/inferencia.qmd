---
title: "Probabilidade e Estat√≠stica"
subtitle: "Infer√™ncia"
author: "Prof. Dr. Alessandro JQ Sarnaglia"
lang: "pt"
format:
  revealjs:
    width: 1280
    height: 720
    min-scale: 0.05
    fig-width: 5.5
    fig-height: 5.5
    theme: [custom.scss]
    css: [fonts.css, style.css]
    callout-icon: false
    toc: true
    toc-depth: 3
    toc-expand: 3
    number-sections: true
    number-depth: 3
    footer: "[T√≥picos üîô](../index.html) | [Sum√°rio üìã](inferencia.html#/TOC)"
    menu:
      useTextContentForMissingTitles: false
editor: source
filters:
  - parse-latex
---


# Distribui√ß√µes Amostrais

## Fun√ß√µes de Vari√°veis Aleat√≥rias

::: {.callout-caution}
## Observa√ß√£o

Muitas vezes √© de interesse determinar a distribui√ß√£o de uma fun√ß√£o de va's $X_1, \ldots, X_n$.
:::

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1:** Um sistema de transa√ß√µes √© composto por dois componentes id√™nticos independentes. Quando o primeiro falha (com probabilidade $p$), √© imediatamente substituido pelo outro. Seja $X_i$ o n√∫mero de transa√ß√µes at√© a falha do componente $i$. Ent√£o $X_i\sim\text{Geo}(p)$. Qual a distribui√ß√£o de $Y = X_1 + X_2$?

::: 

::: {.callout-note}
## Defini√ß√£o - Caso discreto

Sejam $X_1, \ldots, X_n$ vad's com fmp conjunta $p(x_1, \ldots, x_n)$ e $Y = h(X_1, \ldots, X_n)$ uma fun√ß√£o dessas vari√°veis. A fmp de $Y$ √© dada por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\displaystyle p_Y(y) = \underset{x_i\ :\ h(x_1,\ldots, x_n)\ =\ y}{\sum \cdots \sum} p(x_1, \ldots, x_n)$.</td>
  </tr>
</table>
:::

## 1.1 Fun√ß√µes de Vari√°veis Aleat√≥rias {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 6px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 1 (cont):** Note que $Y$ representa a quantidade de total de transa√ß√µes. Pela independ√™ncia de $X_1$ e $X_2$, temos que $p(x_1,x_2) = (1-p)^{x_1-1}p(1-p)^{x_2-1}p = (1-p)^{x_1+x_2-2}p^2$. Logo,
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$p_Y(y)$ </td> <td> $=$ </td> <td> ${\displaystyle\underset{x_1,\ x_2\ :\ x_1\ +\ x_2\ =\ y}{\sum \sum} p(x_1, x_2) = \sum_{x_1 = 1}^{y-1} \sum_{x_2 = y-x_1}^{y-x_1} (1-p)^{x_1+x_2-2}p^2 = \sum_{x_1 = 1}^{y-1} (1-p)^{y-2}p^2}$</td>
  </tr>
  <tr style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> ${\displaystyle (y-1)(1-p)^{y-2}p^2 = {y-1 \choose 1} (1-p)^{y-2}p^2}$.</td>
  </tr>
</table>
Portanto, $Y \sim \text{BN}(2,p)$.
:::

::: {.callout-caution}
## Observa√ß√£o

O Exemplo 1 pode ser generalizado. Mais precisamente, se $X_1, \ldots, X_k$ s√£o vad's independentes com $X_i \sim \text{Geo}(p)$ e $Y = \sum_{i=1}^k X_i$, ent√£o √© poss√≠vel mostrar que $Y \sim \text{BN}(k, p)$. Isto √©, soma de va's geom√©tricas independentes tem distribui√ß√£o binomial negativa.
:::

## 1.1 Fun√ß√µes de Vari√°veis Aleat√≥rias {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 6px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 2:** Uma pessoa chega no caixa em um tempo $X_1$ ap√≥s o instante 0 e uma segunda $X_2$ instantes ap√≥s a primeira. Se $X_1$ e $X_2$ s√£o independentes e $X_i \sim \text{Exp}(\lambda)$, qual √© distribui√ß√£o de $Y = X_1 + X_2$?

:::

::: {.callout-note}
## Defini√ß√£o - Caso cont√≠nuo

Sejam $X_1, \ldots, X_n$ vac's com fdp conjunta $f(x_1, \ldots, x_n)$ e $Y = h(X_1, \ldots, X_n)$ uma fun√ß√£o dessas vari√°veis. A fdp de $Y$ √© dada por $f_Y(y) = \frac{d}{dy}F_Y(y)$ em que $F_Y(y)$ √© a fda de $Y$ definida por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\displaystyle F_Y(y) = P(Y \leq y) = P(h(X_1, \ldots, X_n) \leq y) = \underset{x_i\ :\ h(x_1,\ldots, x_n)\ \leq\ y}{\int \cdots \int} f(x_1, \ldots, x_n) dx_1 \cdots dx_n$.</td>
  </tr>
</table>
:::

::: {.callout-caution}
## Observa√ß√£o

Pela defini√ß√£o, n√£o obtemos diretamente $f_Y(y)$. Devemos obter $F_Y(y)$ e ent√£o calcular $f_Y(y) = \frac{d}{dy}F_Y(y)$.
:::

## 1.1 Fun√ß√µes de Vari√°veis Aleat√≥rias {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 6px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 2 (cont.):** Note que $Y$ o momento em que o segundo cliente chega √† loja. Pela independ√™ncia de $X_1$ e $X_2$, temos que $f(x_1,x_2) = \lambda e^{-\lambda x_1} \lambda e^{-\lambda x_2} = \lambda^2 e^{-\lambda (x_1 + x_2)}$. Logo,
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$F_Y(y)$ </td> <td> $=$ </td> <td> ${\underset{x_1\ +\ x_2\ \leq\ y}{\int \int} f(x_1, x_2) dx_1 dx_2 = \int_{0}^{y} \int_{0}^{y-x_2} \lambda^2 e^{-\lambda (x_1 + x_2)} dx_1 dx_2 = \int_{0}^{y} \lambda e^{-\lambda x_2} [-e^{-\lambda x_1}|_{0}^{y-x_2}] dx_2}$</td>
  </tr>
  <tr style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> ${ \int_{0}^{y} \lambda e^{-\lambda x_2} [1-e^{-\lambda (y-x_2)}] dx_2 = \int_{0}^{y} \lambda e^{-\lambda x_2} - \lambda e^{-\lambda y} dx_2 = [-e^{-\lambda x_2} - \lambda x_2 e^{-\lambda y}|_{0}^{y}]}$</td>
  </tr>
  <tr style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> ${-e^{-\lambda y} - \lambda ye^{-\lambda y} + 1 - 0}$.</td>
  </tr>
</table>
Agora, derivando $F_Y(y)$, obtemos
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$f_Y(y) = \frac{d}{dy}F_Y(y) = \frac{d}{dy}[-e^{-\lambda y} - \lambda ye^{-\lambda y} + 1] = \lambda e^{-\lambda y} - \lambda e^{-\lambda y} + \lambda^2 y e^{-\lambda y} = \frac{1}{(\frac{1}{\lambda})^2\Gamma(2)}y^{2-1}e^{-\frac{y}{\lambda^{-1}}}$. </td>
  </tr>
</table>
Portanto, $Y \sim \text{Gama}(2, \lambda^{-1})$.
:::

::: {.callout-caution}
## Observa√ß√£o

O Ex. 2 pode ser extendido. Se $X_1, \ldots, X_n$ s√£o vac's independentes com $X_i \sim \text{Exp}(\lambda)$ e $Y = \sum_{i=1}^n X_i$, ent√£o $Y \sim \text{Gama}(n, \lambda^{-1})$. Isto √©, soma de exponenciais independentes tem distribui√ß√£o gama.
:::

## Distribui√ß√£o Amostral

::: {.callout-caution}
## Observa√ß√µes

1. As defini√ß√µes anteriores nos permitem obter a distribui√ß√£o de fun√ß√µes de va's de maneira gen√©rica. Existem diversas ferramentas que podem facilitar essa tarefa: convolu√ß√£o, fun√ß√µes geradora de momentos e caracter√≠stica, m√©todo do jacobiano, entre outras. Elas n√£o ser√£o vistas aqui;
2. No contexto de infer√™ncia, dois conceitos importantes s√£o o de amostra aleat√≥ria e de estat√≠stica.
:::

::: {.callout-note}
## Defini√ß√£o - Amostra Aleat√≥ria

Seja $X$ va e $X_1, \ldots, X_n$ va's independentes com a mesma distribui√ß√£o que $X$. Dizemos que $X_1, \ldots, X_n$ √© uma **amostra aleat√≥ria** (aa) de $X$. Nesse caso, podemos dizer que $X$ descreve a vari√°vel na **popula√ß√£o**.
:::

::: {.callout-note}
## Defini√ß√£o - Estat√≠stica

Seja $X_1, \ldots, X_n$ aa de $X$. Uma estat√≠stica $T$ √© qualquer fun√ß√£o $T = T(X_1, \ldots, X_n)$ da referida amostra.
:::

## 1.2 Distribui√ß√£o Amostral {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o - Distribui√ß√£o Amostral

Seja $X_1, \ldots, X_n$ aa de $X$ e $T = T(X_1, \ldots, X_n)$ uma estat√≠stica. Dizemos que a fda $F_T(t) = P(T \leq t) = P(T(X_1,\ldots, X_n) \leq t)$ √© a **distribui√ß√£o amostral** de $T$.
:::

::: {.callout-caution}
## Observa√ß√µes

1. O termo distribui√ß√£o amostral tamb√©m poder√° ser utilizado para nos referirmos a fmp $p_T(t)$ ou a fdp $f_T(t)$ da estat√≠stica $T$ se ela for discreta ou cont√≠nua, respectivamente;
2. As generaliza√ß√µes dos Exemplos 1 e 2 se tratavam de aa's das popula√ß√µes $X \sim \text{Geo}(p)$ e $X \sim \text{Exp}(\lambda)$, respectivamente. A estat√≠stica considerada nos dois exemplos era o **total amostral** $Y = \sum_i X_i$;
3. Por esses exemplos, fica claro que a distribui√ß√£o amostral de uma estat√≠stica depende da distribui√ß√£o da popula√ß√£o $X$: binomial negativa no Exemplo 1; e gama no Exemplo 2.
:::

## 1.2 Distribui√ß√£o Amostral {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Se a distribui√ß√£o populacional e/ou se a estat√≠stica for muito complicada, a obten√ß√£o anal√≠tica da distribui√ß√£o populacional pode ser muito dif√≠cil. Nesses casos, podemos utilizar m√©todos de **Monte Carlo** (t√©cnica computacional) para obter uma aproxima√ß√£o da distribui√ß√£o amostral da estat√≠stica de interesse.
:::

::: {style="border-style: solid; border-width: 3px; padding-top: 12px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 3:** Seja $X_1, \ldots, X_n$ aa de $X \sim \text{Weibull}(10, 2)$, isto √© $\alpha=10$ e $\beta=2$. Considere a estat√≠stica
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\tilde{X} = T(X_1, \ldots, X_n) = \begin{cases} \frac{X_{(n/2)} + X_{(n/2+1)}}{2}, & n \text{ par}; \\ X_{(n+1)/2}, & n \text{ par}. \end{cases}$. </td>
  </tr>
</table>
Isto √©, $\tilde{X}$ √© a mediana amostral. Qual a distribui√ß√£o amostral de $\tilde{X}$ se $n=20$ e $n=40$? A estat√≠stica e a distribui√ß√£o populacional s√£o complexas para uma an√°lise anal√≠tica. Podemos usar m√©todos de Monte Carlo.
:::

## {.unnumbered .unlisted .smaller}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=65%}

```{.r code-line-numbers="|1-8|9-15|11-12|13-14|16-17|18-21|18-19|20-21|22-23"}
# par√¢metros da Weibull(a,b)
a <- 10; b<- 2
# Quantidade de r√©plicas
R <- 1000
# Tamanho da aa
n <- 20
# Vetor vazio para armazenar as medianas amostrais de cada r√©plica
med <- NULL
# Inicia as r√©plicas de Monte Carlo
for(r in 1:R){
  # Gera uma aa de tamanho n da Weibull(a, b)
  x <- rweibull(n=n, shape = a, scale = b)
  # Calcula a mediana amostral da amostra x
  med[r] <- median(x)                      
}
# Mediana populacional
med_pop <- b*(log(2))^(1/a)
# Histograma da distribui√ß√£o amostral da mediana amostral
hist(med)
# Tra√ßa a mediana da popula√ß√£o e intervalo +0.1 ou -0.1 desse valor
abline(v = med_pop + c(-.1,0,.1), col = 'red', lty = c(2,1,2), lwd = 2)
# Prop. das medianas amostrais entre med_pop - 0.1 e med_pop + 0.1
mean(med > med_pop-.1 & med < med_pop+.1)
```

:::

::: {.column #vcenter width=35%}

::: fragment

```{r}

a <- 10; b<- 2 # par√¢metros da Weibull(a,b)
R <- 1000      # Quantidade de r√©plicas
n <- 20        # Tamanho da aa
med <- NULL    # Vetor vazio para armazenar as medianas
               # amostrais de cada r√©plica
for(r in 1:R){ # Inicia as r√©plicas de Monte Carlo
  x <- rweibull(n=n, shape = a, scale = b) # Gera uma aa de tamanho n
                                           # da Weibull(a, b)
  med[r] <- median(x)                      # Calcula a mediana amostral
}
med_pop <- b*(log(2))^(1/a) # Mediana populacional
hist(med)                   # Histograma da distribui√ß√£o
                            # amostral da mediana amostral
abline(v = med_pop + c(-.1,0,.1),
       col = 'red', lty = c(2,1,2),
       lwd = 2) # Tra√ßa a mediana da popula√ß√£o e intervalo
                # +0.1 ou -0.1 desse valor
prp <- mean(med > med_pop-.1 & med < med_pop+.1)
cat("P[Med. Amost. \u2208 (Med. Pop. \u00b1 0.1)] \u2248 ", prp, sep='') 
```

:::

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 3 (cont.) {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=65%}

```{.r code-line-numbers="|5-6"}
# par√¢metros da Weibull(a,b)
a <- 10; b<- 2
# Quantidade de r√©plicas
R <- 1000
# Tamanho da aa
n <- 40
# Vetor vazio para armazenar as medianas amostrais de cada r√©plica
med <- NULL
# Inicia as r√©plicas de Monte Carlo
for(r in 1:R){
  # Gera uma aa de tamanho n da Weibull(a, b)
  x <- rweibull(n=n, shape = a, scale = b)
  # Calcula a mediana amostral da amostra x
  med[r] <- median(x)                      
}
# Mediana populacional
med_pop <- b*(log(2))^(1/a)
# Histograma da distribui√ß√£o amostral da mediana amostral
hist(med)
# Tra√ßa a mediana da popula√ß√£o e intervalo +0.1 ou -0.1 desse valor
abline(v = med_pop + c(-.1,0,.1), col = 'red', lty = c(2,1,2), lwd = 2)
# Prop. das medianas amostrais entre med_pop - 0.1 e med_pop + 0.1
mean(med > med_pop-.1 & med < med_pop+.1)
```

:::

::: {.column #vcenter width=35%}

::: fragment

```{r}

a <- 10; b<- 2 # par√¢metros da Weibull(a,b)
R <- 1000      # Quantidade de r√©plicas
n <- 40        # Tamanho da aa
med <- NULL    # Vetor vazio para armazenar as medianas
               # amostrais de cada r√©plica
for(r in 1:R){ # Inicia as r√©plicas de Monte Carlo
  x <- rweibull(n=n, shape = a, scale = b) # Gera uma aa de tamanho n
                                           # da Weibull(a, b)
  med[r] <- median(x)                      # Calcula a mediana amostral
}
med_pop <- b*(log(2))^(1/a) # Mediana populacional
hist(med)                   # Histograma da distribui√ß√£o
                            # amostral da mediana amostral
abline(v = med_pop + c(-.1,0,.1),
       col = 'red', lty = c(2,1,2),
       lwd = 2) # Tra√ßa a mediana da popula√ß√£o e intervalo
                # +0.1 ou -0.1 desse valor
prp <- mean(med > med_pop-.1 & med < med_pop+.1)
cat("P[Med. Amost. \u2208 (Med. Pop. \u00b1 0.1)] \u2248 ", prp, sep='') 
```

:::

:::

:::

## Distribui√ß√£o Amostral da M√©dia Amostral

::: {.callout-caution}
## Observa√ß√£o

Embora a distribui√ß√£o amostral dependa da distribui√ß√£o da popula√ß√£o, no caso da estat√≠stica **m√©dia amostral**, $\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i$, o Teorema Central do Limite nos fornece a sua [**distribui√ß√£o amostral limite**]{style='color:red;'}, independente da distribui√ß√£o populacional.
:::

::: {.callout-warning}
## Propriedade - Teorema Central do Limite (TCL)

Seja $X_1, \ldots, X_n$ aa de $X$, onde $E(X) = \mu$ e $V(X) = \sigma^2 < \infty$. Ent√£o a estat√≠stica
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle Z_n = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}}$ </td>
  </tr>
</table>
tem distribui√ß√£o aproximadamente normal padr√£o. A aproxima√ß√£o fica melhor com o aumento de $n$.
:::

## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}


::: {.callout-caution}
## Observa√ß√µes

1. A m√©dia amostral tem distribui√ß√£o aproximadamente $\text{N}(\mu, \frac{\sigma^2}{n})$, em que $\mu$ e $\sigma^2$ s√£o a m√©dia e a vari√¢ncia da popula√ß√£o $X$ da qual a amostra ser√° observada;
2. A aproxima√ß√£o se torna melhor com o aumento do tamanho amostral $n$;
3. O valor de $n$ necess√°rio para que a aproxima√ß√£o seja ''boa'' depende da distribui√ß√£o da popula√ß√£o;
4. Por exemplo, dados provenientes de popula√ß√µes assim√©tricas exigem o valor de $n$ maior.
:::

::: {.callout-warning}
## Propriedades - Corol√°rio do TCL

Seja $X_1, \ldots, X_n$ aa de $X$, onde $E(X) = \mu$ e $V(X) = \sigma^2 < \infty$ e $S_n = \sum_{i=1}^n X_i$ o total amostral. Ent√£o
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle Z_n = \frac{S_n - n\mu}{\sigma\sqrt{n}}}$ </td>
  </tr>
</table>
tem distribui√ß√£o aproximadamente normal padr√£o.
:::

## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=65%}

```{.r code-line-numbers="|1|2-3|4|5-11|12|13-19"}
R <- 1000; nn <- c(15, 30, 100)
a <- 0; b <- 10
mu <- (a+b)/2; sig2 <- (b-a)^2/12
medias <- matrix(NA, ncol=length(nn), nrow=R)
for(i in 1:length(nn)){
  n <- nn[i]
  for(r in 1:R){
    x <- runif(n = n, min = 0, max = 10)
    medias[r, i] <- mean(x)
  }
}
rng <- range(medias); xx <- seq(rng[1], rng[2], length.out=1000)
for(i in 1:length(nn)){
  par(family = "serif", mar=c(5,4,1,2))
  hist(medias[, i], probability=TRUE, xlim = rng, xlab = '', main='', ylab='')
  mtext(text = paste('n = ',nn[i],sep=''), side = 1, line = 2)
  mtext(text = 'Densidade', side = 2, line = 2)
  lines(xx, dnorm(xx, mu, sqrt(sig2/nn[i])), col='red', lwd=2)
}
```

:::

::: {.column #vcenter width=35%}

```{r}
a <- 0; b <- 10
x <- seq(a-0.05*(b-a), b+0.05*(b-a), length.out=1000)
fx <- dunif(x, a, b)
par(family = "serif", mar=c(5,4,1,2))
plot(x, fx, type='n', xlab='', ylab='', main='', lwd=3,
     ylim=range(fx)+c(-.1*diff(range(fx)),.13*diff(range(fx))),
     xlim=range(x)+c(-.02*diff(range(x)),.08*diff(range(x))))
arrows(y0 = min(fx)-.1*diff(range(fx)), y1 = max(fx)+.13*diff(range(fx)), x0=0, x1=0, length = .1)
arrows(x0 = min(x)-.02*diff(range(x)), x1 = max(x)+.08*diff(range(x)), y0=0, y1=0, length = .1)
lines(x[x > b], fx[x > b], lwd=3)
lines(x[x < a], fx[x < a], lwd=3)
lines(c(a,a), c(0, 1/(b-a)), lwd=3, lty=2)
lines(c(b,b), c(0, 1/(b-a)), lwd=3, lty=2)
lines(x[x >= a & x <= b], fx[x >= a & x <= b], lwd=3)
points(c(a,a,b,b), c(0, 1/(b-a), 0, 1/(b-a)), cex=1.5, pch=21, bg=c('white','black', 'white','black'))
```

:::

:::

## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

```{r fig.ncol=3}
R <- 1000; nn <- c(15, 30, 100)
a <- 0; b <- 10
mu <- (a+b)/2; sig2 <- (b-a)^2/12
medias <- matrix(NA, ncol=length(nn), nrow=R)
for(i in 1:length(nn)){
  n <- nn[i]
  for(r in 1:R){
    x <- runif(n = n, min = 0, max = 10)
    medias[r, i] <- mean(x)
  }
}
rng <- range(medias); xx <- seq(rng[1], rng[2], length.out=1000)
for(i in 1:length(nn)){
  par(family = "serif", mar=c(5,4,1,2))
  hist(medias[, i], probability=TRUE, xlim = rng, xlab = '', main='', ylab='')
  mtext(text = paste('n = ',nn[i],sep=''), side = 1, line = 2)
  mtext(text = 'Densidade', side = 2, line = 2)
  lines(xx, dnorm(xx, mu, sqrt(sig2/nn[i])), col='red', lwd=2)
}
```


## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=65%}

```{.r code-line-numbers="|1|2-3|4|5-11|12|13-19"}
R <- 1000; nn <- c(15, 30, 100)
lambda <- 0.5
mu <- 1/lambda; sig2 <- 1/(lambda^2)
medias <- matrix(NA, ncol=length(nn), nrow=R)
for(i in 1:length(nn)){
  n <- nn[i]
  for(r in 1:R){
    x <- rexp(n = n, rate = lambda)
    medias[r, i] <- mean(x)
  }
}
rng <- range(medias); xx <- seq(rng[1], rng[2], length.out=1000)
for(i in 1:length(nn)){
  par(family = "serif", mar=c(5,4,1,2))
  hist(medias[, i], probability=TRUE, xlim = rng, xlab = '', main='', ylab='')
  mtext(text = paste('n = ',nn[i],sep=''), side = 1, line = 2)
  mtext(text = 'Densidade', side = 2, line = 2)
  lines(xx, dnorm(xx, mu, sqrt(sig2/nn[i])), col='red', lwd=2)
}
```

:::

::: {.column #vcenter width=35%}

```{r}
lambda = 0.5; M <- 10
x <- seq(0-0.05*(M), M+0.05*(M), length.out=1000)
fx <- dexp(x = x, rate = lambda)
par(family = "serif", mar=c(5,4,1,2))
plot(x, fx, type='n', xlab='', ylab='', main='', lwd=3,
     ylim=range(fx)+c(-.1*diff(range(fx)),.13*diff(range(fx))),
     xlim=range(x)+c(-.02*diff(range(x)),.08*diff(range(x))))
arrows(y0 = min(fx)-.1*diff(range(fx)), y1 = max(fx)+.13*diff(range(fx)), x0=0, x1=0, length = .1)
arrows(x0 = min(x)-.02*diff(range(x)), x1 = max(x)+.08*diff(range(x)), y0=0, y1=0, length = .1)
lines(x[x < 0], fx[x < 0], lwd=3)
lines(c(0,0), c(0, lambda), lwd=3, lty=2)
lines(x[x >= 0], fx[x >= 0], lwd=3)
points(c(0,0), c(0, lambda), cex=1.5, pch=21, bg=c('white','black'))
```

:::

:::

## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

```{r fig.ncol=3}
R <- 1000; nn <- c(15, 30, 100)
lambda <- 0.5
mu <- 1/lambda; sig2 <- 1/(lambda^2)
medias <- matrix(NA, ncol=length(nn), nrow=R)
for(i in 1:length(nn)){
  n <- nn[i]
  for(r in 1:R){
    x <- rexp(n = n, rate = lambda)
    medias[r, i] <- mean(x)
  }
}
rng <- range(medias); xx <- seq(rng[1], rng[2], length.out=1000)
for(i in 1:length(nn)){
  par(family = "serif", mar=c(5,4,1,2))
  hist(medias[, i], probability=TRUE, xlim = rng, xlab = '', main='', ylab='')
  mtext(text = paste('n = ',nn[i],sep=''), side = 1, line = 2)
  mtext(text = 'Densidade', side = 2, line = 2)
  lines(xx, dnorm(xx, mu, sqrt(sig2/nn[i])), col='red', lwd=2)
}
```


## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 0px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 2 (cont.):** Tinhamos uma aa de uma popula√ß√£o $X \sim \text{Exp}(\lambda)$. Note que $\mu = E(X) = \frac{1}{\lambda}$ e $\sigma^2 = V(X) = \frac{1}{\lambda^2}$. Vimos que a distribui√ß√£o exata do total amostral era uma $\text{Gama}(n, \frac{1}{\lambda})$.

Suponha que temos $\lambda = 0.5$ e desejamos encontrar $P(\bar{X} \leq \mu - 0.5) = P(S_n \leq n\mu - 0.5n)$, cujo valor exato pela gama √© `r round(pgamma(20*(2-0.5), shape = 20, scale=2), 4)`, se $n=20$ e `r round(pgamma(40*(2-0.5), shape = 40, scale=2), 4)`, se $n=40$, por exemplo.

Agora, temos que
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$P(S_n \leq n\mu - 0.5n)$ </td> <td>$=$ </td> <td> $P(\frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \leq \frac{\mu - 0.5 - \mu}{\sigma/\sqrt{n}}) = P(Z_n \leq -\frac{\lambda\sqrt{n}}{2})$ </td>
  </tr>
  <tr style="border-bottom:none;text-align:center;">
    <td> </td> <td>$\approx$ </td> <td> ${\displaystyle \begin{cases} P(Z \leq -1.12) = 0.1314, & n=20;\\ P(Z \leq -1.58) = 0.0571, & n=40. \end{cases}}$ </td>
  </tr>
</table>

:::

::: {.callout-caution}
## Observa√ß√£o

Note que, considerando a assimetria da popula√ß√£o ($X\sim\text{Exp}(0.5)$), as probabilidades exatas e a aproximadas s√£o relativamente pr√≥ximas, mesmo para valores de $n$ moderados.
:::

## 1.3 Distribui√ß√£o Amostral da M√©dia Amostral {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 0px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 3:** A resist√™ncia √† ruptura de um rebite possui valor m√©dio de 10000 psi e desvio padr√£o de 500 psi. Qual √© a probabilidade de a resist√™ncia √† ruptura m√©dia de uma amostra aleat√≥ria de 40 rebites estar entre 9900 e 10200?

::: fragment

Note que o enunciado nos fornece que $\mu = E(X) = 10000$, $\sigma = 500$ e $n = 40$. Desejamos obter $P(9900 < \bar{X} < 10200)$.

:::

::: fragment

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$P(9900 < \bar{X} < 10200)$ </td> <td>$=$ </td> <td> $P(\frac{9900 - 10000}{500/\sqrt{40}} < \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} < \frac{10200 - 10000}{500/\sqrt{40}}) \approx P(-1.27 < Z < 2.53)$ </td>
  </tr>
  <tr style="border-bottom:none;text-align:center;">
    <td> </td> <td>$=$ </td> <td> $\Phi(2.53) - \Phi(-1.27) = 0.9943 - 0.1020 = 0.8923$. </td>
  </tr>
</table>

:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Note que sequer foi necess√°rio o conhecimento da distribui√ß√£o da popula√ß√£o. Apenas especificamos sua m√©dia $\mu$ e desvio-padr√£o $\sigma$. Claro que estamos supondo que $n = 40$ √© suficientemente grande para o TCL fornecer boa aproxima√ß√£o.
:::

:::

# Estima√ß√£o

## 2 Estima√ß√£o {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Normalmente, usamos estat√≠sticas para aproximar o valor de algum par√¢metro populacional. Nesse caso, elas recebem um nome especial.
:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Um **estimador** √© uma estat√≠stica utilizada para aproximar o valor de um par√¢metro populacional.
:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Podemos dividir a tarefa de estima√ß√£o de par√¢metros em:

- Estima√ß√£o pontual: lida com a avalia√ß√£o do desempenho de um estimador;
- Estima√ß√£o intervalar: lida com a constru√ß√£o de intervalos que englobam a variabilidade do
estimador no processo de estima√ß√£o.
:::

:::


## Estima√ß√£o Pontual

::: {.callout-note}
## Defini√ß√£o

Sejam $X_1, \ldots, X_n$ uma aa de uma popula√ß√£o $X$ e $\theta$ um par√¢metro. Um estimador $\hat\theta$ de $\theta$ √© qualquer fun√ß√£o da aa $\hat\theta = \hat\theta(X_1, \ldots, X_n)$ utilizada para aproximar o valor de $\theta$. Uma **Estimativa** √© o valor observado de um estimador para uma amostra $x_1, \ldots, x_n$ espec√≠fica.
:::

::: fragment

::: {.callout-important}
## Problema

Sendo poss√≠vel definir diversos estimadores para $\theta$, como avaliar a qualidade de diferentes estimadores?
:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Existem diferentes medidas. Veremos: V√≠cio; Erro-padr√£o; e Erro Quadr√°tico M√©dio (EQM).
:::

:::

## 2.1 Estima√ß√£o Pontual (V√≠cio) {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Deve ser **esperado** de um bom estimador que ele forne√ßa o verdadeiro valor do par√¢metro.
:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Seja $\theta$ um par√¢metro e $\hat\theta$ um estimador de $\theta$. O v√≠cio (ou vi√©s) de $\hat\theta$ √© definido por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$B(\hat\theta) = E(\hat\theta) - \theta = \mu_{\hat\theta} - \theta;$ </td>
  </tr>
</table>
onde $\mu_{\hat\theta} = E(\hat\theta)$ √© o valor esperado de $\hat\theta$.
:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√µes

1. O v√≠cio de um estimador mede, em m√©dia, quanto o estimador se dist√¢ncia do par√¢metro;
2. Dizemos que $\hat\theta$ √© **n√£o viciado** (ou n√£o viesado) se seu v√≠cio $B(\hat\theta) = 0$.

:::

:::

## 2.1 Estima√ß√£o Pontual (V√≠cio) {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=50%}

```{r}
n <- 25; i <- 0
th0 <- th1 <- NULL
while(i < n){
  x0 <- runif(1,-1,1); y0 <- runif(1,-1,1)
  x1 <- runif(1,-1,1); y1 <- runif(1,-1,1)
  if(x0^2 + y0^2 < 1 & x1^2 + y1^2 < 1){
    th0 <- rbind(th0, c(x0, y0))
    th1 <- rbind(th1, c(x1+1, y1+1))
    i <- i+1
  }
}
rngX <- range(th0[,1], th1[,1])
rngY <- range(th0[,2], th1[,2])
par(family = "serif", mar=c(5,4,1,2))
plot(0, type='n', xlab='', ylab='', main ='', frame.plot=FALSE,
     axes = FALSE, xlim=rngX, ylim=rngY)
points(x = th0[,1], y = th0[,2], pch=21, bg='blue', cex=1.5)
points(x = th1[,1], y = th1[,2], pch=21, bg='red', cex=1.5)
points(x = 0, y = 0, pch=21, bg='black', cex=2)
text(x = 0, y = 0, labels=expression(theta), adj=c(2,2), cex=2)
```

:::

::: {.column #vcenter width=50%}

::: {.callout-caution}
## Observa√ß√£o
Ilustra√ß√£o de estimativas fornecidas por um estimador [**n√£o viesado**]{style="color:blue;"} e um [**viesado**]{style="color:red;"} de um par√¢metro $\theta$.
:::

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 4 {.unnumbered .unlisted}

Suponha que ser√£o observadas $n ‚àí 1 \geq 1$ observa√ß√µes $X_1, \ldots, X_{n-1}$ de $X$, com $E(X) = \mu$. Defina a m√©dia dessas $n-1$ observa√ß√µes por $\bar{X}_{n-1} = \frac{1}{n-1}\sum_{i=1}^{n-1}$.

::: fragment

Suponha que uma nova observa√ß√£o $X_n$ ficar√° dispon√≠vel. Considere os seguintes estimadores de $\mu$:
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\hat{\mu}_1 = \frac{n-1}{n}\bar{X}_{n-1} + \frac{1}{n}X_n$ </td> <td> e </td> <td>$\hat{\mu}_2 = \frac{\bar{X}_{n-1} + X_n}{2}$. </td>
  </tr>
</table>

:::

::: fragment

Qual √© o v√≠cio de $\hat{\mu}_1$ e $\hat{\mu}_2$?

:::

::: fragment

Temos que
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$E(\hat{\mu}_1) = E\left[\frac{n-1}{n}\bar{X}_{n-1} + \frac{1}{n}X_n\right] = \frac{n-1}{n}E(\bar{X}_{n-1}) + \frac{1}{n}E(X_n) = \frac{n-1}{n}\mu + \frac{1}{n}\mu = \mu.$</td>
  </tr>
</table>

:::

::: fragment

e que

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$E(\hat{\mu}_2) = E\left[\frac{\bar{X}_{n-1} + X_n}{2}\right] = \frac{E(\bar{X}_{n-1}) + E(X_n)}{2} = \frac{\mu + \mu}{2} = \mu.$</td>
  </tr>
</table>

:::

::: fragment

Assim, $B(\hat{\mu}_1) = B(\hat{\mu}_2) = \mu-\mu = 0$. Portanto, ambos s√£o **n√£o viesados**.

:::

## 2.1 Estima√ß√£o Pontual (V√≠cio) {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 0px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 5:** Verifiquemos que $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n(X_i - \bar{X})^2$ √© um estimador viesado de $\sigma^2$. De fato,

<table>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td>$E(\hat{\sigma}^2)$ </td> <td> $=$ </td> <td> $\frac{1}{n}\sum_{i=1}^nE[(X_i-\bar{X})^2]=\frac{1}{n}\sum_{i=1}^n
E[(X_i-\mu-(\bar{X}-\mu))^2]$</td>
  </tr>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> $\frac{1}{n} \sum_{i=1}^n E[(X_i-\mu)^2-2(X_i-\mu)(\bar{X}-\mu)+(\bar{X}-\mu)^2]$</td>
  </tr>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> $\frac{1}{n}\sum_{i=1}^n\left\{\sigma^2-2E\left[(X_i-\mu)\left(\frac{1}{n}\sum_j(X_j-\mu)\right)\right] + E[(\bar{X}-\mu)^2]\right\}$</td>
  </tr>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> $\frac{1}{n}\sum_{i=1}^n\left\{\sigma^2-\frac{2}{n}\sum_jE[(X_i-\mu)(X_j-\mu)] + \frac{\sigma^2}{n}\right\}$</td>
  </tr>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> $\frac{1}{n}\sum_{i=1}^n\left\{\sigma^2-\frac{2}{n}\sum_j\text{Cov}(X_i, X_j) + \frac{\sigma^2}{n}\right\}$</td>
  </tr>
  <tr class="fragment" style="border-bottom:none;text-align:center;">
    <td> </td> <td> $=$ </td> <td> $\frac{1}{n}\sum_{i=1}^n\left\{\sigma^2-\frac{2\sigma^2}{n}+\frac{\sigma^2}{n}\right\}=\sigma^2 \color{red}{-\frac{\sigma^2}{n}}$.</td>
  </tr>
</table>

:::

::: fragment

::: {.callout-tip}
## Exerc√≠cio

Tendo em vista o Exemplo 5, mostre que $S^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X})^2$ √© um estimador **n√£o** viesado de $\sigma^2$.
:::

:::

## 2.1 Estima√ß√£o Pontual (Erro-Padr√£o) {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√µes

- Estimadores s√£o va;
- Al√©m do v√≠cio, √© importante saber a sua variabilidade;
- Podemos comparar dois estimadores n√£o viesados pelo seu **erro-padr√£o**.

:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Seja $\hat{\theta}$ estimador de $\theta$. O erro-padr√£o ($EP(\hat{\theta})$) de $\hat{\theta}$ √© o desvio-padr√£o de $\hat{\theta}$, isto √©, $EP(\hat{\theta})=\sqrt{V(\hat{\theta})}$.

:::

:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Sejam $\hat{\theta}_1$ e $\hat{\theta}_2$ estimadores n√£o viesados de $\theta$. Dizemos que $\hat{\theta}_1$ √© mais eficiente que $\hat{\theta}_2$ se $EP(\hat{\theta}_1) < EP(\hat{\theta}_2)$.

:::

:::

## 2.1 Estima√ß√£o Pontual (Erro-Padr√£o) {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=50%}

```{r}
n <- 30; i <- 0
th0 <- th1 <- NULL
while(i < n){
  x0 <- runif(1,-1,1); y0 <- runif(1,-1,1)
  x1 <- runif(1,-2,2); y1 <- runif(1,-2,2)
  if(x0^2 + y0^2 < 1 & x1^2 + y1^2 < 2^2){
    th0 <- rbind(th0, c(x0, y0))
    th1 <- rbind(th1, c(x1, y1))
    i <- i+1
  }
}
rngX <- range(th0[,1], th1[,1])
rngY <- range(th0[,2], th1[,2])
par(family = "serif", mar=c(5,4,1,2))
plot(0, type='n', xlab='', ylab='', main ='', frame.plot=FALSE,
     axes = FALSE, xlim=rngX, ylim=rngY)
points(x = th1[,1], y = th1[,2], pch=21, bg='red', cex=1.5)
points(x = th0[,1], y = th0[,2], pch=21, bg='blue', cex=1.5)
points(x = 0, y = 0, pch=21, bg='black', cex=2)
text(x = 0, y = 0, labels=expression(theta), adj=c(2,2), cex=2)
```

:::

::: {.column #vcenter width=50%}

::: {.callout-caution}
## Observa√ß√£o
Ilustra√ß√£o de estimativas fornecidas por dois estimadores [$\hat\theta_1$]{style="color:blue;"} e [$\hat\theta_2$]{style="color:red;"} de um par√¢metro $\theta$, em que [$\hat\theta_1$]{style="color:blue;"} √© mais eficiente que [$\hat\theta_2$]{style="color:red;"}.
:::

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 4 (cont.) {.unnumbered .unlisted}

Verifiquemos os Erros-Padr√£o de $\hat{\mu}_1$ e de $\hat{\mu}_2$. [Temos que]{.fragment fragment-index=1}
<table class="fragment" data-fragment-index="1">
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle V(\hat{\mu}_1) = V\left(\frac{n-1}{n}\bar{X}_{n-1}+\frac{1}{n}X_n\right) = \left(\frac{n-1}{n}\right)^2\frac{\sigma^2}{n-1}+\frac{1}{n^2}\sigma^2 = \frac{n-1+1}{n^2}\sigma^2=\frac{1}{n}\sigma^2}$</td>
  </tr>
</table>
[e]{.fragment fragment-index=2}
<table class="fragment" data-fragment-index="2">
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle V(\hat{\mu}_2) = V\left(\frac{\bar{X}_{n-1}+X_n}{2}\right) = \frac{1}{4}(V(\bar{X}_{n-1})+V(X_n)) = \frac{1}{4} \left(\frac{\sigma^2}{n-1} + \sigma^2\right)=\frac{n}{4(n-1)}\sigma^2}$.</td>
  </tr>
</table>

[Vejamos quando $\hat{\mu}_1$ √© mais eficiente do que $\hat{\mu}_2$.]{.fragment fragment-index=3} [De fato,]{.fragment fragment-index=4}
<table class="fragment" data-fragment-index="4">
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle V(\hat{\mu}_1) < V(\hat{\mu}_2) \Rightarrow \frac{\sigma^2}{n} < \frac{n\sigma^2}{4(n-1)} \Rightarrow n^2-4n+4>0 \Rightarrow (n-2)^2>0}$.</td>
  </tr>
</table>

[Assim, $\hat{\mu}_1$ √© mais eficiente do que $\hat{\mu}_2$ se $(n-2)^2>0 \Rightarrow n > 2$.]{.fragment fragment-index=5} [Se $n = 2$ ambos estimadores s√£o igualmente eficientes.]{.fragment fragment-index=6}

## 2.1 Estima√ß√£o Pontual (Erro Quadr√°tico M√©dio) {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Vimos que o Erro-Padr√£o nos permite comparar a efici√™ncia de estimadores n√£o viesados. Para comparar estimadores com v√≠cios diferentes, podemos usar o [**Erro Quadr√°tico M√©dio**]{style='color:red;'}.
:::

::: fragment

::: {.callout-note}
## Defini√ß√£o

Seja $\hat{\theta}$ estimador de $\theta$. Definimos o **Erro Quadr√°tico M√©dio** (EQM) de
$\hat{\theta}$ por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$EQM(\hat{\theta})=E[(\hat{\theta}-\theta)^2]$.</td>
  </tr>
</table>
:::

:::

::: fragment

::: {.callout-tip}
## Exerc√≠cios

Mostre que o EQM de $\hat\theta$ pode ser escrito como a soma do quadrado do v√≠cio e da vari√¢ncia
de $\hat\theta$, isto √©,
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$EQM(\hat{\theta})=[B(\hat{\theta})]^2+V(\hat{\theta})$.</td>
  </tr>
</table>

:::

:::

## {.unnumbered .unlisted .smaller}

### Exemplo 5 (cont.) {.unnumbered .unlisted}

No Exemplo 5, mostramos que $B(\hat{\sigma}^2)=-\frac{\sigma^2}{n}$. √â poss√≠vel mostrar
que $B(S^2)=0$ e que, se $X\sim N(\mu,\sigma^2)$, ent√£o

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle V(S^2)=\frac{2\sigma^4}{n-1}}$</td> <td> e </td> <td> ${\displaystyle V(\hat{\sigma}^2) = \left( \frac{n-1}{n} \right)^2 \frac{2\sigma^4}{n-1}}$. </td>
  </tr>
</table>

::: fragment

Logo, os EQM's de $S^2$ e de $\hat{\sigma}^2$ s√£o, respectivamente,

<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle EQM(S^2)=\frac{2\sigma^4}{n-1}}$</td> <td> e </td> <td> ${\displaystyle EQM(\hat{\sigma}^2)=\left(\frac{\sigma^2}{n}\right)^2 + \frac{(n-1)2\sigma^4}{n^2} = \frac{2\sigma^4}{n}-\frac{\sigma^4}{n^2}}$. </td>
  </tr>
</table>

:::

::: fragment

Assim, em termos de EQM, vemos que $\hat{\sigma}^2$ √© melhor do que $S^2$.

:::

## 2.1 Estima√ß√£o Pontual (Consist√™ncia) {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Seja $\hat{\theta}$ um estimador do par√¢metro $\theta$. Dizemos que $\hat{\theta}$ √© **consistente** se seu EQM tende a zero quando o tamanho da amostra tende a infinito. Isto √©, se
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle \lim_{n\to\infty}EQM(\hat{\theta})=0}$. </td>
  </tr>
</table>

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

::: incremental

1. A defini√ß√£o acima implica que $\hat{\theta}$ √© consistente se seu v√≠cio e sua vari√¢ncia tendem a 0;
2. Em poucas palavras, quando um estimador √© consistente suas estimativas tendem a ser pr√≥ximas do verdadeiro valor do par√¢metro

:::

:::

:::

::: fragment

::: {style="border-style: solid; border-width: 3px; padding-top: 0px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 5 (cont.):** Como ${\displaystyle \lim_{n\to\infty} EQM(S^2) = \lim_{n\to\infty} EQM(\hat\sigma^2) = 0}$, ambos s√£o consistentes para $\sigma^2$.
:::

:::

## 2.1 Estima√ß√£o Pontual (Consist√™ncia) {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=50%}

```{r out.width="95%"}
x <- seq(0, 10, length.out=1000)
f1 <- dgamma(x, shape=2, scale = 2)
f2 <- dgamma(x, shape=4, scale = 1)
f3 <- dgamma(x, shape=8, scale = 0.5)
par(family = "serif", mar=c(5,4,1,2))
rngY <- range(f1, f2, f3)
rngX <- range(x)
plot(x, f1, type='n', xlab='', ylab='', main='', lwd=3,
     ylim=rngY+c(-.1*diff(rngY),.13*diff(rngY)),
     xlim=rngX+c(-.02*diff(rngX),.08*diff(rngX)))
arrows(y0 = rngY[1]-.1*diff(rngY), y1 = rngY[2]+.13*diff(rngY), x0=0, x1=0, length = .1)
arrows(x0 = rngX[1]-.02*diff(rngX), x1 = rngX[2]+.08*diff(rngX), y0=0, y1=0, length = .1)
lines(x, f1, lwd=3, col='red')
lines(x, f2, lwd=3, col='blue')
lines(x, f3, lwd=3, col='darkgreen')
lines(c(4.2,4.2), rngY+c(0,.13*diff(rngY)), lwd=2.5, lty=3)
lines(c(4.2,4.2), rngY[1]+c(-0.03*diff(rngY),0), lwd=1.5)
text(x=4.2, y=rngY[1]-0.05*diff(rngY), labels=expression(theta), adj=c(.5, 1.2))
```

:::

::: {.column #vcenter width=50%}

::: {.callout-caution}
## Observa√ß√£o

Ilustra√ß√£o da distribui√ß√£o amostral de um estimador consistente com
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle \color{red}{n}< \color{blue}{n} < \color{darkgreen}{n}}$. </td>
  </tr>
</table>
:::

:::

:::

## M√©todos de Obten√ß√£o de Estimadores

::: {.callout-caution}
## Observa√ß√µes

Em alguns casos, o estimador √© escolhido por seu apelo intuitivo. Por exemplo, √© natural usar

::: incremental

- $\bar{X}$ (m√©dia amostral) para estimar $\mu$ (m√©dia populacional);
- $\hat{p}$ (propor√ß√£o amostral) para estimar $p$ (propor√ß√£o populacional);
- $S^2$ (vari√¢ncia amostral) para estimar $\sigma^2$ (vari√¢ncia populacional).

:::

:::

::: fragment

::: {style="border-style: solid; border-width: 3px; padding-top: 0px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 6:** Qual o estimador mais apropriado para a taxa $\lambda$ de uma popula√ß√£o $X \sim \text{Exp}(\lambda)$?

:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

No Exemplo 6, n√£o h√° um candidato natural para $\hat\lambda$. Veremos dois m√©todos de obten√ß√£o de estimadores: o **m√©todo dos momentos**; e o **m√©todo da m√°xima verossimilhan√ßa**.

:::

:::

## 2.2 M√©todos de Obten√ß√£o de Estimadores {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

Nosso objetivo √© encontrar um estimador $\hat\theta$ do vetor de par√¢metros $\theta = (\theta_1,\ldots, \theta_\kappa)'$ por meio de uma aa $X_1,\ldots,X_n$ de uma popula√ß√£o $X$.

:::

::: fragment

::: {.callout-note}
## Defini√ß√µes

1. Fixado $k\in\{1,2,\ldots\}$, o **momento populacional** de ordem $k$ de uma popula√ß√£o $X$ √© definido por $\mu^{(k)}=E(X^k)$;
2. Dada uma aa $X_1,\ldots,X_n$ de uma popula√ß√£o $X$ e fixado $k\in\{1,2, \ldots\}$, o **momento amostral** de ordem $k$ √© definido por $\hat{\mu}^{(k)}=\frac{1}{n}\sum_{i=1}^{n}X^k_i$.

:::

:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Note que $\mu^{(k)}=\mu^{(k)}(\theta)$

:::

:::

## 2.2 M√©todos de Obten√ß√£o de Estimadores {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Se o vetor param√©trico $\theta = (\theta_1,\ldots, \theta_\kappa)'$ tem dimens√£o $\kappa$ e os momentos populacionais at√© a ordem $\kappa$ s√£o $\mu^{(1)} = \mu^{(1)}(\theta), \ldots, \mu^{(\kappa)} = \mu^{(\kappa)}(\theta)$. O Estimador de M√©todo dos Momentos (EMM), $\hat\theta_{\text{MM}}$, √© o vetor $\theta$ que soluciona o sistema de equa√ß√µes
$$
\begin{cases}
  \mu^{(1)}(\theta) &=& \hat{\mu}^{(1)}\\
	&\vdots&\\
	\mu^{(\kappa)}(\theta) &=& \hat{\mu}^{(\kappa)}
\end{cases}
$$
:::

::: fragment

::: {.callout-caution}
## Observa√ß√£o

Caso o sistema acima n√£o seja bijetivo (por exemplo, se $\theta = (\theta_1, \theta_2)'$, mas $\mu^{(1)}=\mu^{(2)}=\theta_1$), podemos utilizar momentos populacionais de ordens superiores.

:::

:::

## 2.2 M√©todos de Obten√ß√£o de Estimadores {.unnumbered .unlisted}

::: {style="border-style: solid; border-width: 3px; padding-top: 0px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%; margin-bottom:24px;"}

**Exemplo 6 (cont.):** Temos que, para a popula√ß√£o $X \sim \text{Exp}(\lambda)$, o momento populacional de ordem 1 √© $\mu^{(1)} = E(X) = \frac{1}{\lambda}$. Como o momento amostral de ordem 1 √© $\hat\mu^{(1)} = \bar{X}$, o EMM de $\lambda$ √© o valor $\hat\lambda_{\text{MM}}$ tal que $\frac{1}{\hat\lambda_{\text{MM}}} = \bar{X}$. Logo, $\hat\lambda_{\text{MM}} = \frac{1}{\bar{X}}$.

:::

::: fragment

::: {style="border-style: solid; border-width: 3px; padding-top: 12px; padding-bottom: 12px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 7:** Se a popula√ß√£o for $X\sim \text{N}(\mu,\sigma^2)$, ent√£o $\mu^{(1)} = E(X) = \mu$ e $\mu^{(2)} = E(X^2) = \sigma^2 + \mu^2$. Como $\hat\mu^{(1)} = \bar{X}$ e $\hat\mu^{(2)} = \frac{1}{n}\sum_i X_i^2$, os EMM's de $\mu$ e $\sigma$ s√£o dados, respectivamente, por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>${\displaystyle \hat\mu_{\text{MM}} = \bar{X}}$ </td> <td> e </td> <td>${\displaystyle \hat\sigma_{\text{MM}} = \left(\frac{1}{n}\sum_i X_i^2 - \hat\mu_{\text{MM}}^2\right)^{1/2}}$. </td>
  </tr>
</table>

:::

:::

::: fragment

::: {.callout-tip}
## Exerc√≠cio

Encontre o EMM do par√¢metro $\theta$ se a popula√ß√£o $X$ tem fdp $f(x)=\frac{\theta}{x^{\theta+1}}$, $x>1$.
:::

:::

## 2.2 M√©todos de Obten√ß√£o de Estimadores {.unnumbered .unlisted}

::: {.callout-note}
## Defini√ß√£o

Seja $X_1,\ldots,X_n$ aa de uma vad $X$ com fmp $p(x;\theta)$. A fun√ß√£o de verossimilhan√ßa √© definida por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\displaystyle L(\theta;x_1,\ldots,x_n) = p(x_1, \ldots, x_n; \theta) = p(x_1;\theta)\cdots p(x_n;\theta) = \prod_{i=1}^n p(x_i;\theta)$. </td>
  </tr>
</table>

Seja $X_1,\ldots,X_n$ aa de uma vac $X$ com fdp $f(x;\theta)$. A fun√ß√£o de verossimilhan√ßa √© definida por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\displaystyle  L(\theta;x_1,\ldots,x_n) = f(x_1, \ldots, x_n; \theta) = f(x_1;\theta)\cdots f(x_n;\theta) = \prod_{i=1}^n f(x_i;\theta)$. </td>
  </tr>
</table>

:::

::: fragment

::: {.callout-caution}
## Observa√ß√µes

1. A fun√ß√£o de verossimilhan√ßa tem a mesma defini√ß√£o de fmp conjunta (se a popula√ß√£o $X$ √© discreta) ou fdp conjunta (se a popula√ß√£o $X$ √© cont√≠nua), mas √© fun√ß√£o de $\theta$ e n√£o de $x_1, \ldots, x_n$;
2. $L(\theta; x_1, \ldots, x_n)$ mede a **plausabilidade** de $\theta$ com respeito a amostra observada $x_1, \ldots, x_n$.
:::

:::

## 2.2 M√©todos de Obten√ß√£o de Estimadores {.unnumbered .unlisted} 

::: {.callout-note}
## Defini√ß√£o

O Estimador de M√°xima Verossimilhan√ßa (EMV) de $\theta$ √© definido por
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\DeclareMathOperator*{\aM}{argmax}\hat\theta_{\text{MV}}=\aM_{\theta}[L(\theta;x_1,\ldots,x_n)]$.</td>
  </tr>
</table>
:::

::: fragment

::: {.callout-caution}
## Observa√ß√µes

::: incremental

1. No caso discreto, pela independ√™ncia, temos $L(\theta;x_1,\ldots,x_n)= P(X_1=x_1,\ldots,X_n=x_n|\theta),$ de modo que $\hat{\theta}_{\text{MV}}$ maximiza essa probabilidade, tornando a amostra $x_1,\ldots,x_n$ mais **plaus√≠vel** ou **veross√≠mil**;
2. Se $L$ √© diferenci√°vel em $\theta$, o EMV √© o valor $\hat{\theta}_{\text{MV}}$ que satisfaz:
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\left[\left.\frac{\partial}{\partial\theta} \ell(\theta) \right|_{\theta=\hat{\theta}_{\text{MV}}} \right] = \left[\left.\frac{\partial}{\partial\theta}\log\left[L(\theta;x_1,\ldots,x_n) \right]\right|_{\theta=\hat{\theta}_{\text{MV}}} \right] = \sum_{i=1}^{n} \left[ \left.\frac{\partial}{\partial\theta}\log f(x_i;\theta)\right|_{\theta=\hat{\theta}_{\text{MV}}} \right]=0$,</td>
  </tr>
</table>
em que $\ell(\theta) = \log\left[L(\theta;x_1,\ldots,x_n) \right]$ √© a [**log-verossimilhan√ßa**]{style='color:red;'}, desde que $\hat{\theta}_{\text{MV}}$ seja ponto de m√°ximo;
3. Sob algumas suposi√ß√µes de regularidade, √© poss√≠vel mostrar que o EMV √© consistente.

:::

:::

:::

## 2.2 M√©todos de Obten√ß√£o de Estimadores {.unnumbered .unlisted} 

::: {style="border-style: solid; border-width: 3px; padding-top: 12px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%; margin-bottom:24px;"}

**Exemplo 6 (cont.):** Se a popula√ß√£o $X\sim \text{Exp}(\lambda)$, a fun√ß√£o de verossimilhan√ßa √©
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$L(\lambda; x_1, \ldots, x_n) = \prod_{i=1}^n[\lambda e^{-\lambda x_i}] = \lambda^n e^{-\lambda \sum_ix_i} = \lambda^n e^{-\lambda n\bar{x}}$.</td>
  </tr>
</table>

::: fragment

Assim, $\ell(\lambda) = n\log(\lambda)-\lambda n \bar{x}$ e $\frac{\partial\ell(\lambda)}{\partial\lambda} = \frac{n}{\lambda} - n\bar{x}$. [Como $\hat\lambda_{\text{MV}}$ satisfaz $\frac{n}{\hat\lambda_{\text{MV}}} - n\bar{x} = 0$, temos $\hat\lambda_{\text{MV}} = \frac{1}{\bar{X}}$.]{.fragment}

:::

:::

::: fragment

::: {style="border-style: solid; border-width: 3px; padding-top: 12px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 7 (cont.):** Vamos determinar o EMV de $(\mu, \sigma)$. Podemos mostrar que a log-verossimilhan√ßa √©
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td>$\ell(\mu,\sigma) = -\frac{n}{2}\log(2\pi) - n\log(\sigma) -\frac{1}{2}\sum_i\left(\frac{x_i-\mu}{\sigma}\right)^2$.</td>
  </tr>
</table>

::: fragment

Assim,
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td> $\frac{\partial\ell}{\partial\mu} = \sum_i\left(\frac{x_i-\mu}{\sigma^2}\right) = \frac{n\bar{x} - n\mu}{\sigma^2}$ </td> <td> e </td> <td> $\frac{\partial\ell}{\partial\sigma} = -\frac{n}{\sigma} + \frac{1}{\sigma^3}\sum_i(x_i-\mu)^2$. </td>
  </tr>
</table>

:::

::: fragment

Logo, os EMV's $\hat\mu_{\text{MV}}$ e $\hat\sigma_{\text{MV}}$ devem satisfazer
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td> $\frac{n\bar{x} - n\hat\mu_{\text{MV}}}{\hat\sigma^2_{\text{MV}}} = 0$ </td> <td> e </td> <td> $-\frac{n}{\hat\sigma_{\text{MV}}} + \frac{1}{\hat\sigma^3_{\text{MV}}}\sum_i(x_i-\hat\mu_{\text{MV}})^2 = 0$, </td>
  </tr>
</table>
[o que fornece $\hat\mu_{\text{MV}} = \bar{X}$ e $\hat\sigma_{\text{MV}} = \left[\frac{1}{n}\sum_i(X_i - \hat\mu_{\text{MV}})^2\right]^{1/2}$.]{.fragment}

:::

:::

:::

## 2.2 M√©todos de Obten√ß√£o de Estimadores {.unnumbered .unlisted} 


::: {.callout-tip}
## Exerc√≠cio

Determine EMV do par√¢metro $\theta$ se a popula√ß√£o $X$ tem fdp $f(x)=\frac{\theta}{x^{\theta+1}}$, $x>1$.
:::

::: fragment

::: {.callout-caution}
## Observa√ß√µes

::: incremental

1. Em alguns casos, n√£o podemos realizar analiticamente a maximiza√ß√£o de $L(\theta; x_1, \ldots, x_n)$, ou equivalentemente de $\ell(\theta)$;
2. Nesses casos, podemos recorrer √© m√©todos num√©ricos. Por exemplo, podemos utilizar o algoritmo de Newton-Raphson para encontrar as ra√≠zes
<table>
  <tr style="border-bottom:none;text-align:center;">
    <td> $\left[\left.\frac{\partial \ell(\theta)}{\partial \theta}\right|_{\theta = \hat\theta_{\text{MV}}}\right] = 0$. </td>
  </tr>
</table>

:::

:::

:::

## Estima√ß√£o Intervalar

# Testes de Hip√≥teses

## Teste para $\mu$

## $p$-valor 

::: {.callout-caution}
## Observa√ß√£os

Relatar o resultado de um teste de hip√≥teses simplesmente dizendo se a hip√≥tese nula foi rejeitada ao n√≠vel de signific√¢ncia especificado pode ser inadequado, pois n√£o mensura a quantidade de evid√™ncia obtida da amostra e imp√µe o mesmo n√≠vel de signific√¢ncia a todos os tomadores de decis√£o.
:::

::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo 8:** O tempo m√©dio de alivio das dores do medicamento mais vendido no mercado √© de 10 min. Uma empresa desenvolveu um medicamento e colocou a venda afirmando que um teste estat√≠stico rejeitou $H_0 : \mu \geq 10$ em favor de $H_1 : \mu < 10$ ao n√≠vel de 0.1.

::: 

::: {.callout-caution}
## Observa√ß√£o

Uma pessoa que costuma comprar o medicamento mais vendido pode optar por n√≠vel de signific√¢ncia menor de 0.01, por exemplo. Isso tornaria a rejei√ß√£o de $H_0$ "mais dif√≠cil". O resultado como divulgado n√£o permite aos compradores tomarem sua pr√≥pria decis√£o.
:::

## {.unnumbered .unlisted .smaller} 

###  Exemplo 8 (cont.) {.unnumbered .unlisted} 

::: columns

::: {.column #vcenter width=55%}

Suponha que a empresa tenha feito o teste com $n=49$ e obteve $\bar{x} = 9$ e $s^2 = 10.89$. A regi√£o cr√≠tica √© da forma $\text{RC} = \{\bar{X} < x_c\}$, por meio da padroniza√ß√£o podemos reescrev√™-la como

<table style="margin-left: auto; margin-right: auto;">
  <tr style="border-bottom:none;text-align:center;">
    <td>$\text{RC} = \left\{\frac{\bar{X}-\mu_0}{S/\sqrt{n}} < \frac{x_c-\mu_0}{S/\sqrt{n}}\right\} \approx \{Z < -z_\alpha\}$,</td>
  </tr>
</table>

em que $Z = \frac{\bar{X}-\mu_0}{S/\sqrt{n}}$ e $z_\alpha$ √© o valor tal que $P(Z > z_\alpha) = \alpha$. Assim, a amostra resulta em um $Z$ observado de $Z_{\text{obs}} = \frac{9-10}{3.3/\sqrt{49}} \approx -2.12$.

As figuras ao lado mostram as regi√µes cr√≠ticas do teste para diferentes n√≠veis de signific√¢ncia $\alpha$.

:::

::: {.column #vcenter width=45%}

::: fragment

```{r}
xb <- 9; s <- 3.3; n <-49; mu0 <- 10; zo <- (xb-mu0)/(s/sqrt(n))
alp <- .1; zc <- qnorm(alp)
x <- seq(-4.5, 4.5, length.out=1000)
fx <- dnorm(x = x)
par(family = "serif", mar=c(5,4,1,2))
plot(x, fx, type='n', xlab='', ylab='', main='', lwd=3,
     ylim=range(fx)+c(-.1*diff(range(fx)),.13*diff(range(fx))),
     xlim=range(x)+c(-.02*diff(range(x)),.08*diff(range(x))))
polygon(c(x[1], x[x < zc], max(x[x < zc])), c(0, fx[x<zc], 0), col = 'lightblue')
arrows(y0 = min(fx)-.1*diff(range(fx)), y1 = max(fx)+.13*diff(range(fx)), x0=0, x1=0, length = .1)
arrows(x0 = min(x)-.02*diff(range(x)), x1 = max(x)+.08*diff(range(x)), y0=0, y1=0, length = .1)
lines(x, fx, lwd=3)
arrows(x0 = 1, y0 = 0.4, x1 = zc, y1 = 0, length = .1)
points(x = zc, y = 0, pch=19, cex=.8)
text(x = 1, y=0.4, labels = bquote(-z[alpha]~"="~-z[.(alp)]~"="~-.(-round(zc, 2))), pos = 4)
points(x = zo, y = 0, pch=19, cex=.8, col='red')
text(x = zo, y=0, labels = bquote(z[obs]==.(round(zo, 2))), pos = 1, col='red')
text(x = -4.5, y=0.4, labels = bquote(Rejeitar~italic(H)[0]), pos = 4)
```
:::

:::

:::

## {.unnumbered .unlisted .smaller} 

###  Exemplo 8 (cont.) {.unnumbered .unlisted} 

::: columns

::: {.column #vcenter width=55%}

Suponha que a empresa tenha feito o teste com $n=49$ e obteve $\bar{x} = 9$ e $s^2 = 10.89$. A regi√£o cr√≠tica √© da forma $\text{RC} = \{\bar{X} < x_c\}$, por meio da padroniza√ß√£o podemos reescrev√™-la como

<table style="margin-left: auto; margin-right: auto;">
  <tr style="border-bottom:none;text-align:center;">
    <td>$\text{RC} = \left\{\frac{\bar{X}-\mu_0}{S/\sqrt{n}} < \frac{x_c-\mu_0}{S/\sqrt{n}}\right\} \approx \{Z < -z_\alpha\}$,</td>
  </tr>
</table>

em que $Z = \frac{\bar{X}-\mu_0}{S/\sqrt{n}}$ e $z_\alpha$ √© o valor tal que $P(Z > z_\alpha) = \alpha$. Assim, a amostra resulta em um $Z$ observado de $Z_{\text{obs}} = \frac{9-10}{3.3/\sqrt{49}} \approx -2.12$.

As figuras ao lado mostram as regi√µes cr√≠ticas do teste para diferentes n√≠veis de signific√¢ncia $\alpha$.

:::

::: {.column #vcenter width=45%}

```{r}
xb <- 9; s <- 3.3; n <-49; mu0 <- 10; zo <- (xb-mu0)/(s/sqrt(n))
alp <- .05; zc <- qnorm(alp)
x <- seq(-4.5, 4.5, length.out=1000)
fx <- dnorm(x = x)
par(family = "serif", mar=c(5,4,1,2))
plot(x, fx, type='n', xlab='', ylab='', main='', lwd=3,
     ylim=range(fx)+c(-.1*diff(range(fx)),.13*diff(range(fx))),
     xlim=range(x)+c(-.02*diff(range(x)),.08*diff(range(x))))
polygon(c(x[1], x[x < zc], max(x[x < zc])), c(0, fx[x<zc], 0), col = 'lightblue')
arrows(y0 = min(fx)-.1*diff(range(fx)), y1 = max(fx)+.13*diff(range(fx)), x0=0, x1=0, length = .1)
arrows(x0 = min(x)-.02*diff(range(x)), x1 = max(x)+.08*diff(range(x)), y0=0, y1=0, length = .1)
lines(x, fx, lwd=3)
arrows(x0 = 1, y0 = 0.4, x1 = zc, y1 = 0, length = .1)
points(x = zc, y = 0, pch=19, cex=.8)
text(x = 1, y=0.4, labels = bquote(-z[alpha]~"="~-z[.(alp)]~"="~-.(-round(zc, 2))), pos = 4)
points(x = zo, y = 0, pch=19, cex=.8, col='red')
text(x = zo, y=0, labels = bquote(z[obs]==.(round(zo, 2))), pos = 1, col='red')
text(x = -4.5, y=0.4, labels = bquote(Rejeitar~italic(H)[0]), pos = 4)
```

:::

:::

## {.unnumbered .unlisted .smaller} 

###  Exemplo 8 (cont.) {.unnumbered .unlisted} 

::: columns

::: {.column #vcenter width=55%}

Suponha que a empresa tenha feito o teste com $n=49$ e obteve $\bar{x} = 9$ e $s^2 = 10.89$. A regi√£o cr√≠tica √© da forma $\text{RC} = \{\bar{X} < x_c\}$, por meio da padroniza√ß√£o podemos reescrev√™-la como

<table style="margin-left: auto; margin-right: auto;">
  <tr style="border-bottom:none;text-align:center;">
    <td>$\text{RC} = \left\{\frac{\bar{X}-\mu_0}{S/\sqrt{n}} < \frac{x_c-\mu_0}{S/\sqrt{n}}\right\} \approx \{Z < -z_\alpha\}$,</td>
  </tr>
</table>

em que $Z = \frac{\bar{X}-\mu_0}{S/\sqrt{n}}$ e $z_\alpha$ √© o valor tal que $P(Z > z_\alpha) = \alpha$. Assim, a amostra resulta em um $Z$ observado de $Z_{\text{obs}} = \frac{9-10}{3.3/\sqrt{49}} \approx -2.12$.

As figuras ao lado mostram as regi√µes cr√≠ticas do teste para diferentes n√≠veis de signific√¢ncia $\alpha$.

:::

::: {.column #vcenter width=45%}

```{r}
xb <- 9; s <- 3.3; n <-49; mu0 <- 10; zo <- (xb-mu0)/(s/sqrt(n))
alp <- .01; zc <- qnorm(alp)
x <- seq(-4.5, 4.5, length.out=1000)
fx <- dnorm(x = x)
par(family = "serif", mar=c(5,4,1,2))
plot(x, fx, type='n', xlab='', ylab='', main='', lwd=3,
     ylim=range(fx)+c(-.1*diff(range(fx)),.13*diff(range(fx))),
     xlim=range(x)+c(-.02*diff(range(x)),.08*diff(range(x))))
polygon(c(x[1], x[x < zc], max(x[x < zc])), c(0, fx[x<zc], 0), col = 'lightblue')
arrows(y0 = min(fx)-.1*diff(range(fx)), y1 = max(fx)+.13*diff(range(fx)), x0=0, x1=0, length = .1)
arrows(x0 = min(x)-.02*diff(range(x)), x1 = max(x)+.08*diff(range(x)), y0=0, y1=0, length = .1)
lines(x, fx, lwd=3)
arrows(x0 = 1, y0 = 0.4, x1 = zc, y1 = 0, length = .1)
points(x = zc, y = 0, pch=19, cex=.8)
text(x = 1, y=0.4, labels = bquote(-z[alpha]~"="~-z[.(alp)]~"="~-.(-round(zc, 2))), pos = 4)
points(x = zo, y = 0, pch=19, cex=.8, col='red')
text(x = zo, y=0, labels = bquote(z[obs]==.(round(zo, 2))), pos = 1, col='red')
text(x = -4.5, y=0.4, labels = bquote(N√£o~rejeitar~italic(H)[0]), pos = 4)
```

:::

:::

## 3.2 $p$-valor {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=55%}

::: {.callout-note}
## Defini√ß√£o

O $p$-valor √© o menor n√≠vel de signific√¢ncia em que $H_0$ √© rejeitada, quando um teste √© usado em um determinado conjunto de dados.
:::

::: {.callout-caution}
## Observa√ß√£o

Uma vez que o $p$-valor foi determinado, a conclus√£o no n√≠vel $\alpha$, resulta da seguinte compara√ß√£o: se $p$-valor $\leq \alpha$ implica rejei√ß√£o de $H_0$ no n√≠vel $\alpha$; se $p$-valor $> \alpha$ implica n√£o-rejei√ß√£o de $H_0$ no n√≠vel $\alpha$.
:::

:::

::: {.column #vcenter width=45%}

::: fragment

```{r}
xb <- 9; s <- 3.3; n <-49; mu0 <- 10; zo <- (xb-mu0)/(s/sqrt(n))
alp <- .01; zc <- qnorm(alp)
x <- seq(-4.5, 4.5, length.out=1000)
pv <- pnorm(q = zo)
fx <- dnorm(x = x)
par(family = "serif", mar=c(5,4,1,2))
plot(x, fx, type='n', xlab='', ylab='', main='', lwd=3,
     ylim=range(fx)+c(-.1*diff(range(fx)),.13*diff(range(fx))),
     xlim=range(x)+c(-.02*diff(range(x)),.08*diff(range(x))))
polygon(c(x[1], x[x < zo], max(x[x < zo])), c(0, fx[x<zo], 0), col = rgb(red=1,green = 0, blue = 0, alpha = .3))
arrows(y0 = min(fx)-.1*diff(range(fx)), y1 = max(fx)+.13*diff(range(fx)), x0=0, x1=0, length = .1)
arrows(x0 = min(x)-.02*diff(range(x)), x1 = max(x)+.08*diff(range(x)), y0=0, y1=0, length = .1)
lines(x, fx, lwd=3)
arrows(x0 = 1.5, y0 = 0.25, x1 = -2.35, y1 = 0.012, length = .05)
text(x = 1.5, y=0.25, labels = bquote(italic(p)-valor==.(round(pv, 4))), pos = 4)
points(x = zo, y = 0, pch=19, cex=.8, col='red')
text(x = zo, y=0, labels = bquote(z[obs]==.(round(zo, 2))), pos = 1, col='red')
```

:::

:::

:::

## 3.2 $p$-valor {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√µes

1. Nos testes para $\mu$ em que usamos as estat√≠sticas $Z = \frac{\bar{X}-\mu_0}{\sigma/\sqrt{n}}$ (ou $S$ no lugar de $\sigma$ se $n$ grande) e $T = \frac{\bar{X}-\mu_0}{S/\sqrt{n}}$ (se $n$ pequeno numa popula√ß√£o normal), o c√°lculo do $p$-valor depende da forma de $H_1$;
2. No teste $Z$, temos
<table style="margin-left: auto; margin-right: auto;">
  <tr style="border-bottom:none;text-align:center;">
    <td>$p$-valor $= \begin{cases}1-\Phi(z_{\text{obs}}), & H_1 : \mu > \mu_0\\ \Phi(z_{\text{obs}}), & H_1 : \mu < \mu_0\\ 2[1-\Phi(|z_{\text{obs}}|)], & H_1 : \mu \neq \mu_0\end{cases}$;</td>
  </tr>
</table>
3. No teste $T$, temos
<table style="margin-left: auto; margin-right: auto;">
  <tr style="border-bottom:none;text-align:center;">
    <td>$p$-valor $= \begin{cases}1-F_T(t_{\text{obs}}), & H_1 : \mu > \mu_0\\ F_T(t_{\text{obs}}), & H_1 : \mu < \mu_0\\ 2[1-F_T(|t_{\text{obs}}|)], & H_1 : \mu \neq \mu_0\end{cases}$,</td>
  </tr>
</table>
em que $F_T(\cdot)$ √© a fda da distribui√ß√£o $t$-student com $n-1$ graus de liberdade e $t_{\text{obs}} = \frac{\bar{x}_{\text{obs}}-\mu_0}{s_{\text{obs}}/\sqrt{n}}$.
:::

# FIM {.unnumbered .unlisted}

# APAGAR DPS 1 {.unnumbered .unlisted}

::: {.callout-caution}
## Observa√ß√£o

:::

::: {.callout-note}
## Defini√ß√£o

:::

::: {.callout-warning}
## Propriedade

:::

::: {.callout-tip}
## Exerc√≠cio

:::

::: {.callout-important}
## Problema

:::


::: {style="border-style: solid; border-width: 3px; padding-bottom: 0px; padding-right: 12px; padding-left: 12px; font-size:70%;"}

**Exemplo:** 

::: 

# APAGAR DPS 2 {.unnumbered .unlisted}

::: columns

::: {.column #vcenter width=45%}
1
:::

::: {.column #vcenter width=55%}
2

3

4
:::

:::



```{=latex}
\begin{align*}
\Omega &= \{(1,1), (1,2), \ldots, (4,4)\}\\
       &= \{(x,y) : x,y = 1,\ldots,4\}.
\end{align*}
```


# FIM {.unnumbered .unlisted}
